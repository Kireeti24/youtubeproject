2024-07-29 12:20:50,665 - root - INFO - i am in the main method..
2024-07-29 12:20:50,665 - root - INFO - calling spark object
2024-07-29 12:20:50,665 - Create_spark - INFO - get_spark_object method started
2024-07-29 12:20:50,666 - Create_spark - INFO - master is Yarn
2024-07-29 12:20:55,057 - root - ERROR - An error occurred when calling main() please check the trace=== Java gateway process exited before sending its port number
2024-07-29 12:22:02,803 - root - INFO - i am in the main method..
2024-07-29 12:22:02,803 - root - INFO - calling spark object
2024-07-29 12:22:02,803 - Create_spark - INFO - get_spark_object method started
2024-07-29 12:22:02,805 - root - ERROR - An error occurred when calling main() please check the trace=== name 'envn' is not defined
2024-07-29 12:22:40,339 - root - INFO - i am in the main method..
2024-07-29 12:22:40,344 - root - INFO - calling spark object
2024-07-29 12:22:40,346 - Create_spark - INFO - get_spark_object method started
2024-07-29 12:22:40,348 - Create_spark - INFO - master is Yarn
2024-07-29 12:22:43,383 - root - ERROR - An error occurred when calling main() please check the trace=== Java gateway process exited before sending its port number
2024-07-29 12:24:08,826 - root - INFO - i am in the main method..
2024-07-29 12:24:08,826 - root - INFO - calling spark object
2024-07-29 12:24:08,827 - Create_spark - INFO - get_spark_object method started
2024-07-29 12:24:08,827 - Create_spark - INFO - master is local
2024-07-29 12:24:16,768 - Create_spark - INFO - Spark object created.....
2024-07-29 12:24:16,768 - root - INFO - Validating spark object..........
2024-07-29 12:24:16,769 - Validate - WARNING - started the get_current_date method...
2024-07-29 12:24:20,824 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 29))]
2024-07-29 12:24:20,824 - Validate - WARNING - Validation done , go frwd...
2024-07-29 12:24:20,825 - root - INFO - Application done
2024-07-29 13:12:49,833 - root - INFO - i am in the main method..
2024-07-29 13:12:49,834 - root - INFO - calling spark object
2024-07-29 13:12:49,834 - Create_spark - INFO - get_spark_object method started
2024-07-29 13:12:49,834 - Create_spark - INFO - master is local
2024-07-29 13:12:57,457 - Create_spark - INFO - Spark object created.....
2024-07-29 13:12:57,457 - root - INFO - Validating spark object..........
2024-07-29 13:12:57,458 - Validate - WARNING - started the get_current_date method...
2024-07-29 13:13:01,994 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 29))]
2024-07-29 13:13:01,994 - Validate - WARNING - Validation done , go frwd...
2024-07-29 13:13:01,995 - root - INFO - Application done
2024-07-29 13:31:00,486 - root - INFO - i am in the main method..
2024-07-29 13:31:00,487 - root - INFO - calling spark object
2024-07-29 13:31:00,487 - Create_spark - INFO - get_spark_object method started
2024-07-29 13:31:00,487 - Create_spark - INFO - master is local
2024-07-29 13:31:07,803 - Create_spark - INFO - Spark object created.....
2024-07-29 13:31:07,803 - root - INFO - Validating spark object..........
2024-07-29 13:31:07,803 - Validate - WARNING - started the get_current_date method...
2024-07-29 13:31:12,293 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 29))]
2024-07-29 13:31:12,293 - Validate - WARNING - Validation done , go frwd...
2024-07-29 13:31:12,294 - root - INFO - reading file which is of type parquet
2024-07-29 13:31:12,294 - root - INFO - Application done
2024-07-29 13:31:45,247 - root - INFO - i am in the main method..
2024-07-29 13:31:45,247 - root - INFO - calling spark object
2024-07-29 13:31:45,247 - Create_spark - INFO - get_spark_object method started
2024-07-29 13:31:45,247 - Create_spark - INFO - master is local
2024-07-29 13:31:52,077 - Create_spark - INFO - Spark object created.....
2024-07-29 13:31:52,077 - root - INFO - Validating spark object..........
2024-07-29 13:31:52,077 - Validate - WARNING - started the get_current_date method...
2024-07-29 13:31:57,679 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 29))]
2024-07-29 13:31:57,680 - Validate - WARNING - Validation done , go frwd...
2024-07-29 13:31:57,681 - root - INFO - reading file which is of type parquet
2024-07-29 13:31:57,681 - root - INFO - Application done
2024-07-29 14:40:26,112 - root - INFO - i am in the main method..
2024-07-29 14:40:26,113 - root - INFO - calling spark object
2024-07-29 14:40:26,113 - Create_spark - INFO - get_spark_object method started
2024-07-29 14:40:26,113 - Create_spark - INFO - master is local
2024-07-29 14:40:35,839 - Create_spark - INFO - Spark object created.....
2024-07-29 14:40:35,839 - root - INFO - Validating spark object..........
2024-07-29 14:40:35,839 - Validate - WARNING - started the get_current_date method...
2024-07-29 14:40:41,215 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 29))]
2024-07-29 14:40:41,215 - Validate - WARNING - Validation done , go frwd...
2024-07-29 14:40:41,216 - root - INFO - reading file which is of type parquet
2024-07-29 14:40:41,216 - root - ERROR - An error occurred when calling main() please check the trace=== load_files() missing 1 required positional argument: 'file_dir'
2024-07-29 14:45:44,293 - root - INFO - i am in the main method..
2024-07-29 14:45:44,315 - root - INFO - calling spark object
2024-07-29 14:45:44,315 - Create_spark - INFO - get_spark_object method started
2024-07-29 14:45:44,315 - Create_spark - INFO - master is local
2024-07-29 14:45:54,365 - Create_spark - INFO - Spark object created.....
2024-07-29 14:45:54,367 - root - INFO - Validating spark object..........
2024-07-29 14:45:54,367 - Validate - WARNING - started the get_current_date method...
2024-07-29 14:45:59,937 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 29))]
2024-07-29 14:45:59,938 - Validate - WARNING - Validation done , go frwd...
2024-07-29 14:45:59,940 - root - INFO - reading file which is of type parquet
2024-07-29 14:46:02,413 - root - INFO - Application done
2024-07-29 14:56:25,023 - root - INFO - i am in the main method..
2024-07-29 14:56:25,023 - root - INFO - calling spark object
2024-07-29 14:56:25,023 - Create_spark - INFO - get_spark_object method started
2024-07-29 14:56:25,023 - Create_spark - INFO - master is local
2024-07-29 14:56:34,541 - Create_spark - INFO - Spark object created.....
2024-07-29 14:56:34,541 - root - INFO - Validating spark object..........
2024-07-29 14:56:34,541 - Validate - WARNING - started the get_current_date method...
2024-07-29 14:56:40,017 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 29))]
2024-07-29 14:56:40,017 - Validate - WARNING - Validation done , go frwd...
2024-07-29 14:56:40,018 - root - INFO - reading file which is of type parquet
2024-07-29 14:56:42,620 - root - INFO - Application done
2024-07-29 18:26:42,663 - root - INFO - i am in the main method..
2024-07-29 18:26:42,663 - root - INFO - calling spark object
2024-07-29 18:26:42,663 - Create_spark - INFO - get_spark_object method started
2024-07-29 18:26:42,663 - Create_spark - INFO - master is local
2024-07-29 18:26:53,079 - Create_spark - INFO - Spark object created.....
2024-07-29 18:26:53,079 - root - INFO - Validating spark object..........
2024-07-29 18:26:53,079 - Validate - WARNING - started the get_current_date method...
2024-07-29 18:26:58,025 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 29))]
2024-07-29 18:26:58,025 - Validate - WARNING - Validation done , go frwd...
2024-07-29 18:26:58,027 - root - INFO - reading file which is of type parquet
2024-07-29 18:26:59,171 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-07-29 18:27:01,548 - root - INFO - Application done
2024-07-29 18:28:13,720 - root - INFO - i am in the main method..
2024-07-29 18:28:13,721 - root - INFO - calling spark object
2024-07-29 18:28:13,721 - Create_spark - INFO - get_spark_object method started
2024-07-29 18:28:13,721 - Create_spark - INFO - master is local
2024-07-29 18:28:23,340 - Create_spark - INFO - Spark object created.....
2024-07-29 18:28:23,341 - root - INFO - Validating spark object..........
2024-07-29 18:28:23,342 - Validate - WARNING - started the get_current_date method...
2024-07-29 18:28:28,698 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 29))]
2024-07-29 18:28:28,698 - Validate - WARNING - Validation done , go frwd...
2024-07-29 18:28:28,699 - root - INFO - reading file which is of type parquet
2024-07-29 18:28:29,889 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-07-29 18:28:32,197 - root - INFO - Application done
2024-07-29 18:31:49,971 - root - INFO - i am in the main method..
2024-07-29 18:31:49,972 - root - INFO - calling spark object
2024-07-29 18:31:49,972 - Create_spark - INFO - get_spark_object method started
2024-07-29 18:31:49,972 - Create_spark - INFO - master is local
2024-07-29 18:31:59,401 - Create_spark - INFO - Spark object created.....
2024-07-29 18:31:59,401 - root - INFO - Validating spark object..........
2024-07-29 18:31:59,402 - Validate - WARNING - started the get_current_date method...
2024-07-29 18:32:04,783 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 29))]
2024-07-29 18:32:04,783 - Validate - WARNING - Validation done , go frwd...
2024-07-29 18:32:04,784 - root - INFO - reading file which is of type parquet
2024-07-29 18:32:05,879 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-07-29 18:32:08,015 - root - INFO - Application done
2024-07-29 18:34:51,358 - root - INFO - i am in the main method..
2024-07-29 18:34:51,359 - root - INFO - calling spark object
2024-07-29 18:34:51,359 - Create_spark - INFO - get_spark_object method started
2024-07-29 18:34:51,359 - Create_spark - INFO - master is local
2024-07-29 18:35:01,322 - Create_spark - INFO - Spark object created.....
2024-07-29 18:35:01,323 - root - INFO - Validating spark object..........
2024-07-29 18:35:01,323 - Validate - WARNING - started the get_current_date method...
2024-07-29 18:35:07,694 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 29))]
2024-07-29 18:35:07,695 - Validate - WARNING - Validation done , go frwd...
2024-07-29 18:35:07,696 - root - INFO - reading file which is of type parquet
2024-07-29 18:35:08,912 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-07-29 18:35:11,159 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-07-29 18:35:11,159 - root - INFO - Application done
2024-07-29 18:49:34,294 - root - INFO - i am in the main method..
2024-07-29 18:49:34,294 - root - INFO - calling spark object
2024-07-29 18:49:34,294 - Create_spark - INFO - get_spark_object method started
2024-07-29 18:49:34,294 - Create_spark - INFO - master is local
2024-07-29 18:49:44,110 - Create_spark - INFO - Spark object created.....
2024-07-29 18:49:44,110 - root - INFO - Validating spark object..........
2024-07-29 18:49:44,110 - Validate - WARNING - started the get_current_date method...
2024-07-29 18:49:50,115 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 29))]
2024-07-29 18:49:50,115 - Validate - WARNING - Validation done , go frwd...
2024-07-29 18:49:50,117 - root - INFO - reading file which is of type parquet
2024-07-29 18:49:50,118 - root - INFO - reading file which is of type csv
2024-07-29 18:49:50,150 - Ingest - ERROR - An error occured while loading files: options() takes 1 positional argument but 3 were given
2024-07-29 18:49:50,151 - root - ERROR - An error occurred when calling main() please check the trace=== options() takes 1 positional argument but 3 were given
2024-07-29 19:03:15,139 - root - INFO - i am in the main method..
2024-07-29 19:03:15,139 - root - INFO - calling spark object
2024-07-29 19:03:15,140 - Create_spark - INFO - get_spark_object method started
2024-07-29 19:03:15,140 - Create_spark - INFO - master is local
2024-07-29 19:03:24,555 - Create_spark - INFO - Spark object created.....
2024-07-29 19:03:24,555 - root - INFO - Validating spark object..........
2024-07-29 19:03:24,556 - Validate - WARNING - started the get_current_date method...
2024-07-29 19:03:29,955 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 29))]
2024-07-29 19:03:29,956 - Validate - WARNING - Validation done , go frwd...
2024-07-29 19:03:29,956 - root - INFO - reading file which is of type parquet
2024-07-29 19:03:29,957 - root - INFO - reading file which is of type csv
2024-07-29 19:03:29,979 - Ingest - ERROR - An error occured while loading files: options() takes 1 positional argument but 3 were given
2024-07-29 19:03:29,979 - root - ERROR - An error occurred when calling main() please check the trace=== options() takes 1 positional argument but 3 were given
2024-07-29 19:16:26,235 - root - INFO - i am in the main method..
2024-07-29 19:16:26,236 - root - INFO - calling spark object
2024-07-29 19:16:26,236 - Create_spark - INFO - get_spark_object method started
2024-07-29 19:16:26,236 - Create_spark - INFO - master is local
2024-07-29 19:16:35,879 - Create_spark - INFO - Spark object created.....
2024-07-29 19:16:35,879 - root - INFO - Validating spark object..........
2024-07-29 19:16:35,880 - Validate - WARNING - started the get_current_date method...
2024-07-29 19:16:41,565 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 29))]
2024-07-29 19:16:41,566 - Validate - WARNING - Validation done , go frwd...
2024-07-29 19:16:41,567 - root - INFO - reading file which is of type parquet
2024-07-29 19:16:41,568 - root - INFO - reading file which is of type csv
2024-07-29 19:16:41,592 - Ingest - ERROR - An error occured while loading files: options() takes 1 positional argument but 3 were given
2024-07-29 19:16:41,593 - root - ERROR - An error occurred when calling main() please check the trace=== options() takes 1 positional argument but 3 were given
2024-07-29 19:30:34,185 - root - INFO - i am in the main method..
2024-07-29 19:30:34,185 - root - INFO - calling spark object
2024-07-29 19:30:34,185 - Create_spark - INFO - get_spark_object method started
2024-07-29 19:30:34,185 - Create_spark - INFO - master is local
2024-07-29 19:30:43,759 - Create_spark - INFO - Spark object created.....
2024-07-29 19:30:43,760 - root - INFO - Validating spark object..........
2024-07-29 19:30:43,760 - Validate - WARNING - started the get_current_date method...
2024-07-29 19:30:49,042 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 29))]
2024-07-29 19:30:49,042 - Validate - WARNING - Validation done , go frwd...
2024-07-29 19:30:49,043 - root - INFO - reading file which is of type parquet
2024-07-29 19:30:50,288 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-07-29 19:30:52,363 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-07-29 19:30:52,364 - root - INFO - reading file which is of type csv
2024-07-29 19:30:52,364 - root - ERROR - An error occurred when calling main() please check the trace=== name 'file_dir2' is not defined
2024-07-29 19:31:24,287 - root - INFO - i am in the main method..
2024-07-29 19:31:24,288 - root - INFO - calling spark object
2024-07-29 19:31:24,288 - Create_spark - INFO - get_spark_object method started
2024-07-29 19:31:24,288 - Create_spark - INFO - master is local
2024-07-29 19:31:34,298 - Create_spark - INFO - Spark object created.....
2024-07-29 19:31:34,298 - root - INFO - Validating spark object..........
2024-07-29 19:31:34,298 - Validate - WARNING - started the get_current_date method...
2024-07-29 19:31:39,753 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 29))]
2024-07-29 19:31:39,753 - Validate - WARNING - Validation done , go frwd...
2024-07-29 19:31:39,755 - root - INFO - reading file which is of type parquet
2024-07-29 19:31:40,876 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-07-29 19:31:43,011 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-07-29 19:31:43,012 - root - INFO - reading file which is of type csv
2024-07-29 19:31:43,014 - Ingest - ERROR - An error occured while loading files: options() takes 1 positional argument but 3 were given
2024-07-29 19:31:43,014 - root - ERROR - An error occurred when calling main() please check the trace=== options() takes 1 positional argument but 3 were given
2024-07-29 19:32:52,967 - root - INFO - i am in the main method..
2024-07-29 19:32:52,967 - root - INFO - calling spark object
2024-07-29 19:32:52,967 - Create_spark - INFO - get_spark_object method started
2024-07-29 19:32:52,967 - Create_spark - INFO - master is local
2024-07-29 19:33:01,942 - Create_spark - INFO - Spark object created.....
2024-07-29 19:33:01,942 - root - INFO - Validating spark object..........
2024-07-29 19:33:01,942 - Validate - WARNING - started the get_current_date method...
2024-07-29 19:33:07,379 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 29))]
2024-07-29 19:33:07,379 - Validate - WARNING - Validation done , go frwd...
2024-07-29 19:33:07,380 - root - INFO - reading file which is of type parquet
2024-07-29 19:33:08,376 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-07-29 19:33:10,746 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-07-29 19:33:10,748 - root - INFO - reading file which is of type csv
2024-07-29 19:33:10,750 - Ingest - ERROR - An error occured while loading files: options() takes 1 positional argument but 3 were given
2024-07-29 19:33:10,750 - root - ERROR - An error occurred when calling main() please check the trace=== options() takes 1 positional argument but 3 were given
2024-07-29 19:33:42,460 - root - INFO - i am in the main method..
2024-07-29 19:33:42,460 - root - INFO - calling spark object
2024-07-29 19:33:42,460 - Create_spark - INFO - get_spark_object method started
2024-07-29 19:33:42,460 - Create_spark - INFO - master is local
2024-07-29 19:33:52,950 - Create_spark - INFO - Spark object created.....
2024-07-29 19:33:52,950 - root - INFO - Validating spark object..........
2024-07-29 19:33:52,950 - Validate - WARNING - started the get_current_date method...
2024-07-29 19:33:58,163 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 29))]
2024-07-29 19:33:58,163 - Validate - WARNING - Validation done , go frwd...
2024-07-29 19:33:58,165 - root - INFO - reading file which is of type parquet
2024-07-29 19:33:59,313 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-07-29 19:34:02,345 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-07-29 19:34:02,348 - root - INFO - reading file which is of type csv
2024-07-29 19:34:02,350 - Ingest - ERROR - An error occured while loading files: options() takes 1 positional argument but 3 were given
2024-07-29 19:34:02,351 - root - ERROR - An error occurred when calling main() please check the trace=== options() takes 1 positional argument but 3 were given
2024-07-29 19:36:20,471 - root - INFO - i am in the main method..
2024-07-29 19:36:20,472 - root - INFO - calling spark object
2024-07-29 19:36:20,472 - Create_spark - INFO - get_spark_object method started
2024-07-29 19:36:20,472 - Create_spark - INFO - master is local
2024-07-29 19:36:30,132 - Create_spark - INFO - Spark object created.....
2024-07-29 19:36:30,133 - root - INFO - Validating spark object..........
2024-07-29 19:36:30,133 - Validate - WARNING - started the get_current_date method...
2024-07-29 19:36:35,746 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 29))]
2024-07-29 19:36:35,747 - Validate - WARNING - Validation done , go frwd...
2024-07-29 19:36:35,747 - root - INFO - reading file which is of type parquet
2024-07-29 19:36:36,933 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-07-29 19:36:39,477 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-07-29 19:36:39,478 - root - INFO - reading file which is of type csv
2024-07-29 19:36:39,540 - Ingest - ERROR - An error occured while loading files: An error occurred while calling o41.load.
: java.lang.Exception: header flag can be true or false
	at org.apache.spark.sql.errors.QueryExecutionErrors$.paramIsNotBooleanValueError(QueryExecutionErrors.scala:1029)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.getBool(CSVOptions.scala:96)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.<init>(CSVOptions.scala:118)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.<init>(CSVOptions.scala:47)
	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:60)
	at org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:210)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:207)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:411)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:274)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:245)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:245)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:188)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)

2024-07-29 19:36:39,542 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o41.load.
: java.lang.Exception: header flag can be true or false
	at org.apache.spark.sql.errors.QueryExecutionErrors$.paramIsNotBooleanValueError(QueryExecutionErrors.scala:1029)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.getBool(CSVOptions.scala:96)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.<init>(CSVOptions.scala:118)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.<init>(CSVOptions.scala:47)
	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:60)
	at org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:210)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:207)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:411)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:274)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:245)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:245)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:188)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)

2024-07-29 19:37:44,560 - root - INFO - i am in the main method..
2024-07-29 19:37:44,561 - root - INFO - calling spark object
2024-07-29 19:37:44,561 - Create_spark - INFO - get_spark_object method started
2024-07-29 19:37:44,561 - Create_spark - INFO - master is local
2024-07-29 19:37:56,109 - Create_spark - INFO - Spark object created.....
2024-07-29 19:37:56,110 - root - INFO - Validating spark object..........
2024-07-29 19:37:56,111 - Validate - WARNING - started the get_current_date method...
2024-07-29 19:38:01,353 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 29))]
2024-07-29 19:38:01,353 - Validate - WARNING - Validation done , go frwd...
2024-07-29 19:38:01,354 - root - INFO - reading file which is of type parquet
2024-07-29 19:38:02,490 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-07-29 19:38:05,012 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-07-29 19:38:05,014 - root - INFO - reading file which is of type csv
2024-07-29 19:38:05,052 - Ingest - ERROR - An error occured while loading files: An error occurred while calling o41.load.
: java.lang.Exception: header flag can be true or false
	at org.apache.spark.sql.errors.QueryExecutionErrors$.paramIsNotBooleanValueError(QueryExecutionErrors.scala:1029)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.getBool(CSVOptions.scala:96)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.<init>(CSVOptions.scala:118)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.<init>(CSVOptions.scala:47)
	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:60)
	at org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:210)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:207)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:411)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:274)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:245)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:245)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:188)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)

2024-07-29 19:38:05,053 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o41.load.
: java.lang.Exception: header flag can be true or false
	at org.apache.spark.sql.errors.QueryExecutionErrors$.paramIsNotBooleanValueError(QueryExecutionErrors.scala:1029)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.getBool(CSVOptions.scala:96)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.<init>(CSVOptions.scala:118)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.<init>(CSVOptions.scala:47)
	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:60)
	at org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:210)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:207)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:411)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:274)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:245)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:245)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:188)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)

2024-07-29 19:39:37,113 - root - INFO - i am in the main method..
2024-07-29 19:39:37,113 - root - INFO - calling spark object
2024-07-29 19:39:37,113 - Create_spark - INFO - get_spark_object method started
2024-07-29 19:39:37,113 - Create_spark - INFO - master is local
2024-07-29 19:39:46,869 - Create_spark - INFO - Spark object created.....
2024-07-29 19:39:46,869 - root - INFO - Validating spark object..........
2024-07-29 19:39:46,869 - Validate - WARNING - started the get_current_date method...
2024-07-29 19:39:52,276 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 29))]
2024-07-29 19:39:52,276 - Validate - WARNING - Validation done , go frwd...
2024-07-29 19:39:52,277 - root - INFO - reading file which is of type parquet
2024-07-29 19:39:53,445 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-07-29 19:39:55,554 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-07-29 19:39:55,555 - root - INFO - reading file which is of type csv
2024-07-29 19:39:55,585 - Ingest - ERROR - An error occured while loading files: An error occurred while calling o41.load.
: java.lang.Exception: header flag can be true or false
	at org.apache.spark.sql.errors.QueryExecutionErrors$.paramIsNotBooleanValueError(QueryExecutionErrors.scala:1029)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.getBool(CSVOptions.scala:96)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.<init>(CSVOptions.scala:118)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.<init>(CSVOptions.scala:47)
	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:60)
	at org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:210)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:207)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:411)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:274)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:245)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:245)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:188)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)

2024-07-29 19:39:55,585 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o41.load.
: java.lang.Exception: header flag can be true or false
	at org.apache.spark.sql.errors.QueryExecutionErrors$.paramIsNotBooleanValueError(QueryExecutionErrors.scala:1029)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.getBool(CSVOptions.scala:96)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.<init>(CSVOptions.scala:118)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.<init>(CSVOptions.scala:47)
	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:60)
	at org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:210)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:207)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:411)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:274)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:245)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:245)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:188)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)

2024-07-29 19:41:20,836 - root - INFO - i am in the main method..
2024-07-29 19:41:20,836 - root - INFO - calling spark object
2024-07-29 19:41:20,836 - Create_spark - INFO - get_spark_object method started
2024-07-29 19:41:20,837 - Create_spark - INFO - master is local
2024-07-29 19:41:30,072 - Create_spark - INFO - Spark object created.....
2024-07-29 19:41:30,073 - root - INFO - Validating spark object..........
2024-07-29 19:41:30,073 - Validate - WARNING - started the get_current_date method...
2024-07-29 19:41:35,374 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 29))]
2024-07-29 19:41:35,374 - Validate - WARNING - Validation done , go frwd...
2024-07-29 19:41:35,376 - root - INFO - reading file which is of type parquet
2024-07-29 19:41:36,465 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-07-29 19:41:38,659 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-07-29 19:41:38,660 - root - INFO - reading file which is of type csv
2024-07-29 19:41:38,695 - Ingest - ERROR - An error occured while loading files: An error occurred while calling o41.load.
: java.lang.Exception: header flag can be true or false
	at org.apache.spark.sql.errors.QueryExecutionErrors$.paramIsNotBooleanValueError(QueryExecutionErrors.scala:1029)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.getBool(CSVOptions.scala:96)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.<init>(CSVOptions.scala:118)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.<init>(CSVOptions.scala:47)
	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:60)
	at org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:210)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:207)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:411)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:274)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:245)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:245)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:188)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)

2024-07-29 19:41:38,696 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o41.load.
: java.lang.Exception: header flag can be true or false
	at org.apache.spark.sql.errors.QueryExecutionErrors$.paramIsNotBooleanValueError(QueryExecutionErrors.scala:1029)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.getBool(CSVOptions.scala:96)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.<init>(CSVOptions.scala:118)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.<init>(CSVOptions.scala:47)
	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:60)
	at org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:210)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:207)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:411)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:274)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:245)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:245)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:188)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)

2024-07-29 19:45:43,294 - root - INFO - i am in the main method..
2024-07-29 19:45:43,294 - root - INFO - calling spark object
2024-07-29 19:45:43,294 - Create_spark - INFO - get_spark_object method started
2024-07-29 19:45:43,294 - Create_spark - INFO - master is local
2024-07-29 19:45:54,768 - Create_spark - INFO - Spark object created.....
2024-07-29 19:45:54,768 - root - INFO - Validating spark object..........
2024-07-29 19:45:54,768 - Validate - WARNING - started the get_current_date method...
2024-07-29 19:46:01,928 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 29))]
2024-07-29 19:46:01,928 - Validate - WARNING - Validation done , go frwd...
2024-07-29 19:46:01,932 - root - INFO - reading file which is of type parquet
2024-07-29 19:46:03,374 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-07-29 19:46:05,726 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-07-29 19:46:05,727 - root - INFO - reading file which is of type csv
2024-07-29 19:46:05,755 - Ingest - ERROR - An error occured while loading files: An error occurred while calling o41.load.
: java.lang.Exception: header flag can be true or false
	at org.apache.spark.sql.errors.QueryExecutionErrors$.paramIsNotBooleanValueError(QueryExecutionErrors.scala:1029)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.getBool(CSVOptions.scala:96)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.<init>(CSVOptions.scala:118)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.<init>(CSVOptions.scala:47)
	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:60)
	at org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:210)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:207)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:411)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:274)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:245)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:245)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:188)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)

2024-07-29 19:46:05,756 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o41.load.
: java.lang.Exception: header flag can be true or false
	at org.apache.spark.sql.errors.QueryExecutionErrors$.paramIsNotBooleanValueError(QueryExecutionErrors.scala:1029)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.getBool(CSVOptions.scala:96)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.<init>(CSVOptions.scala:118)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.<init>(CSVOptions.scala:47)
	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:60)
	at org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:210)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:207)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:411)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:274)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:245)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:245)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:188)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)

2024-07-29 19:47:21,293 - root - INFO - i am in the main method..
2024-07-29 19:47:21,293 - root - INFO - calling spark object
2024-07-29 19:47:21,293 - Create_spark - INFO - get_spark_object method started
2024-07-29 19:47:21,293 - Create_spark - INFO - master is local
2024-07-29 19:47:32,678 - Create_spark - INFO - Spark object created.....
2024-07-29 19:47:32,678 - root - INFO - Validating spark object..........
2024-07-29 19:47:32,678 - Validate - WARNING - started the get_current_date method...
2024-07-29 19:47:38,194 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 29))]
2024-07-29 19:47:38,194 - Validate - WARNING - Validation done , go frwd...
2024-07-29 19:47:38,195 - root - INFO - reading file which is of type parquet
2024-07-29 19:47:39,490 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-07-29 19:47:42,436 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-07-29 19:47:42,436 - root - INFO - reading file which is of type csv
2024-07-29 19:47:42,473 - Ingest - ERROR - An error occured while loading files: An error occurred while calling o41.load.
: java.lang.Exception: header flag can be true or false
	at org.apache.spark.sql.errors.QueryExecutionErrors$.paramIsNotBooleanValueError(QueryExecutionErrors.scala:1029)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.getBool(CSVOptions.scala:96)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.<init>(CSVOptions.scala:118)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.<init>(CSVOptions.scala:47)
	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:60)
	at org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:210)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:207)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:411)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:274)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:245)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:245)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:188)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)

2024-07-29 19:47:42,474 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o41.load.
: java.lang.Exception: header flag can be true or false
	at org.apache.spark.sql.errors.QueryExecutionErrors$.paramIsNotBooleanValueError(QueryExecutionErrors.scala:1029)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.getBool(CSVOptions.scala:96)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.<init>(CSVOptions.scala:118)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.<init>(CSVOptions.scala:47)
	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:60)
	at org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:210)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:207)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:411)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:274)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:245)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:245)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:188)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)

2024-07-29 19:48:15,275 - root - INFO - i am in the main method..
2024-07-29 19:48:15,275 - root - INFO - calling spark object
2024-07-29 19:48:15,275 - Create_spark - INFO - get_spark_object method started
2024-07-29 19:48:15,276 - Create_spark - INFO - master is local
2024-07-29 19:48:27,103 - Create_spark - INFO - Spark object created.....
2024-07-29 19:48:27,103 - root - INFO - Validating spark object..........
2024-07-29 19:48:27,104 - Validate - WARNING - started the get_current_date method...
2024-07-29 19:48:32,480 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 29))]
2024-07-29 19:48:32,481 - Validate - WARNING - Validation done , go frwd...
2024-07-29 19:48:32,481 - root - INFO - reading file which is of type parquet
2024-07-29 19:48:33,588 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-07-29 19:48:36,138 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-07-29 19:48:36,140 - root - INFO - reading file which is of type csv
2024-07-29 19:48:36,184 - Ingest - ERROR - An error occured while loading files: An error occurred while calling o41.load.
: java.lang.Exception: header flag can be true or false
	at org.apache.spark.sql.errors.QueryExecutionErrors$.paramIsNotBooleanValueError(QueryExecutionErrors.scala:1029)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.getBool(CSVOptions.scala:96)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.<init>(CSVOptions.scala:118)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.<init>(CSVOptions.scala:47)
	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:60)
	at org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:210)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:207)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:411)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:274)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:245)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:245)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:188)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)

2024-07-29 19:48:36,185 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o41.load.
: java.lang.Exception: header flag can be true or false
	at org.apache.spark.sql.errors.QueryExecutionErrors$.paramIsNotBooleanValueError(QueryExecutionErrors.scala:1029)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.getBool(CSVOptions.scala:96)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.<init>(CSVOptions.scala:118)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.<init>(CSVOptions.scala:47)
	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:60)
	at org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:210)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:207)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:411)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:274)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:245)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:245)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:188)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)

2024-07-29 19:49:50,577 - root - INFO - i am in the main method..
2024-07-29 19:49:50,577 - root - INFO - calling spark object
2024-07-29 19:49:50,577 - Create_spark - INFO - get_spark_object method started
2024-07-29 19:49:50,578 - Create_spark - INFO - master is local
2024-07-29 19:50:00,055 - Create_spark - INFO - Spark object created.....
2024-07-29 19:50:00,055 - root - INFO - Validating spark object..........
2024-07-29 19:50:00,056 - Validate - WARNING - started the get_current_date method...
2024-07-29 19:50:05,349 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 29))]
2024-07-29 19:50:05,349 - Validate - WARNING - Validation done , go frwd...
2024-07-29 19:50:05,350 - root - INFO - reading file which is of type parquet
2024-07-29 19:50:06,751 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-07-29 19:50:08,721 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-07-29 19:50:08,722 - root - INFO - reading file which is of type csv
2024-07-29 19:50:08,762 - Ingest - ERROR - An error occured while loading files: An error occurred while calling o41.load.
: java.lang.Exception: header flag can be true or false
	at org.apache.spark.sql.errors.QueryExecutionErrors$.paramIsNotBooleanValueError(QueryExecutionErrors.scala:1029)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.getBool(CSVOptions.scala:96)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.<init>(CSVOptions.scala:118)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.<init>(CSVOptions.scala:47)
	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:60)
	at org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:210)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:207)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:411)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:274)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:245)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:245)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:188)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)

2024-07-29 19:50:08,763 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o41.load.
: java.lang.Exception: header flag can be true or false
	at org.apache.spark.sql.errors.QueryExecutionErrors$.paramIsNotBooleanValueError(QueryExecutionErrors.scala:1029)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.getBool(CSVOptions.scala:96)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.<init>(CSVOptions.scala:118)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.<init>(CSVOptions.scala:47)
	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:60)
	at org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:210)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:207)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:411)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:274)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:245)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:245)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:188)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)

2024-07-29 22:34:02,107 - root - INFO - i am in the main method..
2024-07-29 22:34:02,107 - root - INFO - calling spark object
2024-07-29 22:34:02,107 - Create_spark - INFO - get_spark_object method started
2024-07-29 22:34:02,107 - Create_spark - INFO - master is local
2024-07-29 22:34:11,469 - Create_spark - INFO - Spark object created.....
2024-07-29 22:34:11,469 - root - INFO - Validating spark object..........
2024-07-29 22:34:11,469 - Validate - WARNING - started the get_current_date method...
2024-07-29 22:34:16,677 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 29))]
2024-07-29 22:34:16,678 - Validate - WARNING - Validation done , go frwd...
2024-07-29 22:34:16,679 - root - INFO - reading file which is of type parquet
2024-07-29 22:34:17,783 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-07-29 22:34:20,069 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-07-29 22:34:20,070 - root - INFO - reading file which is of type csv
2024-07-29 22:34:20,116 - Ingest - ERROR - An error occured while loading files: An error occurred while calling o41.load.
: java.lang.Exception: header flag can be true or false
	at org.apache.spark.sql.errors.QueryExecutionErrors$.paramIsNotBooleanValueError(QueryExecutionErrors.scala:1029)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.getBool(CSVOptions.scala:96)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.<init>(CSVOptions.scala:118)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.<init>(CSVOptions.scala:47)
	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:60)
	at org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:210)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:207)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:411)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:274)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:245)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:245)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:188)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)

2024-07-29 22:34:20,117 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o41.load.
: java.lang.Exception: header flag can be true or false
	at org.apache.spark.sql.errors.QueryExecutionErrors$.paramIsNotBooleanValueError(QueryExecutionErrors.scala:1029)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.getBool(CSVOptions.scala:96)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.<init>(CSVOptions.scala:118)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.<init>(CSVOptions.scala:47)
	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:60)
	at org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:210)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:207)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:411)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:274)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:245)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:245)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:188)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)

2024-07-29 22:38:40,044 - root - INFO - i am in the main method..
2024-07-29 22:38:40,045 - root - INFO - calling spark object
2024-07-29 22:38:40,045 - Create_spark - INFO - get_spark_object method started
2024-07-29 22:38:40,045 - Create_spark - INFO - master is local
2024-07-29 22:38:49,042 - Create_spark - INFO - Spark object created.....
2024-07-29 22:38:49,043 - root - INFO - Validating spark object..........
2024-07-29 22:38:49,043 - Validate - WARNING - started the get_current_date method...
2024-07-29 22:38:54,342 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 29))]
2024-07-29 22:38:54,342 - Validate - WARNING - Validation done , go frwd...
2024-07-29 22:38:54,344 - root - INFO - reading file which is of type parquet
2024-07-29 22:38:55,343 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-07-29 22:38:57,480 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-07-29 22:38:57,481 - root - INFO - reading file which is of type csv
2024-07-29 22:38:57,526 - Ingest - ERROR - An error occured while loading files: An error occurred while calling o41.load.
: java.lang.Exception: header flag can be true or false
	at org.apache.spark.sql.errors.QueryExecutionErrors$.paramIsNotBooleanValueError(QueryExecutionErrors.scala:1029)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.getBool(CSVOptions.scala:96)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.<init>(CSVOptions.scala:118)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.<init>(CSVOptions.scala:47)
	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:60)
	at org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:210)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:207)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:411)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:274)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:245)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:245)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:188)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)

2024-07-29 22:38:57,528 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o41.load.
: java.lang.Exception: header flag can be true or false
	at org.apache.spark.sql.errors.QueryExecutionErrors$.paramIsNotBooleanValueError(QueryExecutionErrors.scala:1029)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.getBool(CSVOptions.scala:96)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.<init>(CSVOptions.scala:118)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.<init>(CSVOptions.scala:47)
	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:60)
	at org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:210)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:207)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:411)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:274)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:245)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:245)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:188)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)

2024-07-30 09:41:01,195 - root - INFO - i am in the main method..
2024-07-30 09:41:01,196 - root - INFO - calling spark object
2024-07-30 09:41:01,196 - Create_spark - INFO - get_spark_object method started
2024-07-30 09:41:01,196 - Create_spark - INFO - master is local
2024-07-30 09:41:14,980 - Create_spark - INFO - Spark object created.....
2024-07-30 09:41:14,980 - root - INFO - Validating spark object..........
2024-07-30 09:41:14,980 - Validate - WARNING - started the get_current_date method...
2024-07-30 09:41:21,427 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 30))]
2024-07-30 09:41:21,428 - Validate - WARNING - Validation done , go frwd...
2024-07-30 09:41:21,428 - root - INFO - reading file which is of type parquet
2024-07-30 09:41:23,024 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-07-30 09:41:25,767 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-07-30 09:41:25,767 - root - INFO - reading file which is of type csv
2024-07-30 09:41:25,807 - Ingest - ERROR - An error occured while loading files: An error occurred while calling o41.load.
: java.lang.Exception: header flag can be true or false
	at org.apache.spark.sql.errors.QueryExecutionErrors$.paramIsNotBooleanValueError(QueryExecutionErrors.scala:1029)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.getBool(CSVOptions.scala:96)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.<init>(CSVOptions.scala:118)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.<init>(CSVOptions.scala:47)
	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:60)
	at org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:210)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:207)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:411)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:274)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:245)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:245)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:188)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)

2024-07-30 09:41:25,808 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o41.load.
: java.lang.Exception: header flag can be true or false
	at org.apache.spark.sql.errors.QueryExecutionErrors$.paramIsNotBooleanValueError(QueryExecutionErrors.scala:1029)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.getBool(CSVOptions.scala:96)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.<init>(CSVOptions.scala:118)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.<init>(CSVOptions.scala:47)
	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:60)
	at org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:210)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:207)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:411)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:274)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:245)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:245)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:188)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)

2024-07-30 11:03:20,561 - root - INFO - i am in the main method..
2024-07-30 11:03:20,562 - root - INFO - calling spark object
2024-07-30 11:03:20,562 - Create_spark - INFO - get_spark_object method started
2024-07-30 11:03:20,562 - Create_spark - INFO - master is local
2024-07-30 11:03:33,536 - Create_spark - INFO - Spark object created.....
2024-07-30 11:03:33,537 - root - INFO - Validating spark object..........
2024-07-30 11:03:33,537 - Validate - WARNING - started the get_current_date method...
2024-07-30 11:03:40,196 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 30))]
2024-07-30 11:03:40,197 - Validate - WARNING - Validation done , go frwd...
2024-07-30 11:03:40,198 - root - INFO - reading file which is of type parquet
2024-07-30 11:03:41,572 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-07-30 11:03:44,790 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-07-30 11:03:44,791 - root - INFO - reading file which is of type csv
2024-07-30 11:03:44,839 - Ingest - ERROR - An error occured while loading files: An error occurred while calling o41.load.
: java.lang.Exception: header flag can be true or false
	at org.apache.spark.sql.errors.QueryExecutionErrors$.paramIsNotBooleanValueError(QueryExecutionErrors.scala:1029)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.getBool(CSVOptions.scala:96)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.<init>(CSVOptions.scala:118)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.<init>(CSVOptions.scala:47)
	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:60)
	at org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:210)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:207)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:411)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:274)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:245)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:245)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:188)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)

2024-07-30 11:03:44,840 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o41.load.
: java.lang.Exception: header flag can be true or false
	at org.apache.spark.sql.errors.QueryExecutionErrors$.paramIsNotBooleanValueError(QueryExecutionErrors.scala:1029)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.getBool(CSVOptions.scala:96)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.<init>(CSVOptions.scala:118)
	at org.apache.spark.sql.catalyst.csv.CSVOptions.<init>(CSVOptions.scala:47)
	at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:60)
	at org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:210)
	at scala.Option.orElse(Option.scala:447)
	at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:207)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:411)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:274)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:245)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:245)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:188)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)

2024-07-30 11:07:14,701 - root - INFO - i am in the main method..
2024-07-30 11:07:14,702 - root - INFO - calling spark object
2024-07-30 11:07:14,702 - Create_spark - INFO - get_spark_object method started
2024-07-30 11:07:14,702 - Create_spark - INFO - master is local
2024-07-30 11:07:30,824 - Create_spark - INFO - Spark object created.....
2024-07-30 11:07:30,825 - root - INFO - Validating spark object..........
2024-07-30 11:07:30,825 - Validate - WARNING - started the get_current_date method...
2024-07-30 11:07:37,610 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 30))]
2024-07-30 11:07:37,610 - Validate - WARNING - Validation done , go frwd...
2024-07-30 11:07:37,611 - root - INFO - reading file which is of type parquet
2024-07-30 11:07:38,937 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-07-30 11:07:41,575 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-07-30 11:07:41,577 - root - INFO - reading file which is of type csv
2024-07-30 11:07:50,273 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-07-30 11:07:52,312 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-07-30 11:07:52,313 - root - INFO - Application done
2024-07-30 11:10:15,374 - root - INFO - i am in the main method..
2024-07-30 11:10:15,378 - root - INFO - calling spark object
2024-07-30 11:10:15,378 - Create_spark - INFO - get_spark_object method started
2024-07-30 11:10:15,378 - Create_spark - INFO - master is local
2024-07-30 11:10:28,499 - Create_spark - INFO - Spark object created.....
2024-07-30 11:10:28,499 - root - INFO - Validating spark object..........
2024-07-30 11:10:28,499 - Validate - WARNING - started the get_current_date method...
2024-07-30 11:10:34,566 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 30))]
2024-07-30 11:10:34,566 - Validate - WARNING - Validation done , go frwd...
2024-07-30 11:10:34,568 - root - INFO - reading file which is of type parquet
2024-07-30 11:10:36,225 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-07-30 11:10:39,422 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-07-30 11:10:39,424 - root - INFO - reading file which is of type csv
2024-07-30 11:10:48,206 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-07-30 11:10:50,213 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-07-30 11:10:50,213 - root - INFO - Application done
2024-07-30 12:33:41,226 - root - INFO - i am in the main method..
2024-07-30 12:33:41,226 - root - INFO - calling spark object
2024-07-30 12:33:41,227 - Create_spark - INFO - get_spark_object method started
2024-07-30 12:33:41,227 - Create_spark - INFO - master is local
2024-07-30 12:33:48,964 - Create_spark - INFO - Spark object created.....
2024-07-30 12:33:48,964 - root - INFO - Validating spark object..........
2024-07-30 12:33:48,964 - Validate - WARNING - started the get_current_date method...
2024-07-30 12:33:53,804 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 30))]
2024-07-30 12:33:53,804 - Validate - WARNING - Validation done , go frwd...
2024-07-30 12:33:53,807 - root - INFO - reading file which is of type parquet
2024-07-30 12:33:54,658 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-07-30 12:33:56,768 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-07-30 12:33:56,769 - root - INFO - reading file which is of type csv
2024-07-30 12:34:01,937 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-07-30 12:34:03,012 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-07-30 12:34:03,013 - root - INFO - Implementing data Processing now....
2024-07-30 12:34:03,013 - Data_processing - WARNING - Cleaning data method started
2024-07-30 12:34:03,111 - Data_processing - WARNING - Cleaning data method completed
2024-07-30 12:34:03,112 - root - INFO - Application done
2024-07-30 12:37:18,200 - root - INFO - i am in the main method..
2024-07-30 12:37:18,202 - root - INFO - calling spark object
2024-07-30 12:37:18,202 - Create_spark - INFO - get_spark_object method started
2024-07-30 12:37:18,202 - Create_spark - INFO - master is local
2024-07-30 12:37:26,430 - Create_spark - INFO - Spark object created.....
2024-07-30 12:37:26,431 - root - INFO - Validating spark object..........
2024-07-30 12:37:26,431 - Validate - WARNING - started the get_current_date method...
2024-07-30 12:37:30,802 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 30))]
2024-07-30 12:37:30,802 - Validate - WARNING - Validation done , go frwd...
2024-07-30 12:37:30,805 - root - INFO - reading file which is of type parquet
2024-07-30 12:37:31,794 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-07-30 12:37:33,855 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-07-30 12:37:33,857 - root - INFO - reading file which is of type csv
2024-07-30 12:37:39,160 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-07-30 12:37:40,562 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-07-30 12:37:40,563 - root - INFO - Implementing data Processing now....
2024-07-30 12:37:40,563 - Data_processing - WARNING - Cleaning data method started
2024-07-30 12:37:40,628 - Data_processing - WARNING - displaying city dataframe...
2024-07-30 12:37:40,882 - Data_processing - WARNING - displaying presc dataframe...
2024-07-30 12:37:41,163 - Data_processing - WARNING - Cleaning data method completed
2024-07-30 12:37:41,164 - root - INFO - Application done
2024-07-30 12:46:03,183 - root - INFO - i am in the main method..
2024-07-30 12:46:03,184 - root - INFO - calling spark object
2024-07-30 12:46:03,184 - Create_spark - INFO - get_spark_object method started
2024-07-30 12:46:03,185 - Create_spark - INFO - master is local
2024-07-30 12:46:11,172 - Create_spark - INFO - Spark object created.....
2024-07-30 12:46:11,172 - root - INFO - Validating spark object..........
2024-07-30 12:46:11,172 - Validate - WARNING - started the get_current_date method...
2024-07-30 12:46:15,297 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 30))]
2024-07-30 12:46:15,297 - Validate - WARNING - Validation done , go frwd...
2024-07-30 12:46:15,298 - root - INFO - reading file which is of type parquet
2024-07-30 12:46:16,427 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-07-30 12:46:18,861 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-07-30 12:46:18,863 - root - INFO - reading file which is of type csv
2024-07-30 12:46:24,829 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-07-30 12:46:26,212 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-07-30 12:46:26,213 - root - INFO - Implementing data Processing now....
2024-07-30 12:46:26,213 - Data_processing - WARNING - Cleaning data method started
2024-07-30 12:46:26,341 - Data_processing - WARNING - Cleaning data method completed
2024-07-30 12:46:26,717 - root - INFO - Application done
2024-07-30 12:52:16,876 - root - INFO - i am in the main method..
2024-07-30 12:52:16,877 - root - INFO - calling spark object
2024-07-30 12:52:16,877 - Create_spark - INFO - get_spark_object method started
2024-07-30 12:52:16,877 - Create_spark - INFO - master is local
2024-07-30 12:52:28,612 - Create_spark - INFO - Spark object created.....
2024-07-30 12:52:28,612 - root - INFO - Validating spark object..........
2024-07-30 12:52:28,612 - Validate - WARNING - started the get_current_date method...
2024-07-30 12:52:36,219 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 30))]
2024-07-30 12:52:36,219 - Validate - WARNING - Validation done , go frwd...
2024-07-30 12:52:36,220 - root - INFO - reading file which is of type parquet
2024-07-30 12:52:37,303 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-07-30 12:52:39,526 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-07-30 12:52:39,528 - root - INFO - reading file which is of type csv
2024-07-30 12:52:45,857 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-07-30 12:52:47,686 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-07-30 12:52:47,686 - root - INFO - Implementing data Processing now....
2024-07-30 12:52:47,687 - Data_processing - WARNING - Cleaning data method started
2024-07-30 12:52:47,835 - Data_processing - WARNING - Cleaning data method completed
2024-07-30 12:52:48,276 - root - INFO - Application done
2024-07-30 17:27:01,871 - root - INFO - i am in the main method..
2024-07-30 17:27:01,871 - root - INFO - calling spark object
2024-07-30 17:27:01,871 - Create_spark - INFO - get_spark_object method started
2024-07-30 17:27:01,871 - Create_spark - INFO - master is local
2024-07-30 17:27:10,893 - Create_spark - INFO - Spark object created.....
2024-07-30 17:27:10,893 - root - INFO - Validating spark object..........
2024-07-30 17:27:10,893 - Validate - WARNING - started the get_current_date method...
2024-07-30 17:27:15,348 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 30))]
2024-07-30 17:27:15,349 - Validate - WARNING - Validation done , go frwd...
2024-07-30 17:27:15,349 - root - INFO - reading file which is of type parquet
2024-07-30 17:27:16,245 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-07-30 17:27:18,033 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-07-30 17:27:18,035 - root - INFO - reading file which is of type csv
2024-07-30 17:27:23,289 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-07-30 17:27:24,488 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-07-30 17:27:24,488 - root - INFO - Implementing data Processing now....
2024-07-30 17:27:24,488 - Data_processing - WARNING - Cleaning data method started
2024-07-30 17:27:24,649 - Data_processing - WARNING - Cleaning data method completed
2024-07-30 17:27:25,057 - root - INFO - Application done
2024-07-30 17:27:52,027 - root - INFO - i am in the main method..
2024-07-30 17:27:52,027 - root - INFO - calling spark object
2024-07-30 17:27:52,027 - Create_spark - INFO - get_spark_object method started
2024-07-30 17:27:52,027 - Create_spark - INFO - master is local
2024-07-30 17:27:59,448 - Create_spark - INFO - Spark object created.....
2024-07-30 17:27:59,448 - root - INFO - Validating spark object..........
2024-07-30 17:27:59,449 - Validate - WARNING - started the get_current_date method...
2024-07-30 17:28:03,928 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 30))]
2024-07-30 17:28:03,928 - Validate - WARNING - Validation done , go frwd...
2024-07-30 17:28:03,928 - root - INFO - reading file which is of type parquet
2024-07-30 17:28:05,043 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-07-30 17:28:06,886 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-07-30 17:28:06,887 - root - INFO - reading file which is of type csv
2024-07-30 17:28:11,892 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-07-30 17:28:13,195 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-07-30 17:28:13,195 - root - INFO - Implementing data Processing now....
2024-07-30 17:28:13,195 - Data_processing - WARNING - Cleaning data method started
2024-07-30 17:28:13,342 - Data_processing - WARNING - Cleaning data method completed
2024-07-30 17:28:13,652 - root - INFO - Application done
2024-07-30 17:37:28,660 - root - INFO - i am in the main method..
2024-07-30 17:37:28,660 - root - INFO - calling spark object
2024-07-30 17:37:28,660 - Create_spark - INFO - get_spark_object method started
2024-07-30 17:37:28,660 - Create_spark - INFO - master is local
2024-07-30 17:37:36,440 - Create_spark - INFO - Spark object created.....
2024-07-30 17:37:36,440 - root - INFO - Validating spark object..........
2024-07-30 17:37:36,440 - Validate - WARNING - started the get_current_date method...
2024-07-30 17:37:40,681 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 30))]
2024-07-30 17:37:40,682 - Validate - WARNING - Validation done , go frwd...
2024-07-30 17:37:40,683 - root - INFO - reading file which is of type parquet
2024-07-30 17:37:41,626 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-07-30 17:37:43,838 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-07-30 17:37:43,840 - root - INFO - reading file which is of type csv
2024-07-30 17:37:49,243 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-07-30 17:37:50,733 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-07-30 17:37:50,735 - root - INFO - Implementing data Processing now....
2024-07-30 17:37:50,736 - Data_processing - WARNING - Cleaning data method started
2024-07-30 17:37:50,944 - Data_processing - WARNING - Cleaning data method completed
2024-07-30 17:37:51,270 - root - INFO - Application done
2024-07-30 17:52:50,007 - root - INFO - i am in the main method..
2024-07-30 17:52:50,008 - root - INFO - calling spark object
2024-07-30 17:52:50,008 - Create_spark - INFO - get_spark_object method started
2024-07-30 17:52:50,008 - Create_spark - INFO - master is local
2024-07-30 17:52:57,532 - Create_spark - INFO - Spark object created.....
2024-07-30 17:52:57,532 - root - INFO - Validating spark object..........
2024-07-30 17:52:57,532 - Validate - WARNING - started the get_current_date method...
2024-07-30 17:53:01,592 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 30))]
2024-07-30 17:53:01,593 - Validate - WARNING - Validation done , go frwd...
2024-07-30 17:53:01,594 - root - INFO - reading file which is of type parquet
2024-07-30 17:53:02,617 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-07-30 17:53:04,925 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-07-30 17:53:04,925 - root - INFO - reading file which is of type csv
2024-07-30 17:53:10,247 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-07-30 17:53:11,839 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-07-30 17:53:11,839 - root - INFO - Implementing data Processing now....
2024-07-30 17:53:11,839 - Data_processing - WARNING - Cleaning data method started
2024-07-30 17:53:12,036 - Data_processing - WARNING - Cleaning data method completed
2024-07-30 17:53:12,483 - root - INFO - Application done
2024-07-30 17:58:30,007 - root - INFO - i am in the main method..
2024-07-30 17:58:30,008 - root - INFO - calling spark object
2024-07-30 17:58:30,008 - Create_spark - INFO - get_spark_object method started
2024-07-30 17:58:30,008 - Create_spark - INFO - master is local
2024-07-30 17:58:39,227 - Create_spark - INFO - Spark object created.....
2024-07-30 17:58:39,227 - root - INFO - Validating spark object..........
2024-07-30 17:58:39,228 - Validate - WARNING - started the get_current_date method...
2024-07-30 17:58:45,158 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 30))]
2024-07-30 17:58:45,160 - Validate - WARNING - Validation done , go frwd...
2024-07-30 17:58:45,160 - root - INFO - reading file which is of type parquet
2024-07-30 17:58:46,446 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-07-30 17:58:49,130 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-07-30 17:58:49,132 - root - INFO - reading file which is of type csv
2024-07-30 17:58:56,071 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-07-30 17:58:57,821 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-07-30 17:58:57,821 - root - INFO - Implementing data Processing now....
2024-07-30 17:58:57,821 - Data_processing - WARNING - Cleaning data method started
2024-07-30 17:58:58,008 - Data_processing - WARNING - Cleaning data method completed
2024-07-30 17:58:58,386 - root - INFO - Application done
2024-07-30 18:04:45,928 - root - INFO - i am in the main method..
2024-07-30 18:04:45,929 - root - INFO - calling spark object
2024-07-30 18:04:45,929 - Create_spark - INFO - get_spark_object method started
2024-07-30 18:04:45,929 - Create_spark - INFO - master is local
2024-07-30 18:04:55,650 - Create_spark - INFO - Spark object created.....
2024-07-30 18:04:55,651 - root - INFO - Validating spark object..........
2024-07-30 18:04:55,651 - Validate - WARNING - started the get_current_date method...
2024-07-30 18:05:00,059 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 30))]
2024-07-30 18:05:00,059 - Validate - WARNING - Validation done , go frwd...
2024-07-30 18:05:00,061 - root - INFO - reading file which is of type parquet
2024-07-30 18:05:01,061 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-07-30 18:05:02,828 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-07-30 18:05:02,830 - root - INFO - reading file which is of type csv
2024-07-30 18:05:08,376 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-07-30 18:05:09,927 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-07-30 18:05:09,927 - root - INFO - Implementing data Processing now....
2024-07-30 18:05:09,927 - Data_processing - WARNING - Cleaning data method started
2024-07-30 18:05:10,121 - Data_processing - WARNING - Cleaning data method completed
2024-07-30 18:05:10,425 - root - INFO - Application done
2024-07-30 18:14:17,534 - root - INFO - i am in the main method..
2024-07-30 18:14:17,661 - root - INFO - calling spark object
2024-07-30 18:14:17,663 - Create_spark - INFO - get_spark_object method started
2024-07-30 18:14:17,664 - Create_spark - INFO - master is local
2024-07-30 18:14:25,588 - Create_spark - INFO - Spark object created.....
2024-07-30 18:14:25,588 - root - INFO - Validating spark object..........
2024-07-30 18:14:25,588 - Validate - WARNING - started the get_current_date method...
2024-07-30 18:14:29,671 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 30))]
2024-07-30 18:14:29,671 - Validate - WARNING - Validation done , go frwd...
2024-07-30 18:14:29,672 - root - INFO - reading file which is of type parquet
2024-07-30 18:14:30,596 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-07-30 18:14:32,686 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-07-30 18:14:32,687 - root - INFO - reading file which is of type csv
2024-07-30 18:14:38,059 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-07-30 18:14:39,188 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-07-30 18:14:39,188 - root - INFO - Implementing data Processing now....
2024-07-30 18:14:39,189 - Data_processing - WARNING - Cleaning data method started
2024-07-30 18:14:39,445 - Data_processing - WARNING - Cleaning data method completed
2024-07-30 18:14:39,795 - root - INFO - Application done
2024-07-30 18:16:40,727 - root - INFO - i am in the main method..
2024-07-30 18:16:40,728 - root - INFO - calling spark object
2024-07-30 18:16:40,729 - Create_spark - INFO - get_spark_object method started
2024-07-30 18:16:40,729 - Create_spark - INFO - master is local
2024-07-30 18:16:48,144 - Create_spark - INFO - Spark object created.....
2024-07-30 18:16:48,144 - root - INFO - Validating spark object..........
2024-07-30 18:16:48,144 - Validate - WARNING - started the get_current_date method...
2024-07-30 18:16:54,471 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 30))]
2024-07-30 18:16:54,471 - Validate - WARNING - Validation done , go frwd...
2024-07-30 18:16:54,473 - root - INFO - reading file which is of type parquet
2024-07-30 18:16:55,801 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-07-30 18:17:01,327 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-07-30 18:17:01,330 - root - INFO - reading file which is of type csv
2024-07-30 18:17:08,588 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-07-30 18:17:09,891 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-07-30 18:17:09,892 - root - INFO - Implementing data Processing now....
2024-07-30 18:17:09,892 - Data_processing - WARNING - Cleaning data method started
2024-07-30 18:17:10,142 - Data_processing - WARNING - Cleaning data method completed
2024-07-30 18:17:10,686 - root - INFO - Application done
2024-07-30 18:18:23,473 - root - INFO - i am in the main method..
2024-07-30 18:18:23,475 - root - INFO - calling spark object
2024-07-30 18:18:23,475 - Create_spark - INFO - get_spark_object method started
2024-07-30 18:18:23,475 - Create_spark - INFO - master is local
2024-07-30 18:18:33,203 - Create_spark - INFO - Spark object created.....
2024-07-30 18:18:33,203 - root - INFO - Validating spark object..........
2024-07-30 18:18:33,203 - Validate - WARNING - started the get_current_date method...
2024-07-30 18:18:39,111 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 30))]
2024-07-30 18:18:39,112 - Validate - WARNING - Validation done , go frwd...
2024-07-30 18:18:39,112 - root - INFO - reading file which is of type parquet
2024-07-30 18:18:40,426 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-07-30 18:18:42,901 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-07-30 18:18:42,902 - root - INFO - reading file which is of type csv
2024-07-30 18:18:50,761 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-07-30 18:18:52,872 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-07-30 18:18:52,872 - root - INFO - Implementing data Processing now....
2024-07-30 18:18:52,872 - Data_processing - WARNING - Cleaning data method started
2024-07-30 18:18:53,401 - Data_processing - WARNING - Cleaning data method completed
2024-07-30 18:18:54,248 - root - INFO - Application done
2024-07-30 18:38:00,139 - root - INFO - i am in the main method..
2024-07-30 18:38:00,141 - root - INFO - calling spark object
2024-07-30 18:38:00,141 - Create_spark - INFO - get_spark_object method started
2024-07-30 18:38:00,142 - Create_spark - INFO - master is local
2024-07-30 18:38:10,833 - Create_spark - INFO - Spark object created.....
2024-07-30 18:38:10,834 - root - INFO - Validating spark object..........
2024-07-30 18:38:10,834 - Validate - WARNING - started the get_current_date method...
2024-07-30 18:38:15,085 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 30))]
2024-07-30 18:38:15,085 - Validate - WARNING - Validation done , go frwd...
2024-07-30 18:38:15,086 - root - INFO - reading file which is of type parquet
2024-07-30 18:38:16,308 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-07-30 18:38:18,859 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-07-30 18:38:18,860 - root - INFO - reading file which is of type csv
2024-07-30 18:38:24,425 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-07-30 18:38:25,779 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-07-30 18:38:25,779 - root - INFO - Implementing data Processing now....
2024-07-30 18:38:25,779 - Data_processing - WARNING - Cleaning data method started
2024-07-30 18:38:25,981 - Data_processing - WARNING - Cleaning data method completed
2024-07-30 18:38:26,479 - root - ERROR - An error occurred when calling main() please check the trace=== 'Column' object is not callable
2024-07-30 18:39:03,220 - root - INFO - i am in the main method..
2024-07-30 18:39:03,221 - root - INFO - calling spark object
2024-07-30 18:39:03,221 - Create_spark - INFO - get_spark_object method started
2024-07-30 18:39:03,221 - Create_spark - INFO - master is local
2024-07-30 18:39:10,783 - Create_spark - INFO - Spark object created.....
2024-07-30 18:39:10,784 - root - INFO - Validating spark object..........
2024-07-30 18:39:10,784 - Validate - WARNING - started the get_current_date method...
2024-07-30 18:39:14,773 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 30))]
2024-07-30 18:39:14,773 - Validate - WARNING - Validation done , go frwd...
2024-07-30 18:39:14,773 - root - INFO - reading file which is of type parquet
2024-07-30 18:39:15,783 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-07-30 18:39:17,909 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-07-30 18:39:17,909 - root - INFO - reading file which is of type csv
2024-07-30 18:39:23,484 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-07-30 18:39:25,519 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-07-30 18:39:25,520 - root - INFO - Implementing data Processing now....
2024-07-30 18:39:25,520 - Data_processing - WARNING - Cleaning data method started
2024-07-30 18:39:25,882 - Data_processing - WARNING - Cleaning data method completed
2024-07-30 18:39:26,448 - root - ERROR - An error occurred when calling main() please check the trace=== 'Column' object is not callable
2024-07-30 18:40:55,961 - root - INFO - i am in the main method..
2024-07-30 18:40:55,961 - root - INFO - calling spark object
2024-07-30 18:40:55,962 - Create_spark - INFO - get_spark_object method started
2024-07-30 18:40:55,962 - Create_spark - INFO - master is local
2024-07-30 18:41:03,389 - Create_spark - INFO - Spark object created.....
2024-07-30 18:41:03,389 - root - INFO - Validating spark object..........
2024-07-30 18:41:03,389 - Validate - WARNING - started the get_current_date method...
2024-07-30 18:41:07,735 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 30))]
2024-07-30 18:41:07,736 - Validate - WARNING - Validation done , go frwd...
2024-07-30 18:41:07,736 - root - INFO - reading file which is of type parquet
2024-07-30 18:41:08,771 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-07-30 18:41:10,851 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-07-30 18:41:10,852 - root - INFO - reading file which is of type csv
2024-07-30 18:41:15,943 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-07-30 18:41:17,599 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-07-30 18:41:17,599 - root - INFO - Implementing data Processing now....
2024-07-30 18:41:17,599 - Data_processing - WARNING - Cleaning data method started
2024-07-30 18:41:17,815 - Data_processing - WARNING - Cleaning data method completed
2024-07-30 18:41:18,138 - root - ERROR - An error occurred when calling main() please check the trace=== when() missing 1 required positional argument: 'value'
2024-07-30 18:53:52,890 - root - INFO - i am in the main method..
2024-07-30 18:53:52,891 - root - INFO - calling spark object
2024-07-30 18:53:52,891 - Create_spark - INFO - get_spark_object method started
2024-07-30 18:53:52,891 - Create_spark - INFO - master is local
2024-07-30 18:54:02,067 - Create_spark - INFO - Spark object created.....
2024-07-30 18:54:02,068 - root - INFO - Validating spark object..........
2024-07-30 18:54:02,068 - Validate - WARNING - started the get_current_date method...
2024-07-30 18:54:06,586 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 30))]
2024-07-30 18:54:06,586 - Validate - WARNING - Validation done , go frwd...
2024-07-30 18:54:06,587 - root - INFO - reading file which is of type parquet
2024-07-30 18:54:07,511 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-07-30 18:54:09,367 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-07-30 18:54:09,368 - root - INFO - reading file which is of type csv
2024-07-30 18:54:14,889 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-07-30 18:54:16,246 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-07-30 18:54:16,247 - root - INFO - Implementing data Processing now....
2024-07-30 18:54:16,247 - Data_processing - WARNING - Cleaning data method started
2024-07-30 18:54:16,460 - Data_processing - WARNING - Cleaning data method completed
2024-07-30 18:54:22,689 - root - INFO - Application done
2024-07-30 18:59:29,525 - root - INFO - i am in the main method..
2024-07-30 18:59:29,526 - root - INFO - calling spark object
2024-07-30 18:59:29,526 - Create_spark - INFO - get_spark_object method started
2024-07-30 18:59:29,526 - Create_spark - INFO - master is local
2024-07-30 18:59:37,686 - Create_spark - INFO - Spark object created.....
2024-07-30 18:59:37,686 - root - INFO - Validating spark object..........
2024-07-30 18:59:37,686 - Validate - WARNING - started the get_current_date method...
2024-07-30 18:59:42,109 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 30))]
2024-07-30 18:59:42,109 - Validate - WARNING - Validation done , go frwd...
2024-07-30 18:59:42,110 - root - INFO - reading file which is of type parquet
2024-07-30 18:59:43,537 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-07-30 18:59:46,778 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-07-30 18:59:46,779 - root - INFO - reading file which is of type csv
2024-07-30 18:59:53,476 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-07-30 18:59:54,765 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-07-30 18:59:54,765 - root - INFO - Implementing data Processing now....
2024-07-30 18:59:54,765 - Data_processing - WARNING - Cleaning data method started
2024-07-30 18:59:54,973 - Data_processing - WARNING - Cleaning data method completed
2024-07-30 19:00:01,241 - root - INFO - Application done
2024-07-30 19:05:40,833 - root - INFO - i am in the main method..
2024-07-30 19:05:40,833 - root - INFO - calling spark object
2024-07-30 19:05:40,833 - Create_spark - INFO - get_spark_object method started
2024-07-30 19:05:40,834 - Create_spark - INFO - master is local
2024-07-30 19:05:57,093 - Create_spark - INFO - Spark object created.....
2024-07-30 19:05:57,095 - root - INFO - Validating spark object..........
2024-07-30 19:05:57,095 - Validate - WARNING - started the get_current_date method...
2024-07-30 19:06:08,300 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 30))]
2024-07-30 19:06:08,300 - Validate - WARNING - Validation done , go frwd...
2024-07-30 19:06:08,302 - root - INFO - reading file which is of type parquet
2024-07-30 19:06:10,606 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-07-30 19:06:14,958 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-07-30 19:06:14,958 - root - INFO - reading file which is of type csv
2024-07-30 19:06:30,278 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-07-30 19:06:33,538 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-07-30 19:06:33,538 - root - INFO - Implementing data Processing now....
2024-07-30 19:06:33,538 - Data_processing - WARNING - Cleaning data method started
2024-07-30 19:06:34,038 - Data_processing - WARNING - Cleaning data method completed
2024-07-30 19:06:51,600 - root - INFO - Application done
2024-07-30 19:33:12,566 - root - INFO - i am in the main method..
2024-07-30 19:33:12,567 - root - INFO - calling spark object
2024-07-30 19:33:12,567 - Create_spark - INFO - get_spark_object method started
2024-07-30 19:33:12,567 - Create_spark - INFO - master is local
2024-07-30 19:33:21,795 - Create_spark - INFO - Spark object created.....
2024-07-30 19:33:21,795 - root - INFO - Validating spark object..........
2024-07-30 19:33:21,796 - Validate - WARNING - started the get_current_date method...
2024-07-30 19:33:26,165 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 30))]
2024-07-30 19:33:26,166 - Validate - WARNING - Validation done , go frwd...
2024-07-30 19:33:26,167 - root - INFO - reading file which is of type parquet
2024-07-30 19:33:27,240 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-07-30 19:33:29,328 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-07-30 19:33:29,331 - root - INFO - reading file which is of type csv
2024-07-30 19:33:35,738 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-07-30 19:33:37,371 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-07-30 19:33:37,372 - root - INFO - Implementing data Processing now....
2024-07-30 19:33:37,372 - Data_processing - WARNING - Cleaning data method started
2024-07-30 19:33:38,299 - root - ERROR - An error occurred when calling main() please check the trace=== Cannot resolve column name "presc_id" among (city, state_id, state_name, county_name, population)
2024-07-30 19:37:48,378 - root - INFO - i am in the main method..
2024-07-30 19:37:48,378 - root - INFO - calling spark object
2024-07-30 19:37:48,378 - Create_spark - INFO - get_spark_object method started
2024-07-30 19:37:48,379 - Create_spark - INFO - master is local
2024-07-30 19:37:57,316 - Create_spark - INFO - Spark object created.....
2024-07-30 19:37:57,316 - root - INFO - Validating spark object..........
2024-07-30 19:37:57,316 - Validate - WARNING - started the get_current_date method...
2024-07-30 19:38:02,872 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 30))]
2024-07-30 19:38:02,872 - Validate - WARNING - Validation done , go frwd...
2024-07-30 19:38:02,873 - root - INFO - reading file which is of type parquet
2024-07-30 19:38:03,912 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-07-30 19:38:05,804 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-07-30 19:38:05,805 - root - INFO - reading file which is of type csv
2024-07-30 19:38:11,741 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-07-30 19:38:13,104 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-07-30 19:38:13,104 - root - INFO - Implementing data Processing now....
2024-07-30 19:38:13,104 - Data_processing - WARNING - Cleaning data method started
2024-07-30 19:38:13,385 - root - ERROR - An error occurred when calling main() please check the trace=== Cannot resolve column name "presc_id" among (city, state_id, state_name, county_name, population)
2024-07-30 19:38:29,010 - root - INFO - i am in the main method..
2024-07-30 19:38:29,028 - root - INFO - calling spark object
2024-07-30 19:38:29,028 - Create_spark - INFO - get_spark_object method started
2024-07-30 19:38:29,028 - Create_spark - INFO - master is local
2024-07-30 19:38:40,290 - Create_spark - INFO - Spark object created.....
2024-07-30 19:38:40,291 - root - INFO - Validating spark object..........
2024-07-30 19:38:40,291 - Validate - WARNING - started the get_current_date method...
2024-07-30 19:38:45,091 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 30))]
2024-07-30 19:38:45,091 - Validate - WARNING - Validation done , go frwd...
2024-07-30 19:38:45,091 - root - INFO - reading file which is of type parquet
2024-07-30 19:38:46,033 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-07-30 19:38:48,224 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-07-30 19:38:48,225 - root - INFO - reading file which is of type csv
2024-07-30 19:38:53,933 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-07-30 19:38:55,071 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-07-30 19:38:55,071 - root - INFO - Implementing data Processing now....
2024-07-30 19:38:55,071 - Data_processing - WARNING - Cleaning data method started
2024-07-30 19:38:55,422 - root - ERROR - An error occurred when calling main() please check the trace=== Cannot resolve column name "presc_id" among (city, state_id, state_name, county_name, population)
2024-07-30 19:40:42,723 - root - INFO - i am in the main method..
2024-07-30 19:40:42,723 - root - INFO - calling spark object
2024-07-30 19:40:42,723 - Create_spark - INFO - get_spark_object method started
2024-07-30 19:40:42,723 - Create_spark - INFO - master is local
2024-07-30 19:40:50,970 - Create_spark - INFO - Spark object created.....
2024-07-30 19:40:50,970 - root - INFO - Validating spark object..........
2024-07-30 19:40:50,970 - Validate - WARNING - started the get_current_date method...
2024-07-30 19:40:55,365 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 30))]
2024-07-30 19:40:55,365 - Validate - WARNING - Validation done , go frwd...
2024-07-30 19:40:55,366 - root - INFO - reading file which is of type parquet
2024-07-30 19:40:56,277 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-07-30 19:40:58,347 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-07-30 19:40:58,349 - root - INFO - reading file which is of type csv
2024-07-30 19:41:04,559 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-07-30 19:41:06,696 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-07-30 19:41:06,696 - root - INFO - Implementing data Processing now....
2024-07-30 19:41:06,696 - root - ERROR - An error occurred when calling main() please check the trace=== clean() missing 2 required positional arguments: 'df1_drop_null' and 'df2_drop_null'
2024-07-30 19:43:34,359 - root - INFO - i am in the main method..
2024-07-30 19:43:34,360 - root - INFO - calling spark object
2024-07-30 19:43:34,360 - Create_spark - INFO - get_spark_object method started
2024-07-30 19:43:34,360 - Create_spark - INFO - master is local
2024-07-30 19:43:43,097 - Create_spark - INFO - Spark object created.....
2024-07-30 19:43:43,098 - root - INFO - Validating spark object..........
2024-07-30 19:43:43,098 - Validate - WARNING - started the get_current_date method...
2024-07-30 19:43:48,021 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 30))]
2024-07-30 19:43:48,021 - Validate - WARNING - Validation done , go frwd...
2024-07-30 19:43:48,023 - root - INFO - reading file which is of type parquet
2024-07-30 19:43:48,929 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-07-30 19:43:51,256 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-07-30 19:43:51,272 - root - INFO - reading file which is of type csv
2024-07-30 19:43:57,388 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-07-30 19:43:59,408 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-07-30 19:43:59,409 - root - INFO - Implementing data Processing now....
2024-07-30 19:43:59,409 - Data_processing - WARNING - Cleaning data method started
2024-07-30 19:44:01,136 - root - ERROR - An error occurred when calling main() please check the trace=== Cannot resolve column name "presc_id" among (city, state_id, state_name, county_name, population)
2024-07-30 19:44:53,599 - root - INFO - i am in the main method..
2024-07-30 19:44:53,599 - root - INFO - calling spark object
2024-07-30 19:44:53,599 - Create_spark - INFO - get_spark_object method started
2024-07-30 19:44:53,599 - Create_spark - INFO - master is local
2024-07-30 19:45:02,215 - Create_spark - INFO - Spark object created.....
2024-07-30 19:45:02,215 - root - INFO - Validating spark object..........
2024-07-30 19:45:02,215 - Validate - WARNING - started the get_current_date method...
2024-07-30 19:45:06,626 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 30))]
2024-07-30 19:45:06,626 - Validate - WARNING - Validation done , go frwd...
2024-07-30 19:45:06,626 - root - INFO - reading file which is of type parquet
2024-07-30 19:45:07,650 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-07-30 19:45:09,628 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-07-30 19:45:09,629 - root - INFO - reading file which is of type csv
2024-07-30 19:45:15,111 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-07-30 19:45:16,535 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-07-30 19:45:16,540 - root - INFO - Implementing data Processing now....
2024-07-30 19:45:16,540 - Data_processing - WARNING - Cleaning data method started
2024-07-30 19:45:17,839 - root - ERROR - An error occurred when calling main() please check the trace=== Cannot resolve column name "presc_id" among (city, state_id, state_name, county_name, population)
2024-07-30 19:49:34,434 - root - INFO - i am in the main method..
2024-07-30 19:49:34,455 - root - INFO - calling spark object
2024-07-30 19:49:34,455 - Create_spark - INFO - get_spark_object method started
2024-07-30 19:49:34,455 - Create_spark - INFO - master is local
2024-07-30 19:49:43,027 - Create_spark - INFO - Spark object created.....
2024-07-30 19:49:43,028 - root - INFO - Validating spark object..........
2024-07-30 19:49:43,028 - Validate - WARNING - started the get_current_date method...
2024-07-30 19:49:47,982 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 30))]
2024-07-30 19:49:47,982 - Validate - WARNING - Validation done , go frwd...
2024-07-30 19:49:47,983 - root - INFO - reading file which is of type parquet
2024-07-30 19:49:48,912 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-07-30 19:49:50,727 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-07-30 19:49:50,728 - root - INFO - reading file which is of type csv
2024-07-30 19:49:57,331 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-07-30 19:49:59,002 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-07-30 19:49:59,003 - root - INFO - Implementing data Processing now....
2024-07-30 19:49:59,004 - Data_processing - WARNING - Cleaning data method started
2024-07-30 19:50:00,616 - root - ERROR - An error occurred when calling main() please check the trace=== Cannot resolve column name "presc_id" among (city, state_id, state_name, county_name, population)
2024-07-30 19:50:17,683 - root - INFO - i am in the main method..
2024-07-30 19:50:17,684 - root - INFO - calling spark object
2024-07-30 19:50:17,684 - Create_spark - INFO - get_spark_object method started
2024-07-30 19:50:17,684 - Create_spark - INFO - master is local
2024-07-30 19:50:25,441 - Create_spark - INFO - Spark object created.....
2024-07-30 19:50:25,441 - root - INFO - Validating spark object..........
2024-07-30 19:50:25,442 - Validate - WARNING - started the get_current_date method...
2024-07-30 19:50:30,443 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 30))]
2024-07-30 19:50:30,443 - Validate - WARNING - Validation done , go frwd...
2024-07-30 19:50:30,444 - root - INFO - reading file which is of type parquet
2024-07-30 19:50:31,462 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-07-30 19:50:33,492 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-07-30 19:50:33,494 - root - INFO - reading file which is of type csv
2024-07-30 19:50:40,069 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-07-30 19:50:42,069 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-07-30 19:50:42,070 - root - INFO - Implementing data Processing now....
2024-07-30 19:50:42,071 - Data_processing - WARNING - Cleaning data method started
2024-07-30 19:50:43,851 - root - ERROR - An error occurred when calling main() please check the trace=== Cannot resolve column name "presc_id" among (city, state_id, state_name, county_name, population)
2024-07-30 19:52:19,885 - root - INFO - i am in the main method..
2024-07-30 19:52:19,885 - root - INFO - calling spark object
2024-07-30 19:52:19,885 - Create_spark - INFO - get_spark_object method started
2024-07-30 19:52:19,885 - Create_spark - INFO - master is local
2024-07-30 19:52:30,624 - Create_spark - INFO - Spark object created.....
2024-07-30 19:52:30,624 - root - INFO - Validating spark object..........
2024-07-30 19:52:30,624 - Validate - WARNING - started the get_current_date method...
2024-07-30 19:52:35,305 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 30))]
2024-07-30 19:52:35,305 - Validate - WARNING - Validation done , go frwd...
2024-07-30 19:52:35,307 - root - INFO - reading file which is of type parquet
2024-07-30 19:52:36,212 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-07-30 19:52:38,410 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-07-30 19:52:38,412 - root - INFO - reading file which is of type csv
2024-07-30 19:52:44,122 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-07-30 19:52:45,239 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-07-30 19:52:45,242 - root - INFO - Implementing data Processing now....
2024-07-30 19:52:45,242 - Data_processing - WARNING - Cleaning data method started
2024-07-30 19:52:46,307 - root - ERROR - An error occurred when calling main() please check the trace=== Cannot resolve column name "presc_id" among (city, state_id, state_name, county_name, population)
2024-07-30 19:55:48,529 - root - INFO - i am in the main method..
2024-07-30 19:55:48,608 - root - INFO - calling spark object
2024-07-30 19:55:48,608 - Create_spark - INFO - get_spark_object method started
2024-07-30 19:55:48,608 - Create_spark - INFO - master is local
2024-07-30 19:55:58,342 - Create_spark - INFO - Spark object created.....
2024-07-30 19:55:58,342 - root - INFO - Validating spark object..........
2024-07-30 19:55:58,342 - Validate - WARNING - started the get_current_date method...
2024-07-30 19:56:02,919 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 30))]
2024-07-30 19:56:02,919 - Validate - WARNING - Validation done , go frwd...
2024-07-30 19:56:02,920 - root - INFO - reading file which is of type parquet
2024-07-30 19:56:03,853 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-07-30 19:56:05,894 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-07-30 19:56:05,895 - root - INFO - reading file which is of type csv
2024-07-30 19:56:11,782 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-07-30 19:56:13,351 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-07-30 19:56:13,357 - root - INFO - Implementing data Processing now....
2024-07-30 19:56:13,358 - Data_processing - WARNING - Cleaning data method started
2024-07-30 19:56:14,528 - Data_processing - WARNING - Cleaning data method completed
2024-07-30 19:56:14,528 - root - ERROR - An error occurred when calling main() please check the trace=== cannot unpack non-iterable NoneType object
2024-07-30 20:09:15,573 - root - INFO - i am in the main method..
2024-07-30 20:09:15,574 - root - INFO - calling spark object
2024-07-30 20:09:15,574 - Create_spark - INFO - get_spark_object method started
2024-07-30 20:09:15,574 - Create_spark - INFO - master is local
2024-07-30 20:09:23,790 - Create_spark - INFO - Spark object created.....
2024-07-30 20:09:23,790 - root - INFO - Validating spark object..........
2024-07-30 20:09:23,790 - Validate - WARNING - started the get_current_date method...
2024-07-30 20:09:28,224 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 30))]
2024-07-30 20:09:28,224 - Validate - WARNING - Validation done , go frwd...
2024-07-30 20:09:28,225 - root - INFO - reading file which is of type parquet
2024-07-30 20:09:29,256 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-07-30 20:09:31,339 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-07-30 20:09:31,339 - root - INFO - reading file which is of type csv
2024-07-30 20:09:37,342 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-07-30 20:09:39,233 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-07-30 20:09:39,236 - root - INFO - Implementing data Processing now....
2024-07-30 20:09:39,236 - Data_processing - WARNING - Cleaning data method started
2024-07-30 20:09:40,700 - Data_processing - WARNING - Cleaning data method completed
2024-07-30 20:09:40,700 - root - ERROR - An error occurred when calling main() please check the trace=== cannot unpack non-iterable NoneType object
2024-07-30 20:11:57,290 - root - INFO - i am in the main method..
2024-07-30 20:11:57,292 - root - INFO - calling spark object
2024-07-30 20:11:57,292 - Create_spark - INFO - get_spark_object method started
2024-07-30 20:11:57,292 - Create_spark - INFO - master is local
2024-07-30 20:12:05,943 - Create_spark - INFO - Spark object created.....
2024-07-30 20:12:05,944 - root - INFO - Validating spark object..........
2024-07-30 20:12:05,944 - Validate - WARNING - started the get_current_date method...
2024-07-30 20:12:10,537 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 30))]
2024-07-30 20:12:10,538 - Validate - WARNING - Validation done , go frwd...
2024-07-30 20:12:10,538 - root - INFO - reading file which is of type parquet
2024-07-30 20:12:11,495 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-07-30 20:12:13,701 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-07-30 20:12:13,702 - root - INFO - reading file which is of type csv
2024-07-30 20:12:20,510 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-07-30 20:12:22,027 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-07-30 20:12:22,033 - root - INFO - Implementing data Processing now....
2024-07-30 20:12:22,033 - Data_processing - WARNING - Cleaning data method started
2024-07-30 20:12:23,316 - Data_processing - WARNING - Cleaning data method completed
2024-07-30 20:12:23,316 - root - ERROR - An error occurred when calling main() please check the trace=== cannot unpack non-iterable NoneType object
2024-07-30 20:13:12,400 - root - INFO - i am in the main method..
2024-07-30 20:13:12,402 - root - INFO - calling spark object
2024-07-30 20:13:12,402 - Create_spark - INFO - get_spark_object method started
2024-07-30 20:13:12,402 - Create_spark - INFO - master is local
2024-07-30 20:13:21,088 - Create_spark - INFO - Spark object created.....
2024-07-30 20:13:21,088 - root - INFO - Validating spark object..........
2024-07-30 20:13:21,088 - Validate - WARNING - started the get_current_date method...
2024-07-30 20:13:25,833 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 30))]
2024-07-30 20:13:25,834 - Validate - WARNING - Validation done , go frwd...
2024-07-30 20:13:25,835 - root - INFO - reading file which is of type parquet
2024-07-30 20:13:26,908 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-07-30 20:13:29,076 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-07-30 20:13:29,077 - root - INFO - reading file which is of type csv
2024-07-30 20:13:35,429 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-07-30 20:13:37,402 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-07-30 20:13:37,408 - root - INFO - Implementing data Processing now....
2024-07-30 20:13:37,409 - Data_processing - WARNING - Cleaning data method started
2024-07-30 20:13:38,880 - Data_processing - WARNING - Cleaning data method completed
2024-07-30 20:13:38,881 - root - ERROR - An error occurred when calling main() please check the trace=== cannot unpack non-iterable NoneType object
2024-07-30 20:14:37,708 - root - INFO - i am in the main method..
2024-07-30 20:14:37,709 - root - INFO - calling spark object
2024-07-30 20:14:37,709 - Create_spark - INFO - get_spark_object method started
2024-07-30 20:14:37,709 - Create_spark - INFO - master is local
2024-07-30 20:14:46,631 - Create_spark - INFO - Spark object created.....
2024-07-30 20:14:46,632 - root - INFO - Validating spark object..........
2024-07-30 20:14:46,632 - Validate - WARNING - started the get_current_date method...
2024-07-30 20:14:51,718 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 30))]
2024-07-30 20:14:51,718 - Validate - WARNING - Validation done , go frwd...
2024-07-30 20:14:51,719 - root - INFO - reading file which is of type parquet
2024-07-30 20:14:52,769 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-07-30 20:14:58,125 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-07-30 20:14:58,126 - root - INFO - reading file which is of type csv
2024-07-30 20:15:04,677 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-07-30 20:15:05,882 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-07-30 20:15:05,884 - root - INFO - Implementing data Processing now....
2024-07-30 20:15:05,885 - Data_processing - WARNING - Cleaning data method started
2024-07-30 20:15:07,292 - Data_processing - WARNING - Cleaning data method completed
2024-07-30 20:15:07,292 - root - ERROR - An error occurred when calling main() please check the trace=== cannot unpack non-iterable NoneType object
2024-07-30 20:15:27,612 - root - INFO - i am in the main method..
2024-07-30 20:15:27,612 - root - INFO - calling spark object
2024-07-30 20:15:27,612 - Create_spark - INFO - get_spark_object method started
2024-07-30 20:15:27,612 - Create_spark - INFO - master is local
2024-07-30 20:15:35,920 - Create_spark - INFO - Spark object created.....
2024-07-30 20:15:35,920 - root - INFO - Validating spark object..........
2024-07-30 20:15:35,920 - Validate - WARNING - started the get_current_date method...
2024-07-30 20:15:40,813 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 7, 30))]
2024-07-30 20:15:40,813 - Validate - WARNING - Validation done , go frwd...
2024-07-30 20:15:40,813 - root - INFO - reading file which is of type parquet
2024-07-30 20:15:41,736 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-07-30 20:15:44,328 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-07-30 20:15:44,329 - root - INFO - reading file which is of type csv
2024-07-30 20:15:50,309 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-07-30 20:15:51,794 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-07-30 20:15:51,797 - root - INFO - Implementing data Processing now....
2024-07-30 20:15:51,797 - Data_processing - WARNING - Cleaning data method started
2024-07-30 20:15:53,007 - Data_processing - WARNING - Cleaning data method completed
2024-07-30 20:15:53,007 - root - ERROR - An error occurred when calling main() please check the trace=== cannot unpack non-iterable NoneType object
2024-08-01 09:41:15,623 - root - INFO - i am in the main method..
2024-08-01 09:41:15,623 - root - INFO - calling spark object
2024-08-01 09:41:15,623 - Create_spark - INFO - get_spark_object method started
2024-08-01 09:41:15,623 - Create_spark - INFO - master is local
2024-08-01 09:41:53,967 - Create_spark - INFO - Spark object created.....
2024-08-01 09:41:53,967 - root - INFO - Validating spark object..........
2024-08-01 09:41:53,967 - Validate - WARNING - started the get_current_date method...
2024-08-01 09:42:07,662 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 1))]
2024-08-01 09:42:07,662 - Validate - WARNING - Validation done , go frwd...
2024-08-01 09:42:07,662 - root - INFO - reading file which is of type parquet
2024-08-01 09:42:10,400 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-01 09:42:16,084 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-01 09:42:16,084 - root - INFO - reading file which is of type csv
2024-08-01 09:42:25,940 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-01 09:42:27,309 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-01 09:42:27,312 - root - INFO - Implementing data Processing now....
2024-08-01 09:42:27,312 - Data_processing - WARNING - Cleaning data method started
2024-08-01 09:42:28,501 - Data_processing - WARNING - Cleaning data method completed
2024-08-01 09:42:28,501 - root - ERROR - An error occurred when calling main() please check the trace=== cannot unpack non-iterable NoneType object
2024-08-01 09:45:30,615 - root - INFO - i am in the main method..
2024-08-01 09:45:30,616 - root - INFO - calling spark object
2024-08-01 09:45:30,616 - Create_spark - INFO - get_spark_object method started
2024-08-01 09:45:30,616 - Create_spark - INFO - master is local
2024-08-01 09:45:38,651 - Create_spark - INFO - Spark object created.....
2024-08-01 09:45:38,651 - root - INFO - Validating spark object..........
2024-08-01 09:45:38,651 - Validate - WARNING - started the get_current_date method...
2024-08-01 09:45:43,037 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 1))]
2024-08-01 09:45:43,037 - Validate - WARNING - Validation done , go frwd...
2024-08-01 09:45:43,037 - root - INFO - reading file which is of type parquet
2024-08-01 09:45:44,293 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-01 09:45:46,169 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-01 09:45:46,170 - root - INFO - reading file which is of type csv
2024-08-01 09:45:52,112 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-01 09:45:53,463 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-01 09:45:53,467 - root - INFO - Implementing data Processing now....
2024-08-01 09:45:53,468 - Data_processing - WARNING - Cleaning data method started
2024-08-01 09:46:00,681 - Data_processing - WARNING - Cleaning data method completed
2024-08-01 09:46:00,682 - root - ERROR - An error occurred when calling main() please check the trace=== cannot unpack non-iterable NoneType object
2024-08-01 09:46:31,536 - root - INFO - i am in the main method..
2024-08-01 09:46:31,536 - root - INFO - calling spark object
2024-08-01 09:46:31,536 - Create_spark - INFO - get_spark_object method started
2024-08-01 09:46:31,536 - Create_spark - INFO - master is local
2024-08-01 09:46:39,066 - Create_spark - INFO - Spark object created.....
2024-08-01 09:46:39,066 - root - INFO - Validating spark object..........
2024-08-01 09:46:39,066 - Validate - WARNING - started the get_current_date method...
2024-08-01 09:46:43,571 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 1))]
2024-08-01 09:46:43,571 - Validate - WARNING - Validation done , go frwd...
2024-08-01 09:46:43,571 - root - INFO - reading file which is of type parquet
2024-08-01 09:46:44,598 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-01 09:46:46,697 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-01 09:46:46,698 - root - INFO - reading file which is of type csv
2024-08-01 09:46:52,063 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-01 09:46:53,380 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-01 09:46:53,383 - root - INFO - Implementing data Processing now....
2024-08-01 09:46:53,384 - Data_processing - WARNING - Cleaning data method started
2024-08-01 09:47:00,783 - Data_processing - WARNING - Cleaning data method completed
2024-08-01 09:47:00,784 - root - ERROR - An error occurred when calling main() please check the trace=== cannot unpack non-iterable NoneType object
2024-08-01 09:48:14,203 - root - INFO - i am in the main method..
2024-08-01 09:48:14,203 - root - INFO - calling spark object
2024-08-01 09:48:14,203 - Create_spark - INFO - get_spark_object method started
2024-08-01 09:48:14,204 - Create_spark - INFO - master is local
2024-08-01 09:48:22,085 - Create_spark - INFO - Spark object created.....
2024-08-01 09:48:22,085 - root - INFO - Validating spark object..........
2024-08-01 09:48:22,085 - Validate - WARNING - started the get_current_date method...
2024-08-01 09:48:26,368 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 1))]
2024-08-01 09:48:26,369 - Validate - WARNING - Validation done , go frwd...
2024-08-01 09:48:26,369 - root - INFO - reading file which is of type parquet
2024-08-01 09:48:27,454 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-01 09:48:29,433 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-01 09:48:29,434 - root - INFO - reading file which is of type csv
2024-08-01 09:48:35,210 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-01 09:48:36,548 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-01 09:48:36,552 - root - INFO - Implementing data Processing now....
2024-08-01 09:48:36,552 - Data_processing - WARNING - Cleaning data method started
2024-08-01 09:48:43,670 - Data_processing - WARNING - Cleaning data method completed
2024-08-01 09:48:43,670 - root - INFO - Application done
2024-08-01 10:38:15,726 - root - INFO - i am in the main method..
2024-08-01 10:38:15,726 - root - INFO - calling spark object
2024-08-01 10:38:15,726 - Create_spark - INFO - get_spark_object method started
2024-08-01 10:38:15,726 - Create_spark - INFO - master is local
2024-08-01 10:38:23,301 - Create_spark - INFO - Spark object created.....
2024-08-01 10:38:23,301 - root - INFO - Validating spark object..........
2024-08-01 10:38:23,301 - Validate - WARNING - started the get_current_date method...
2024-08-01 10:38:27,387 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 1))]
2024-08-01 10:38:27,387 - Validate - WARNING - Validation done , go frwd...
2024-08-01 10:38:27,388 - root - INFO - reading file which is of type parquet
2024-08-01 10:38:28,219 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-01 10:38:30,122 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-01 10:38:30,124 - root - INFO - reading file which is of type csv
2024-08-01 10:38:35,396 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-01 10:38:36,641 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-01 10:38:36,644 - root - INFO - Implementing data Processing now....
2024-08-01 10:38:36,645 - Data_processing - WARNING - Cleaning data method started
2024-08-01 10:38:44,056 - Data_processing - WARNING - Cleaning data method completed
2024-08-01 10:38:44,056 - root - INFO - Application done
2024-08-01 10:48:39,260 - root - INFO - i am in the main method..
2024-08-01 10:48:39,269 - root - INFO - calling spark object
2024-08-01 10:48:39,270 - Create_spark - INFO - get_spark_object method started
2024-08-01 10:48:39,270 - Create_spark - INFO - master is local
2024-08-01 10:48:46,910 - Create_spark - INFO - Spark object created.....
2024-08-01 10:48:46,910 - root - INFO - Validating spark object..........
2024-08-01 10:48:46,910 - Validate - WARNING - started the get_current_date method...
2024-08-01 10:48:51,187 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 1))]
2024-08-01 10:48:51,187 - Validate - WARNING - Validation done , go frwd...
2024-08-01 10:48:51,188 - root - INFO - reading file which is of type parquet
2024-08-01 10:48:52,210 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-01 10:48:53,975 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-01 10:48:53,975 - root - INFO - reading file which is of type csv
2024-08-01 10:48:59,602 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-01 10:49:01,121 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-01 10:49:01,130 - root - INFO - Implementing data Processing now....
2024-08-01 10:49:01,131 - Data_processing - WARNING - Cleaning data method started
2024-08-01 10:49:08,436 - Data_processing - WARNING - Cleaning data method completed
2024-08-01 10:49:08,469 - root - INFO - Data transformed successfully
2024-08-01 10:49:08,664 - root - INFO - Application done
2024-08-01 10:49:46,443 - root - INFO - i am in the main method..
2024-08-01 10:49:46,444 - root - INFO - calling spark object
2024-08-01 10:49:46,444 - Create_spark - INFO - get_spark_object method started
2024-08-01 10:49:46,444 - Create_spark - INFO - master is local
2024-08-01 10:49:53,148 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.
: java.lang.AssertionError: assertion failed: Expected hostname or IPv6 IP enclosed in [] but got 2401:4900:1c27:80b2:3dd:3667:3942:fece
	at scala.Predef$.assert(Predef.scala:223)
	at org.apache.spark.util.Utils$.checkHost(Utils.scala:1108)
	at org.apache.spark.executor.Executor.<init>(Executor.scala:89)
	at org.apache.spark.scheduler.local.LocalEndpoint.<init>(LocalSchedulerBackend.scala:64)
	at org.apache.spark.scheduler.local.LocalSchedulerBackend.start(LocalSchedulerBackend.scala:132)
	at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:220)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:581)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)

2024-08-01 10:50:13,377 - root - INFO - i am in the main method..
2024-08-01 10:50:13,377 - root - INFO - calling spark object
2024-08-01 10:50:13,377 - Create_spark - INFO - get_spark_object method started
2024-08-01 10:50:13,377 - Create_spark - INFO - master is local
2024-08-01 10:50:19,726 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.
: java.lang.AssertionError: assertion failed: Expected hostname or IPv6 IP enclosed in [] but got 2401:4900:1c27:80b2:3dd:3667:3942:fece
	at scala.Predef$.assert(Predef.scala:223)
	at org.apache.spark.util.Utils$.checkHost(Utils.scala:1108)
	at org.apache.spark.executor.Executor.<init>(Executor.scala:89)
	at org.apache.spark.scheduler.local.LocalEndpoint.<init>(LocalSchedulerBackend.scala:64)
	at org.apache.spark.scheduler.local.LocalSchedulerBackend.start(LocalSchedulerBackend.scala:132)
	at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:220)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:581)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)

2024-08-01 10:50:38,154 - root - INFO - i am in the main method..
2024-08-01 10:50:38,155 - root - INFO - calling spark object
2024-08-01 10:50:38,155 - Create_spark - INFO - get_spark_object method started
2024-08-01 10:50:38,155 - Create_spark - INFO - master is local
2024-08-01 10:50:44,553 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.
: java.lang.AssertionError: assertion failed: Expected hostname or IPv6 IP enclosed in [] but got 2401:4900:1c27:80b2:3dd:3667:3942:fece
	at scala.Predef$.assert(Predef.scala:223)
	at org.apache.spark.util.Utils$.checkHost(Utils.scala:1108)
	at org.apache.spark.executor.Executor.<init>(Executor.scala:89)
	at org.apache.spark.scheduler.local.LocalEndpoint.<init>(LocalSchedulerBackend.scala:64)
	at org.apache.spark.scheduler.local.LocalSchedulerBackend.start(LocalSchedulerBackend.scala:132)
	at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:220)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:581)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)

2024-08-01 10:51:17,199 - root - INFO - i am in the main method..
2024-08-01 10:51:17,200 - root - INFO - calling spark object
2024-08-01 10:51:17,200 - Create_spark - INFO - get_spark_object method started
2024-08-01 10:51:17,200 - Create_spark - INFO - master is local
2024-08-01 10:51:22,849 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.
: java.lang.AssertionError: assertion failed: Expected hostname or IPv6 IP enclosed in [] but got 2401:4900:1c27:80b2:3dd:3667:3942:fece
	at scala.Predef$.assert(Predef.scala:223)
	at org.apache.spark.util.Utils$.checkHost(Utils.scala:1108)
	at org.apache.spark.executor.Executor.<init>(Executor.scala:89)
	at org.apache.spark.scheduler.local.LocalEndpoint.<init>(LocalSchedulerBackend.scala:64)
	at org.apache.spark.scheduler.local.LocalSchedulerBackend.start(LocalSchedulerBackend.scala:132)
	at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:220)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:581)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)

2024-08-01 10:52:43,381 - root - INFO - i am in the main method..
2024-08-01 10:52:43,546 - root - INFO - calling spark object
2024-08-01 10:52:43,546 - Create_spark - INFO - get_spark_object method started
2024-08-01 10:52:43,546 - Create_spark - INFO - master is local
2024-08-01 10:52:50,831 - Create_spark - INFO - Spark object created.....
2024-08-01 10:52:50,831 - root - INFO - Validating spark object..........
2024-08-01 10:52:50,831 - Validate - WARNING - started the get_current_date method...
2024-08-01 10:52:54,863 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 1))]
2024-08-01 10:52:54,863 - Validate - WARNING - Validation done , go frwd...
2024-08-01 10:52:54,864 - root - INFO - reading file which is of type parquet
2024-08-01 10:52:55,653 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-01 10:52:57,690 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-01 10:52:57,691 - root - INFO - reading file which is of type csv
2024-08-01 10:53:02,825 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-01 10:53:04,440 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-01 10:53:04,443 - root - INFO - Implementing data Processing now....
2024-08-01 10:53:04,443 - Data_processing - WARNING - Cleaning data method started
2024-08-01 10:53:15,060 - Data_processing - WARNING - Cleaning data method completed
2024-08-01 10:53:15,097 - root - INFO - Data transformed successfully
2024-08-01 10:53:16,463 - root - INFO - Application done
2024-08-01 10:53:22,689 - root - INFO - i am in the main method..
2024-08-01 10:53:22,689 - root - INFO - calling spark object
2024-08-01 10:53:22,689 - Create_spark - INFO - get_spark_object method started
2024-08-01 10:53:22,690 - Create_spark - INFO - master is local
2024-08-01 10:53:29,941 - Create_spark - INFO - Spark object created.....
2024-08-01 10:53:29,941 - root - INFO - Validating spark object..........
2024-08-01 10:53:29,941 - Validate - WARNING - started the get_current_date method...
2024-08-01 10:53:34,096 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 1))]
2024-08-01 10:53:34,096 - Validate - WARNING - Validation done , go frwd...
2024-08-01 10:53:34,097 - root - INFO - reading file which is of type parquet
2024-08-01 10:53:35,167 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-01 10:53:37,229 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-01 10:53:37,230 - root - INFO - reading file which is of type csv
2024-08-01 10:53:42,872 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-01 10:53:44,343 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-01 10:53:44,348 - root - INFO - Implementing data Processing now....
2024-08-01 10:53:44,349 - Data_processing - WARNING - Cleaning data method started
2024-08-01 10:53:51,943 - Data_processing - WARNING - Cleaning data method completed
2024-08-01 10:53:51,970 - root - INFO - Data transformed successfully
2024-08-01 10:53:52,274 - root - INFO - Application done
2024-08-01 10:54:17,105 - root - INFO - i am in the main method..
2024-08-01 10:54:17,105 - root - INFO - calling spark object
2024-08-01 10:54:17,105 - Create_spark - INFO - get_spark_object method started
2024-08-01 10:54:17,105 - Create_spark - INFO - master is local
2024-08-01 10:54:24,132 - Create_spark - INFO - Spark object created.....
2024-08-01 10:54:24,132 - root - INFO - Validating spark object..........
2024-08-01 10:54:24,133 - Validate - WARNING - started the get_current_date method...
2024-08-01 10:54:28,528 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 1))]
2024-08-01 10:54:28,528 - Validate - WARNING - Validation done , go frwd...
2024-08-01 10:54:28,529 - root - INFO - reading file which is of type parquet
2024-08-01 10:54:29,371 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-01 10:54:31,145 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-01 10:54:31,146 - root - INFO - reading file which is of type csv
2024-08-01 10:54:36,886 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-01 10:54:38,723 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-01 10:54:38,727 - root - INFO - Implementing data Processing now....
2024-08-01 10:54:38,728 - Data_processing - WARNING - Cleaning data method started
2024-08-01 10:54:47,055 - Data_processing - WARNING - Cleaning data method completed
2024-08-01 10:54:47,074 - root - INFO - Data transformed successfully
2024-08-01 10:54:47,264 - root - INFO - Application done
2024-08-01 18:31:31,591 - root - INFO - i am in the main method..
2024-08-01 18:31:31,592 - root - INFO - calling spark object
2024-08-01 18:31:31,592 - Create_spark - INFO - get_spark_object method started
2024-08-01 18:31:31,592 - Create_spark - INFO - master is local
2024-08-01 18:31:40,132 - Create_spark - INFO - Spark object created.....
2024-08-01 18:31:40,132 - root - INFO - Validating spark object..........
2024-08-01 18:31:40,132 - Validate - WARNING - started the get_current_date method...
2024-08-01 18:31:44,780 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 1))]
2024-08-01 18:31:44,781 - Validate - WARNING - Validation done , go frwd...
2024-08-01 18:31:44,781 - root - INFO - reading file which is of type parquet
2024-08-01 18:31:45,684 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-01 18:31:47,787 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-01 18:31:47,788 - root - INFO - reading file which is of type csv
2024-08-01 18:31:52,948 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-01 18:31:54,183 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-01 18:31:54,186 - root - INFO - Implementing data Processing now....
2024-08-01 18:31:54,187 - Data_processing - WARNING - Cleaning data method started
2024-08-01 18:32:00,922 - Data_processing - WARNING - Cleaning data method completed
2024-08-01 18:32:01,023 - root - INFO - Data transformed successfully
2024-08-01 18:32:01,461 - root - INFO - Application done
2024-08-01 18:35:40,081 - root - INFO - i am in the main method..
2024-08-01 18:35:40,081 - root - INFO - calling spark object
2024-08-01 18:35:40,082 - Create_spark - INFO - get_spark_object method started
2024-08-01 18:35:40,082 - Create_spark - INFO - master is local
2024-08-01 18:35:48,806 - Create_spark - INFO - Spark object created.....
2024-08-01 18:35:48,806 - root - INFO - Validating spark object..........
2024-08-01 18:35:48,806 - Validate - WARNING - started the get_current_date method...
2024-08-01 18:35:53,218 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 1))]
2024-08-01 18:35:53,218 - Validate - WARNING - Validation done , go frwd...
2024-08-01 18:35:53,219 - root - INFO - reading file which is of type parquet
2024-08-01 18:35:54,364 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-01 18:35:56,228 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-01 18:35:56,229 - root - INFO - reading file which is of type csv
2024-08-01 18:36:02,130 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-01 18:36:03,725 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-01 18:36:03,728 - root - INFO - Implementing data Processing now....
2024-08-01 18:36:03,729 - Data_processing - WARNING - Cleaning data method started
2024-08-01 18:36:10,737 - Data_processing - WARNING - Cleaning data method completed
2024-08-01 18:36:10,822 - root - INFO - Data transformed successfully
2024-08-01 18:36:11,214 - root - INFO - Application done
2024-08-01 18:37:12,748 - root - INFO - i am in the main method..
2024-08-01 18:37:12,748 - root - INFO - calling spark object
2024-08-01 18:37:12,748 - Create_spark - INFO - get_spark_object method started
2024-08-01 18:37:12,748 - Create_spark - INFO - master is local
2024-08-01 18:37:20,862 - Create_spark - INFO - Spark object created.....
2024-08-01 18:37:20,863 - root - INFO - Validating spark object..........
2024-08-01 18:37:20,863 - Validate - WARNING - started the get_current_date method...
2024-08-01 18:37:25,509 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 1))]
2024-08-01 18:37:25,509 - Validate - WARNING - Validation done , go frwd...
2024-08-01 18:37:25,511 - root - INFO - reading file which is of type parquet
2024-08-01 18:37:26,512 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-01 18:37:30,570 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-01 18:37:30,571 - root - INFO - reading file which is of type csv
2024-08-01 18:37:37,214 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-01 18:37:38,798 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-01 18:37:38,802 - root - INFO - Implementing data Processing now....
2024-08-01 18:37:38,802 - Data_processing - WARNING - Cleaning data method started
2024-08-01 18:37:47,030 - Data_processing - WARNING - Cleaning data method completed
2024-08-01 18:37:47,033 - root - INFO - Transforming data DataFrame[city: string, state_id: string, state_name: string, county_name: string, population: int, zips: string]
2024-08-01 18:37:47,191 - root - INFO - Data transformed successfully
2024-08-01 18:37:47,858 - root - INFO - Application done
2024-08-01 18:38:33,652 - root - INFO - i am in the main method..
2024-08-01 18:38:33,653 - root - INFO - calling spark object
2024-08-01 18:38:33,655 - Create_spark - INFO - get_spark_object method started
2024-08-01 18:38:33,655 - Create_spark - INFO - master is local
2024-08-01 18:38:40,347 - root - ERROR - KeyboardInterrupt while sending command.
Traceback (most recent call last):
  File "C:\Python38\lib\socket.py", line 669, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Admin\PycharmProjects\youtubeproject\.venv\lib\site-packages\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "C:\Users\Admin\PycharmProjects\youtubeproject\.venv\lib\site-packages\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "C:\Python38\lib\socket.py", line 669, in readinto
    return self._sock.recv_into(b)
KeyboardInterrupt
2024-08-01 18:38:41,150 - root - INFO - i am in the main method..
2024-08-01 18:38:41,150 - root - INFO - calling spark object
2024-08-01 18:38:41,150 - Create_spark - INFO - get_spark_object method started
2024-08-01 18:38:41,150 - Create_spark - INFO - master is local
2024-08-01 18:38:55,275 - Create_spark - INFO - Spark object created.....
2024-08-01 18:38:55,275 - root - INFO - Validating spark object..........
2024-08-01 18:38:55,275 - Validate - WARNING - started the get_current_date method...
2024-08-01 18:38:57,258 - root - INFO - Exception while sending command.
Traceback (most recent call last):
  File "C:\Python38\lib\socket.py", line 669, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Admin\PycharmProjects\youtubeproject\.venv\lib\site-packages\py4j\clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Admin\PycharmProjects\youtubeproject\.venv\lib\site-packages\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "C:\Users\Admin\PycharmProjects\youtubeproject\.venv\lib\site-packages\py4j\clientserver.py", line 506, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending
2024-08-01 18:38:59,315 - root - ERROR - Exception while sending command.
Traceback (most recent call last):
  File "C:\Python38\lib\socket.py", line 669, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Admin\PycharmProjects\youtubeproject\.venv\lib\site-packages\py4j\clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Admin\PycharmProjects\youtubeproject\.venv\lib\site-packages\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "C:\Users\Admin\PycharmProjects\youtubeproject\.venv\lib\site-packages\py4j\clientserver.py", line 506, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Admin\PycharmProjects\youtubeproject\.venv\lib\site-packages\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "C:\Python38\lib\socket.py", line 669, in readinto
    return self._sock.recv_into(b)
  File "C:\Users\Admin\Downloads\spark-3.2.3-bin-hadoop3.2\spark-3.2.3-bin-hadoop3.2\python\pyspark\context.py", line 292, in signal_handler
    self.cancelAllJobs()
  File "C:\Users\Admin\Downloads\spark-3.2.3-bin-hadoop3.2\spark-3.2.3-bin-hadoop3.2\python\pyspark\context.py", line 1195, in cancelAllJobs
    self._jsc.sc().cancelAllJobs()
  File "C:\Users\Admin\PycharmProjects\youtubeproject\.venv\lib\site-packages\py4j\java_gateway.py", line 1320, in __call__
    answer = self.gateway_client.send_command(command)
  File "C:\Users\Admin\PycharmProjects\youtubeproject\.venv\lib\site-packages\py4j\java_gateway.py", line 1053, in send_command
    response = self.send_command(command, binary=binary)
  File "C:\Users\Admin\PycharmProjects\youtubeproject\.venv\lib\site-packages\py4j\java_gateway.py", line 1036, in send_command
    connection = self._get_connection()
  File "C:\Users\Admin\PycharmProjects\youtubeproject\.venv\lib\site-packages\py4j\clientserver.py", line 284, in _get_connection
    connection = self._create_new_connection()
  File "C:\Users\Admin\PycharmProjects\youtubeproject\.venv\lib\site-packages\py4j\clientserver.py", line 291, in _create_new_connection
    connection.connect_to_java_server()
  File "C:\Users\Admin\PycharmProjects\youtubeproject\.venv\lib\site-packages\py4j\clientserver.py", line 438, in connect_to_java_server
    self.socket.connect((self.java_address, self.java_port))
ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Admin\PycharmProjects\youtubeproject\.venv\lib\site-packages\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "C:\Users\Admin\PycharmProjects\youtubeproject\.venv\lib\site-packages\py4j\clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2024-08-01 18:38:59,606 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o23.sql
2024-08-01 18:39:00,327 - root - INFO - i am in the main method..
2024-08-01 18:39:00,327 - root - INFO - calling spark object
2024-08-01 18:39:00,327 - Create_spark - INFO - get_spark_object method started
2024-08-01 18:39:00,327 - Create_spark - INFO - master is local
2024-08-01 18:39:07,451 - Create_spark - INFO - Spark object created.....
2024-08-01 18:39:07,452 - root - INFO - Validating spark object..........
2024-08-01 18:39:07,452 - Validate - WARNING - started the get_current_date method...
2024-08-01 18:39:11,851 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 1))]
2024-08-01 18:39:11,851 - Validate - WARNING - Validation done , go frwd...
2024-08-01 18:39:11,852 - root - INFO - reading file which is of type parquet
2024-08-01 18:39:12,695 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-01 18:39:14,613 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-01 18:39:14,614 - root - INFO - reading file which is of type csv
2024-08-01 18:39:20,839 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-01 18:39:22,257 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-01 18:39:22,260 - root - INFO - Implementing data Processing now....
2024-08-01 18:39:22,260 - Data_processing - WARNING - Cleaning data method started
2024-08-01 18:39:29,476 - Data_processing - WARNING - Cleaning data method completed
2024-08-01 18:39:29,478 - root - INFO - Transforming data DataFrame[city: string, state_id: string, state_name: string, county_name: string, population: int, zips: string]
2024-08-01 18:39:29,554 - root - INFO - Data transformed successfully
2024-08-01 18:39:29,971 - root - INFO - Application done
2024-08-01 18:40:25,132 - root - INFO - i am in the main method..
2024-08-01 18:40:25,132 - root - INFO - calling spark object
2024-08-01 18:40:25,133 - Create_spark - INFO - get_spark_object method started
2024-08-01 18:40:25,133 - Create_spark - INFO - master is local
2024-08-01 18:40:33,173 - Create_spark - INFO - Spark object created.....
2024-08-01 18:40:33,173 - root - INFO - Validating spark object..........
2024-08-01 18:40:33,173 - Validate - WARNING - started the get_current_date method...
2024-08-01 18:40:37,785 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 1))]
2024-08-01 18:40:37,785 - Validate - WARNING - Validation done , go frwd...
2024-08-01 18:40:37,785 - root - INFO - reading file which is of type parquet
2024-08-01 18:40:38,729 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-01 18:40:40,566 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-01 18:40:40,567 - root - INFO - reading file which is of type csv
2024-08-01 18:40:46,477 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-01 18:40:47,717 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-01 18:40:47,720 - root - INFO - Implementing data Processing now....
2024-08-01 18:40:47,720 - Data_processing - WARNING - Cleaning data method started
2024-08-01 18:40:54,717 - Data_processing - WARNING - Cleaning data method completed
2024-08-01 18:40:54,718 - root - INFO - Transforming data ['city', 'state_id', 'state_name', 'county_name', 'population', 'zips']
2024-08-01 18:40:54,822 - root - INFO - Data transformed successfully
2024-08-01 18:40:55,294 - root - INFO - Application done
2024-08-01 18:42:28,148 - root - INFO - i am in the main method..
2024-08-01 18:42:28,148 - root - INFO - calling spark object
2024-08-01 18:42:28,149 - Create_spark - INFO - get_spark_object method started
2024-08-01 18:42:28,149 - Create_spark - INFO - master is local
2024-08-01 18:42:36,315 - Create_spark - INFO - Spark object created.....
2024-08-01 18:42:36,316 - root - INFO - Validating spark object..........
2024-08-01 18:42:36,316 - Validate - WARNING - started the get_current_date method...
2024-08-01 18:42:41,192 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 1))]
2024-08-01 18:42:41,192 - Validate - WARNING - Validation done , go frwd...
2024-08-01 18:42:41,193 - root - INFO - reading file which is of type parquet
2024-08-01 18:42:42,783 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-01 18:42:45,000 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-01 18:42:45,001 - root - INFO - reading file which is of type csv
2024-08-01 18:42:51,104 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-01 18:42:52,357 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-01 18:42:52,361 - root - INFO - Implementing data Processing now....
2024-08-01 18:42:52,361 - Data_processing - WARNING - Cleaning data method started
2024-08-01 18:42:59,897 - Data_processing - WARNING - Cleaning data method completed
2024-08-01 18:43:00,077 - root - INFO - Data transformed successfully
2024-08-01 18:43:00,503 - root - INFO - Application done
2024-08-01 18:43:45,028 - root - INFO - i am in the main method..
2024-08-01 18:43:45,028 - root - INFO - calling spark object
2024-08-01 18:43:45,028 - Create_spark - INFO - get_spark_object method started
2024-08-01 18:43:45,028 - Create_spark - INFO - master is local
2024-08-01 18:43:53,391 - Create_spark - INFO - Spark object created.....
2024-08-01 18:43:53,391 - root - INFO - Validating spark object..........
2024-08-01 18:43:53,391 - Validate - WARNING - started the get_current_date method...
2024-08-01 18:43:58,074 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 1))]
2024-08-01 18:43:58,074 - Validate - WARNING - Validation done , go frwd...
2024-08-01 18:43:58,075 - root - INFO - reading file which is of type parquet
2024-08-01 18:43:59,042 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-01 18:44:01,064 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-01 18:44:01,065 - root - INFO - reading file which is of type csv
2024-08-01 18:44:07,165 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-01 18:44:08,320 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-01 18:44:08,324 - root - INFO - Implementing data Processing now....
2024-08-01 18:44:08,325 - Data_processing - WARNING - Cleaning data method started
2024-08-01 18:44:15,394 - Data_processing - WARNING - Cleaning data method completed
2024-08-01 18:44:15,476 - root - INFO - Data transformed successfully
2024-08-01 18:44:15,877 - root - INFO - Application done
2024-08-01 19:38:31,769 - root - INFO - i am in the main method..
2024-08-01 19:38:31,772 - root - INFO - calling spark object
2024-08-01 19:38:31,773 - Create_spark - INFO - get_spark_object method started
2024-08-01 19:38:31,774 - Create_spark - INFO - master is local
2024-08-01 19:38:40,295 - Create_spark - INFO - Spark object created.....
2024-08-01 19:38:40,295 - root - INFO - Validating spark object..........
2024-08-01 19:38:40,295 - Validate - WARNING - started the get_current_date method...
2024-08-01 19:38:44,639 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 1))]
2024-08-01 19:38:44,639 - Validate - WARNING - Validation done , go frwd...
2024-08-01 19:38:44,639 - root - INFO - reading file which is of type parquet
2024-08-01 19:38:45,542 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-01 19:38:47,606 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-01 19:38:47,608 - root - INFO - reading file which is of type csv
2024-08-01 19:38:53,129 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-01 19:38:54,233 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-01 19:38:54,235 - root - INFO - Implementing data Processing now....
2024-08-01 19:38:54,235 - Data_processing - WARNING - Cleaning data method started
2024-08-01 19:39:00,853 - Data_processing - WARNING - Cleaning data method completed
2024-08-01 19:39:01,014 - root - INFO - Data transformed successfully
2024-08-01 19:39:13,002 - root - INFO - Application done
2024-08-01 19:40:15,344 - root - INFO - i am in the main method..
2024-08-01 19:40:15,345 - root - INFO - calling spark object
2024-08-01 19:40:15,345 - Create_spark - INFO - get_spark_object method started
2024-08-01 19:40:15,345 - Create_spark - INFO - master is local
2024-08-01 19:40:23,171 - Create_spark - INFO - Spark object created.....
2024-08-01 19:40:23,171 - root - INFO - Validating spark object..........
2024-08-01 19:40:23,171 - Validate - WARNING - started the get_current_date method...
2024-08-01 19:40:27,632 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 1))]
2024-08-01 19:40:27,632 - Validate - WARNING - Validation done , go frwd...
2024-08-01 19:40:27,633 - root - INFO - reading file which is of type parquet
2024-08-01 19:40:28,487 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-01 19:40:30,158 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-01 19:40:30,159 - root - INFO - reading file which is of type csv
2024-08-01 19:40:36,249 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-01 19:40:37,674 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-01 19:40:37,678 - root - INFO - Implementing data Processing now....
2024-08-01 19:40:37,678 - Data_processing - WARNING - Cleaning data method started
2024-08-01 19:40:45,677 - Data_processing - WARNING - Cleaning data method completed
2024-08-01 19:40:45,809 - root - INFO - Data transformed successfully
2024-08-01 19:40:56,558 - root - INFO - Application done
2024-08-01 19:51:31,701 - root - INFO - i am in the main method..
2024-08-01 19:51:31,701 - root - INFO - calling spark object
2024-08-01 19:51:31,701 - Create_spark - INFO - get_spark_object method started
2024-08-01 19:51:31,701 - Create_spark - INFO - master is local
2024-08-01 19:51:44,945 - Create_spark - INFO - Spark object created.....
2024-08-01 19:51:44,945 - root - INFO - Validating spark object..........
2024-08-01 19:51:44,945 - Validate - WARNING - started the get_current_date method...
2024-08-01 19:51:56,722 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 1))]
2024-08-01 19:51:56,722 - Validate - WARNING - Validation done , go frwd...
2024-08-01 19:51:56,722 - root - INFO - reading file which is of type parquet
2024-08-01 19:51:59,430 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-01 19:52:03,699 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-01 19:52:03,699 - root - INFO - reading file which is of type csv
2024-08-01 19:52:19,670 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-01 19:52:22,514 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-01 19:52:22,519 - root - INFO - Implementing data Processing now....
2024-08-01 19:52:22,519 - Data_processing - WARNING - Cleaning data method started
2024-08-01 19:52:44,397 - Data_processing - WARNING - Cleaning data method completed
2024-08-01 19:52:44,949 - root - INFO - Data transformed successfully
2024-08-01 19:53:16,660 - root - INFO - Application done
2024-08-01 19:58:10,135 - root - INFO - i am in the main method..
2024-08-01 19:58:10,136 - root - INFO - calling spark object
2024-08-01 19:58:10,136 - Create_spark - INFO - get_spark_object method started
2024-08-01 19:58:10,136 - Create_spark - INFO - master is local
2024-08-01 19:58:24,308 - Create_spark - INFO - Spark object created.....
2024-08-01 19:58:24,308 - root - INFO - Validating spark object..........
2024-08-01 19:58:24,308 - Validate - WARNING - started the get_current_date method...
2024-08-01 19:58:35,643 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 1))]
2024-08-01 19:58:35,644 - Validate - WARNING - Validation done , go frwd...
2024-08-01 19:58:35,645 - root - INFO - reading file which is of type parquet
2024-08-01 19:58:38,001 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-01 19:58:42,191 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-01 19:58:42,192 - root - INFO - reading file which is of type csv
2024-08-01 19:58:57,813 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-01 19:59:00,818 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-01 19:59:00,823 - root - INFO - Implementing data Processing now....
2024-08-01 19:59:00,823 - Data_processing - WARNING - Cleaning data method started
2024-08-01 19:59:20,341 - Data_processing - WARNING - Cleaning data method completed
2024-08-01 19:59:20,790 - root - INFO - Data transformed successfully
2024-08-01 20:32:12,671 - root - INFO - i am in the main method..
2024-08-01 20:32:12,671 - root - INFO - calling spark object
2024-08-01 20:32:12,671 - Create_spark - INFO - get_spark_object method started
2024-08-01 20:32:12,672 - Create_spark - INFO - master is local
2024-08-01 20:32:28,753 - Create_spark - INFO - Spark object created.....
2024-08-01 20:32:28,753 - root - INFO - Validating spark object..........
2024-08-01 20:32:28,755 - Validate - WARNING - started the get_current_date method...
2024-08-01 20:32:41,037 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 1))]
2024-08-01 20:32:41,037 - Validate - WARNING - Validation done , go frwd...
2024-08-01 20:32:41,037 - root - INFO - reading file which is of type parquet
2024-08-01 20:32:43,290 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-01 20:32:49,114 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-01 20:32:49,114 - root - INFO - reading file which is of type csv
2024-08-01 20:33:03,497 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-01 20:33:05,382 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-01 20:33:05,388 - root - INFO - Implementing data Processing now....
2024-08-01 20:33:05,388 - Data_processing - WARNING - Cleaning data method started
2024-08-01 20:33:15,210 - Data_processing - WARNING - Cleaning data method completed
2024-08-01 20:33:16,405 - root - INFO - Data transformed successfully
2024-08-01 21:16:57,588 - root - INFO - i am in the main method..
2024-08-01 21:16:57,593 - root - INFO - calling spark object
2024-08-01 21:16:57,593 - Create_spark - INFO - get_spark_object method started
2024-08-01 21:16:57,593 - Create_spark - INFO - master is local
2024-08-01 21:17:07,104 - Create_spark - INFO - Spark object created.....
2024-08-01 21:17:07,105 - root - INFO - Validating spark object..........
2024-08-01 21:17:07,105 - Validate - WARNING - started the get_current_date method...
2024-08-01 21:17:11,710 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 1))]
2024-08-01 21:17:11,710 - Validate - WARNING - Validation done , go frwd...
2024-08-01 21:17:11,711 - root - INFO - reading file which is of type parquet
2024-08-01 21:17:12,653 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-01 21:17:14,721 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-01 21:17:14,722 - root - INFO - reading file which is of type csv
2024-08-01 21:17:20,654 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-01 21:17:21,922 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-01 21:17:21,926 - root - INFO - Implementing data Processing now....
2024-08-01 21:17:21,926 - Data_processing - WARNING - Cleaning data method started
2024-08-01 21:17:29,660 - Data_processing - WARNING - Cleaning data method completed
2024-08-01 21:17:43,385 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o225.save.
: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:736)
	at org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:271)
	at org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:287)
	at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)
	at org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:178)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:192)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1886)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1846)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1819)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)
	at org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)
	at org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)
	at org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)
	at org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)
	at org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)
	at org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)
	at org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)
	at org.apache.spark.util.Utils$.createTempDir(Utils.scala:335)
	at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:344)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:898)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1043)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1052)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 22 more

2024-08-01 21:49:19,927 - root - INFO - i am in the main method..
2024-08-01 21:49:19,928 - root - INFO - calling spark object
2024-08-01 21:49:19,928 - Create_spark - INFO - get_spark_object method started
2024-08-01 21:49:19,928 - Create_spark - INFO - master is local
2024-08-01 21:49:27,314 - Create_spark - INFO - Spark object created.....
2024-08-01 21:49:27,314 - root - INFO - Validating spark object..........
2024-08-01 21:49:27,314 - Validate - WARNING - started the get_current_date method...
2024-08-01 21:49:31,394 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 1))]
2024-08-01 21:49:31,394 - Validate - WARNING - Validation done , go frwd...
2024-08-01 21:49:31,395 - root - INFO - reading file which is of type parquet
2024-08-01 21:49:32,284 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-01 21:49:34,124 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-01 21:49:34,125 - root - INFO - reading file which is of type csv
2024-08-01 21:49:39,533 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-01 21:49:40,861 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-01 21:49:40,865 - root - INFO - Implementing data Processing now....
2024-08-01 21:49:40,865 - Data_processing - WARNING - Cleaning data method started
2024-08-01 21:49:47,732 - Data_processing - WARNING - Cleaning data method completed
2024-08-01 21:49:57,789 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o225.save.
: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:736)
	at org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:271)
	at org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:287)
	at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)
	at org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:178)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:192)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1886)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1846)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1819)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)
	at org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)
	at org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)
	at org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)
	at org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)
	at org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)
	at org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)
	at org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)
	at org.apache.spark.util.Utils$.createTempDir(Utils.scala:335)
	at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:344)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:898)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1043)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1052)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 22 more

2024-08-03 09:21:52,280 - root - INFO - i am in the main method..
2024-08-03 09:21:52,281 - root - INFO - calling spark object
2024-08-03 09:21:52,281 - Create_spark - INFO - get_spark_object method started
2024-08-03 09:21:52,281 - Create_spark - INFO - master is local
2024-08-03 09:22:00,305 - Create_spark - INFO - Spark object created.....
2024-08-03 09:22:00,305 - root - INFO - Validating spark object..........
2024-08-03 09:22:00,305 - Validate - WARNING - started the get_current_date method...
2024-08-03 09:22:04,731 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 3))]
2024-08-03 09:22:04,731 - Validate - WARNING - Validation done , go frwd...
2024-08-03 09:22:04,732 - root - INFO - reading file which is of type parquet
2024-08-03 09:22:05,730 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-03 09:22:07,590 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-03 09:22:07,590 - root - INFO - reading file which is of type csv
2024-08-03 09:22:13,445 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-03 09:22:15,017 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-03 09:22:15,021 - root - INFO - Implementing data Processing now....
2024-08-03 09:22:15,022 - Data_processing - WARNING - Cleaning data method started
2024-08-03 09:22:22,126 - Data_processing - WARNING - Cleaning data method completed
2024-08-03 09:22:35,190 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o225.save.
: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:736)
	at org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:271)
	at org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:287)
	at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)
	at org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:178)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:192)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1886)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1846)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1819)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)
	at org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)
	at org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)
	at org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)
	at org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)
	at org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)
	at org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)
	at org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)
	at org.apache.spark.util.Utils$.createTempDir(Utils.scala:335)
	at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:344)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:898)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1043)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1052)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 22 more

2024-08-03 09:34:51,167 - root - INFO - i am in the main method..
2024-08-03 09:34:51,168 - root - INFO - calling spark object
2024-08-03 09:34:51,168 - Create_spark - INFO - get_spark_object method started
2024-08-03 09:34:51,168 - Create_spark - INFO - master is local
2024-08-03 09:34:58,995 - Create_spark - INFO - Spark object created.....
2024-08-03 09:34:58,995 - root - INFO - Validating spark object..........
2024-08-03 09:34:58,995 - Validate - WARNING - started the get_current_date method...
2024-08-03 09:35:03,529 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 3))]
2024-08-03 09:35:03,529 - Validate - WARNING - Validation done , go frwd...
2024-08-03 09:35:03,530 - root - INFO - reading file which is of type parquet
2024-08-03 09:35:04,574 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-03 09:35:06,340 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-03 09:35:06,341 - root - INFO - reading file which is of type csv
2024-08-03 09:35:11,949 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-03 09:35:13,136 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-03 09:35:13,139 - root - INFO - Implementing data Processing now....
2024-08-03 09:35:13,140 - Data_processing - WARNING - Cleaning data method started
2024-08-03 09:35:20,073 - Data_processing - WARNING - Cleaning data method completed
2024-08-03 09:35:31,294 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o225.save.
: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:736)
	at org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:271)
	at org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:287)
	at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)
	at org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:178)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:192)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1886)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1846)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1819)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)
	at org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)
	at org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)
	at org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)
	at org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)
	at org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)
	at org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)
	at org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)
	at org.apache.spark.util.Utils$.createTempDir(Utils.scala:335)
	at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:344)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:898)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1043)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1052)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 22 more

2024-08-03 09:54:20,622 - root - INFO - i am in the main method..
2024-08-03 09:54:20,623 - root - INFO - calling spark object
2024-08-03 09:54:20,623 - Create_spark - INFO - get_spark_object method started
2024-08-03 09:54:20,623 - Create_spark - INFO - master is local
2024-08-03 09:54:28,672 - Create_spark - INFO - Spark object created.....
2024-08-03 09:54:28,672 - root - INFO - Validating spark object..........
2024-08-03 09:54:28,673 - Validate - WARNING - started the get_current_date method...
2024-08-03 09:54:32,941 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 3))]
2024-08-03 09:54:32,941 - Validate - WARNING - Validation done , go frwd...
2024-08-03 09:54:32,941 - root - INFO - reading file which is of type parquet
2024-08-03 09:54:33,883 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-03 09:54:35,638 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-03 09:54:35,639 - root - INFO - reading file which is of type csv
2024-08-03 09:54:41,259 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-03 09:54:42,606 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-03 09:54:42,609 - root - INFO - Implementing data Processing now....
2024-08-03 09:54:42,609 - Data_processing - WARNING - Cleaning data method started
2024-08-03 09:54:49,403 - Data_processing - WARNING - Cleaning data method completed
2024-08-03 09:55:01,429 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o225.save.
: org.apache.spark.SparkException: Job aborted.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:496)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:261)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)
	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1215)
	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1420)
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:182)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:250)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:634)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:250)
	... 41 more

2024-08-03 09:56:11,668 - root - INFO - i am in the main method..
2024-08-03 09:56:11,668 - root - INFO - calling spark object
2024-08-03 09:56:11,668 - Create_spark - INFO - get_spark_object method started
2024-08-03 09:56:11,668 - Create_spark - INFO - master is local
2024-08-03 09:56:21,499 - Create_spark - INFO - Spark object created.....
2024-08-03 09:56:21,499 - root - INFO - Validating spark object..........
2024-08-03 09:56:21,499 - Validate - WARNING - started the get_current_date method...
2024-08-03 09:56:32,197 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 3))]
2024-08-03 09:56:32,197 - Validate - WARNING - Validation done , go frwd...
2024-08-03 09:56:32,197 - root - INFO - reading file which is of type parquet
2024-08-03 09:56:36,504 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-03 09:56:40,336 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-03 09:56:40,337 - root - INFO - reading file which is of type csv
2024-08-03 09:56:48,584 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-03 09:56:50,318 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-03 09:56:50,322 - root - INFO - Implementing data Processing now....
2024-08-03 09:56:50,322 - Data_processing - WARNING - Cleaning data method started
2024-08-03 09:57:02,935 - Data_processing - WARNING - Cleaning data method completed
2024-08-03 09:57:15,497 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o225.save.
: org.apache.spark.SparkException: Job aborted.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:496)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:261)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)
	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1215)
	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1420)
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:182)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:250)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:634)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:250)
	... 41 more

2024-08-03 09:58:08,586 - root - INFO - i am in the main method..
2024-08-03 09:58:08,586 - root - INFO - calling spark object
2024-08-03 09:58:08,586 - Create_spark - INFO - get_spark_object method started
2024-08-03 09:58:08,586 - Create_spark - INFO - master is local
2024-08-03 09:58:16,471 - Create_spark - INFO - Spark object created.....
2024-08-03 09:58:16,472 - root - INFO - Validating spark object..........
2024-08-03 09:58:16,472 - Validate - WARNING - started the get_current_date method...
2024-08-03 09:58:20,976 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 3))]
2024-08-03 09:58:20,976 - Validate - WARNING - Validation done , go frwd...
2024-08-03 09:58:20,976 - root - INFO - reading file which is of type parquet
2024-08-03 09:58:21,917 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-03 09:58:23,895 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-03 09:58:23,895 - root - INFO - reading file which is of type csv
2024-08-03 09:58:29,205 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-03 09:58:30,458 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-03 09:58:30,461 - root - INFO - Implementing data Processing now....
2024-08-03 09:58:30,461 - Data_processing - WARNING - Cleaning data method started
2024-08-03 09:58:37,176 - Data_processing - WARNING - Cleaning data method completed
2024-08-03 09:58:48,779 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o225.save.
: org.apache.spark.SparkException: Job aborted.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:496)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:261)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)
	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1215)
	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1420)
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:182)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:250)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:634)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:250)
	... 41 more

2024-08-03 10:02:23,399 - root - INFO - i am in the main method..
2024-08-03 10:02:23,400 - root - INFO - calling spark object
2024-08-03 10:02:23,400 - Create_spark - INFO - get_spark_object method started
2024-08-03 10:02:23,400 - Create_spark - INFO - master is local
2024-08-03 10:02:30,934 - Create_spark - INFO - Spark object created.....
2024-08-03 10:02:30,935 - root - INFO - Validating spark object..........
2024-08-03 10:02:30,935 - Validate - WARNING - started the get_current_date method...
2024-08-03 10:02:35,422 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 3))]
2024-08-03 10:02:35,422 - Validate - WARNING - Validation done , go frwd...
2024-08-03 10:02:35,423 - root - INFO - reading file which is of type parquet
2024-08-03 10:02:36,390 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-03 10:02:38,521 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-03 10:02:38,521 - root - INFO - reading file which is of type csv
2024-08-03 10:02:44,048 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-03 10:02:45,219 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-03 10:02:45,223 - root - INFO - Implementing data Processing now....
2024-08-03 10:02:45,223 - Data_processing - WARNING - Cleaning data method started
2024-08-03 10:02:52,623 - Data_processing - WARNING - Cleaning data method completed
2024-08-03 10:03:04,226 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o225.save.
: org.apache.spark.SparkException: Job aborted.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:496)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:261)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)
	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1215)
	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1420)
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:182)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:250)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:634)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:250)
	... 41 more

2024-08-03 10:03:45,132 - root - INFO - i am in the main method..
2024-08-03 10:03:45,132 - root - INFO - calling spark object
2024-08-03 10:03:45,133 - Create_spark - INFO - get_spark_object method started
2024-08-03 10:03:45,133 - Create_spark - INFO - master is local
2024-08-03 10:03:52,650 - Create_spark - INFO - Spark object created.....
2024-08-03 10:03:52,650 - root - INFO - Validating spark object..........
2024-08-03 10:03:52,650 - Validate - WARNING - started the get_current_date method...
2024-08-03 10:03:57,360 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 3))]
2024-08-03 10:03:57,361 - Validate - WARNING - Validation done , go frwd...
2024-08-03 10:03:57,361 - root - INFO - reading file which is of type parquet
2024-08-03 10:03:58,307 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-03 10:04:00,530 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-03 10:04:00,531 - root - INFO - reading file which is of type csv
2024-08-03 10:04:06,070 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-03 10:04:07,121 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-03 10:04:07,124 - root - INFO - Implementing data Processing now....
2024-08-03 10:04:07,125 - Data_processing - WARNING - Cleaning data method started
2024-08-03 10:04:13,815 - Data_processing - WARNING - Cleaning data method completed
2024-08-03 10:04:25,971 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o225.save.
: org.apache.spark.SparkException: Job aborted.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:496)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:261)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)
	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1215)
	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1420)
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:182)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:250)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:634)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:250)
	... 41 more

2024-08-03 10:05:19,908 - root - INFO - i am in the main method..
2024-08-03 10:05:19,908 - root - INFO - calling spark object
2024-08-03 10:05:19,908 - Create_spark - INFO - get_spark_object method started
2024-08-03 10:05:19,908 - Create_spark - INFO - master is local
2024-08-03 10:05:27,543 - Create_spark - INFO - Spark object created.....
2024-08-03 10:05:27,544 - root - INFO - Validating spark object..........
2024-08-03 10:05:27,544 - Validate - WARNING - started the get_current_date method...
2024-08-03 10:05:36,533 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 3))]
2024-08-03 10:05:36,533 - Validate - WARNING - Validation done , go frwd...
2024-08-03 10:05:36,540 - root - INFO - reading file which is of type parquet
2024-08-03 10:05:38,666 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-03 10:05:42,839 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-03 10:05:42,839 - root - INFO - reading file which is of type csv
2024-08-03 10:06:00,933 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-03 10:06:05,472 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-03 10:06:05,479 - root - INFO - Implementing data Processing now....
2024-08-03 10:06:05,479 - Data_processing - WARNING - Cleaning data method started
2024-08-03 10:06:23,770 - Data_processing - WARNING - Cleaning data method completed
2024-08-03 10:06:58,917 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o225.save.
: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: Hadoop home directory D:\hadoopin does not exist -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:736)
	at org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:271)
	at org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:287)
	at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)
	at org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:178)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:192)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.io.FileNotFoundException: java.io.FileNotFoundException: Hadoop home directory D:\hadoopin does not exist -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1886)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1846)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1819)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)
	at org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)
	at org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)
	at org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)
	at org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)
	at org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)
	at org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)
	at org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)
	at org.apache.spark.util.Utils$.createTempDir(Utils.scala:335)
	at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:344)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:898)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1043)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1052)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.io.FileNotFoundException: Hadoop home directory D:\hadoopin does not exist
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:491)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 22 more

2024-08-03 10:14:00,660 - root - INFO - i am in the main method..
2024-08-03 10:14:00,660 - root - INFO - calling spark object
2024-08-03 10:14:00,660 - Create_spark - INFO - get_spark_object method started
2024-08-03 10:14:00,660 - Create_spark - INFO - master is local
2024-08-03 10:14:09,197 - Create_spark - INFO - Spark object created.....
2024-08-03 10:14:09,198 - root - INFO - Validating spark object..........
2024-08-03 10:14:09,198 - Validate - WARNING - started the get_current_date method...
2024-08-03 10:14:13,386 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 3))]
2024-08-03 10:14:13,387 - Validate - WARNING - Validation done , go frwd...
2024-08-03 10:14:13,387 - root - INFO - reading file which is of type parquet
2024-08-03 10:14:14,259 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-03 10:14:16,039 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-03 10:14:16,040 - root - INFO - reading file which is of type csv
2024-08-03 10:14:21,770 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-03 10:14:23,064 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-03 10:14:23,069 - root - INFO - Implementing data Processing now....
2024-08-03 10:14:23,070 - Data_processing - WARNING - Cleaning data method started
2024-08-03 10:14:30,278 - Data_processing - WARNING - Cleaning data method completed
2024-08-03 10:14:43,101 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o225.save.
: java.lang.RuntimeException: java.io.FileNotFoundException: Hadoop bin directory does not exist: D:\hadoop\bin\bin -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:736)
	at org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:271)
	at org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:287)
	at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)
	at org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:178)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:192)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.io.FileNotFoundException: Hadoop bin directory does not exist: D:\hadoop\bin\bin -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.getQualifiedBinInner(Shell.java:608)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1886)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1846)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1819)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)
	at org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)
	at org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)
	at org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)
	at org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)
	at org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)
	at org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)
	at org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)
	at org.apache.spark.util.Utils$.createTempDir(Utils.scala:335)
	at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:344)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:898)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1043)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1052)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

2024-08-03 10:15:19,480 - root - INFO - i am in the main method..
2024-08-03 10:15:19,481 - root - INFO - calling spark object
2024-08-03 10:15:19,481 - Create_spark - INFO - get_spark_object method started
2024-08-03 10:15:19,481 - Create_spark - INFO - master is local
2024-08-03 10:15:27,553 - Create_spark - INFO - Spark object created.....
2024-08-03 10:15:27,553 - root - INFO - Validating spark object..........
2024-08-03 10:15:27,553 - Validate - WARNING - started the get_current_date method...
2024-08-03 10:15:32,073 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 3))]
2024-08-03 10:15:32,073 - Validate - WARNING - Validation done , go frwd...
2024-08-03 10:15:32,073 - root - INFO - reading file which is of type parquet
2024-08-03 10:15:32,918 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-03 10:15:34,637 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-03 10:15:34,638 - root - INFO - reading file which is of type csv
2024-08-03 10:15:40,384 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-03 10:15:41,683 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-03 10:15:41,686 - root - INFO - Implementing data Processing now....
2024-08-03 10:15:41,686 - Data_processing - WARNING - Cleaning data method started
2024-08-03 10:15:50,882 - Data_processing - WARNING - Cleaning data method completed
2024-08-03 10:16:27,126 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o225.save.
: org.apache.spark.SparkException: Job aborted.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:496)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:261)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)
	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1215)
	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1420)
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:182)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:250)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:634)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:250)
	... 41 more

2024-08-03 10:21:27,251 - root - INFO - i am in the main method..
2024-08-03 10:21:27,252 - root - INFO - calling spark object
2024-08-03 10:21:27,252 - Create_spark - INFO - get_spark_object method started
2024-08-03 10:21:27,252 - Create_spark - INFO - master is local
2024-08-03 10:21:35,260 - Create_spark - INFO - Spark object created.....
2024-08-03 10:21:35,260 - root - INFO - Validating spark object..........
2024-08-03 10:21:35,260 - Validate - WARNING - started the get_current_date method...
2024-08-03 10:21:40,673 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 3))]
2024-08-03 10:21:40,673 - Validate - WARNING - Validation done , go frwd...
2024-08-03 10:21:40,674 - root - INFO - reading file which is of type parquet
2024-08-03 10:21:42,017 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-03 10:21:47,439 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-03 10:21:47,440 - root - INFO - reading file which is of type csv
2024-08-03 10:22:01,150 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-03 10:22:02,141 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-03 10:22:02,145 - root - INFO - Implementing data Processing now....
2024-08-03 10:22:02,146 - Data_processing - WARNING - Cleaning data method started
2024-08-03 10:22:09,056 - Data_processing - WARNING - Cleaning data method completed
2024-08-03 10:22:22,807 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o225.save.
: org.apache.spark.SparkException: Job aborted.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:496)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:261)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)
	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1215)
	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1420)
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:182)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:250)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:634)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:250)
	... 41 more

2024-08-03 10:24:13,800 - root - INFO - i am in the main method..
2024-08-03 10:24:13,800 - root - INFO - calling spark object
2024-08-03 10:24:13,801 - Create_spark - INFO - get_spark_object method started
2024-08-03 10:24:13,801 - Create_spark - INFO - master is local
2024-08-03 10:24:21,678 - Create_spark - INFO - Spark object created.....
2024-08-03 10:24:21,679 - root - INFO - Validating spark object..........
2024-08-03 10:24:21,679 - Validate - WARNING - started the get_current_date method...
2024-08-03 10:24:27,422 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 3))]
2024-08-03 10:24:27,422 - Validate - WARNING - Validation done , go frwd...
2024-08-03 10:24:27,422 - root - INFO - reading file which is of type parquet
2024-08-03 10:24:30,626 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-03 10:24:33,212 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-03 10:24:33,213 - root - INFO - reading file which is of type csv
2024-08-03 10:24:38,696 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-03 10:24:39,790 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-03 10:24:39,794 - root - INFO - Implementing data Processing now....
2024-08-03 10:24:39,795 - Data_processing - WARNING - Cleaning data method started
2024-08-03 10:24:46,743 - Data_processing - WARNING - Cleaning data method completed
2024-08-03 10:24:57,094 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o226.save.
: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:736)
	at org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:271)
	at org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:287)
	at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)
	at org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:178)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:192)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1886)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1846)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1819)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)
	at org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)
	at org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)
	at org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)
	at org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)
	at org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)
	at org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)
	at org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)
	at org.apache.spark.util.Utils$.createTempDir(Utils.scala:335)
	at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:344)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:898)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1043)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1052)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 22 more

2024-08-03 10:31:07,370 - root - INFO - i am in the main method..
2024-08-03 10:31:07,371 - root - INFO - calling spark object
2024-08-03 10:31:07,371 - Create_spark - INFO - get_spark_object method started
2024-08-03 10:31:07,371 - Create_spark - INFO - master is local
2024-08-03 10:31:14,755 - Create_spark - INFO - Spark object created.....
2024-08-03 10:31:14,755 - root - INFO - Validating spark object..........
2024-08-03 10:31:14,755 - Validate - WARNING - started the get_current_date method...
2024-08-03 10:31:19,010 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 3))]
2024-08-03 10:31:19,010 - Validate - WARNING - Validation done , go frwd...
2024-08-03 10:31:19,011 - root - INFO - reading file which is of type parquet
2024-08-03 10:31:19,899 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-03 10:31:21,813 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-03 10:31:21,813 - root - INFO - reading file which is of type csv
2024-08-03 10:31:27,510 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-03 10:31:28,835 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-03 10:31:28,838 - root - INFO - Implementing data Processing now....
2024-08-03 10:31:28,838 - Data_processing - WARNING - Cleaning data method started
2024-08-03 10:31:35,651 - Data_processing - WARNING - Cleaning data method completed
2024-08-03 10:31:46,999 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o226.save.
: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:736)
	at org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:271)
	at org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:287)
	at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)
	at org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:178)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:192)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1886)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1846)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1819)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)
	at org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)
	at org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)
	at org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)
	at org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)
	at org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)
	at org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)
	at org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)
	at org.apache.spark.util.Utils$.createTempDir(Utils.scala:335)
	at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:344)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:898)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1043)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1052)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 22 more

2024-08-03 10:33:37,742 - root - INFO - i am in the main method..
2024-08-03 10:33:37,742 - root - INFO - calling spark object
2024-08-03 10:33:37,742 - Create_spark - INFO - get_spark_object method started
2024-08-03 10:33:37,742 - Create_spark - INFO - master is local
2024-08-03 10:33:45,531 - Create_spark - INFO - Spark object created.....
2024-08-03 10:33:45,531 - root - INFO - Validating spark object..........
2024-08-03 10:33:45,531 - Validate - WARNING - started the get_current_date method...
2024-08-03 10:33:50,530 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 3))]
2024-08-03 10:33:50,530 - Validate - WARNING - Validation done , go frwd...
2024-08-03 10:33:50,531 - root - INFO - reading file which is of type parquet
2024-08-03 10:33:51,561 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-03 10:33:53,509 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-03 10:33:53,510 - root - INFO - reading file which is of type csv
2024-08-03 10:33:58,915 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-03 10:34:00,128 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-03 10:34:00,132 - root - INFO - Implementing data Processing now....
2024-08-03 10:34:00,132 - Data_processing - WARNING - Cleaning data method started
2024-08-03 10:34:06,706 - Data_processing - WARNING - Cleaning data method completed
2024-08-03 10:34:24,690 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o226.save.
: org.apache.spark.SparkException: Job aborted.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:496)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:261)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)
	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1215)
	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1420)
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:182)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:250)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:634)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:250)
	... 41 more

2024-08-03 10:36:04,349 - root - INFO - i am in the main method..
2024-08-03 10:36:04,349 - root - INFO - calling spark object
2024-08-03 10:36:04,349 - Create_spark - INFO - get_spark_object method started
2024-08-03 10:36:04,349 - Create_spark - INFO - master is local
2024-08-03 10:36:12,592 - Create_spark - INFO - Spark object created.....
2024-08-03 10:36:12,592 - root - INFO - Validating spark object..........
2024-08-03 10:36:12,592 - Validate - WARNING - started the get_current_date method...
2024-08-03 10:36:16,827 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 3))]
2024-08-03 10:36:16,827 - Validate - WARNING - Validation done , go frwd...
2024-08-03 10:36:16,828 - root - INFO - reading file which is of type parquet
2024-08-03 10:36:17,755 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-03 10:36:19,484 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-03 10:36:19,484 - root - INFO - reading file which is of type csv
2024-08-03 10:36:28,773 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-03 10:36:30,187 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-03 10:36:30,190 - root - INFO - Implementing data Processing now....
2024-08-03 10:36:30,191 - Data_processing - WARNING - Cleaning data method started
2024-08-03 10:36:39,809 - Data_processing - WARNING - Cleaning data method completed
2024-08-03 10:36:57,611 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o226.save.
: org.apache.spark.SparkException: Job aborted.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:496)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:261)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)
	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1215)
	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1420)
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:182)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:250)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:634)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:250)
	... 41 more

2024-08-03 10:46:01,882 - root - INFO - i am in the main method..
2024-08-03 10:46:01,883 - root - INFO - calling spark object
2024-08-03 10:46:01,883 - Create_spark - INFO - get_spark_object method started
2024-08-03 10:46:01,883 - Create_spark - INFO - master is local
2024-08-03 10:46:10,702 - Create_spark - INFO - Spark object created.....
2024-08-03 10:46:10,703 - root - INFO - Validating spark object..........
2024-08-03 10:46:10,703 - Validate - WARNING - started the get_current_date method...
2024-08-03 10:46:15,484 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 3))]
2024-08-03 10:46:15,485 - Validate - WARNING - Validation done , go frwd...
2024-08-03 10:46:15,485 - root - INFO - reading file which is of type parquet
2024-08-03 10:46:16,616 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-03 10:46:19,021 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-03 10:46:19,022 - root - INFO - reading file which is of type csv
2024-08-03 10:46:24,613 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-03 10:46:26,090 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-03 10:46:26,094 - root - INFO - Implementing data Processing now....
2024-08-03 10:46:26,094 - Data_processing - WARNING - Cleaning data method started
2024-08-03 10:46:34,554 - Data_processing - WARNING - Cleaning data method completed
2024-08-03 10:46:47,618 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o226.save.
: org.apache.spark.SparkException: Job aborted.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:496)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:261)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)
	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1215)
	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1420)
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:182)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:250)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:634)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:250)
	... 41 more

2024-08-03 10:47:29,811 - root - INFO - i am in the main method..
2024-08-03 10:47:29,811 - root - INFO - calling spark object
2024-08-03 10:47:29,811 - Create_spark - INFO - get_spark_object method started
2024-08-03 10:47:29,811 - Create_spark - INFO - master is local
2024-08-03 10:47:37,602 - Create_spark - INFO - Spark object created.....
2024-08-03 10:47:37,602 - root - INFO - Validating spark object..........
2024-08-03 10:47:37,602 - Validate - WARNING - started the get_current_date method...
2024-08-03 10:47:42,447 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 3))]
2024-08-03 10:47:42,447 - Validate - WARNING - Validation done , go frwd...
2024-08-03 10:47:42,448 - root - INFO - reading file which is of type parquet
2024-08-03 10:47:43,510 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-03 10:47:45,633 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-03 10:47:45,633 - root - INFO - reading file which is of type csv
2024-08-03 10:47:52,721 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-03 10:47:54,244 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-03 10:47:54,247 - root - INFO - Implementing data Processing now....
2024-08-03 10:47:54,248 - Data_processing - WARNING - Cleaning data method started
2024-08-03 10:48:04,669 - Data_processing - WARNING - Cleaning data method completed
2024-08-03 10:48:21,214 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o226.save.
: org.apache.spark.SparkException: Job aborted.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:496)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:261)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)
	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1215)
	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1420)
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:182)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:250)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:634)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:250)
	... 41 more

2024-08-03 10:50:00,718 - root - INFO - i am in the main method..
2024-08-03 10:50:00,719 - root - INFO - calling spark object
2024-08-03 10:50:00,719 - Create_spark - INFO - get_spark_object method started
2024-08-03 10:50:00,719 - Create_spark - INFO - master is local
2024-08-03 10:50:08,703 - Create_spark - INFO - Spark object created.....
2024-08-03 10:50:08,703 - root - INFO - Validating spark object..........
2024-08-03 10:50:08,703 - Validate - WARNING - started the get_current_date method...
2024-08-03 10:50:14,201 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 3))]
2024-08-03 10:50:14,201 - Validate - WARNING - Validation done , go frwd...
2024-08-03 10:50:14,201 - root - INFO - reading file which is of type parquet
2024-08-03 10:50:15,697 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-03 10:50:18,259 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-03 10:50:18,259 - root - INFO - reading file which is of type csv
2024-08-03 10:50:24,336 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-03 10:50:25,515 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-03 10:50:25,518 - root - INFO - Implementing data Processing now....
2024-08-03 10:50:25,519 - Data_processing - WARNING - Cleaning data method started
2024-08-03 10:50:32,096 - Data_processing - WARNING - Cleaning data method completed
2024-08-03 10:50:43,439 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o226.save.
: org.apache.spark.SparkException: Job aborted.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:496)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:261)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)
	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1215)
	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1420)
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:182)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:250)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:634)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:250)
	... 41 more

2024-08-03 10:51:49,785 - root - INFO - i am in the main method..
2024-08-03 10:51:49,785 - root - INFO - calling spark object
2024-08-03 10:51:49,785 - Create_spark - INFO - get_spark_object method started
2024-08-03 10:51:49,785 - Create_spark - INFO - master is local
2024-08-03 10:51:56,932 - Create_spark - INFO - Spark object created.....
2024-08-03 10:51:56,932 - root - INFO - Validating spark object..........
2024-08-03 10:51:56,932 - Validate - WARNING - started the get_current_date method...
2024-08-03 10:52:01,413 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 3))]
2024-08-03 10:52:01,414 - Validate - WARNING - Validation done , go frwd...
2024-08-03 10:52:01,414 - root - INFO - reading file which is of type parquet
2024-08-03 10:52:02,307 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-03 10:52:03,989 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-03 10:52:03,990 - root - INFO - reading file which is of type csv
2024-08-03 10:52:09,185 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-03 10:52:10,364 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-03 10:52:10,368 - root - INFO - Implementing data Processing now....
2024-08-03 10:52:10,368 - Data_processing - WARNING - Cleaning data method started
2024-08-03 10:52:17,129 - Data_processing - WARNING - Cleaning data method completed
2024-08-03 10:52:30,217 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o226.save.
: org.apache.spark.SparkException: Job aborted.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:496)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:261)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)
	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1215)
	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1420)
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:182)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:250)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:634)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:250)
	... 41 more

2024-08-03 11:01:46,775 - root - INFO - i am in the main method..
2024-08-03 11:01:46,776 - root - INFO - calling spark object
2024-08-03 11:01:46,776 - Create_spark - INFO - get_spark_object method started
2024-08-03 11:01:46,776 - Create_spark - INFO - master is local
2024-08-03 11:01:54,782 - Create_spark - INFO - Spark object created.....
2024-08-03 11:01:54,782 - root - INFO - Validating spark object..........
2024-08-03 11:01:54,782 - Validate - WARNING - started the get_current_date method...
2024-08-03 11:01:59,536 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 3))]
2024-08-03 11:01:59,536 - Validate - WARNING - Validation done , go frwd...
2024-08-03 11:01:59,537 - root - INFO - reading file which is of type parquet
2024-08-03 11:02:00,610 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-03 11:02:02,444 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-03 11:02:02,446 - root - INFO - reading file which is of type csv
2024-08-03 11:02:07,742 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-03 11:02:08,829 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-03 11:02:08,834 - root - INFO - Implementing data Processing now....
2024-08-03 11:02:08,834 - Data_processing - WARNING - Cleaning data method started
2024-08-03 11:02:15,767 - Data_processing - WARNING - Cleaning data method completed
2024-08-03 11:02:31,348 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o226.save.
: org.apache.spark.SparkException: Job aborted.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:496)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:261)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)
	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1215)
	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1420)
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:182)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:250)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:634)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:250)
	... 41 more

2024-08-03 11:04:09,187 - root - INFO - i am in the main method..
2024-08-03 11:04:09,187 - root - INFO - calling spark object
2024-08-03 11:04:09,188 - Create_spark - INFO - get_spark_object method started
2024-08-03 11:04:09,188 - Create_spark - INFO - master is local
2024-08-03 11:04:16,615 - Create_spark - INFO - Spark object created.....
2024-08-03 11:04:16,616 - root - INFO - Validating spark object..........
2024-08-03 11:04:16,616 - Validate - WARNING - started the get_current_date method...
2024-08-03 11:04:20,660 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 3))]
2024-08-03 11:04:20,660 - Validate - WARNING - Validation done , go frwd...
2024-08-03 11:04:20,661 - root - INFO - reading file which is of type parquet
2024-08-03 11:04:21,624 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-03 11:04:23,355 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-03 11:04:23,356 - root - INFO - reading file which is of type csv
2024-08-03 11:04:28,555 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-03 11:04:29,749 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-03 11:04:29,752 - root - INFO - Implementing data Processing now....
2024-08-03 11:04:29,753 - Data_processing - WARNING - Cleaning data method started
2024-08-03 11:04:36,415 - Data_processing - WARNING - Cleaning data method completed
2024-08-03 11:04:50,387 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o226.save.
: org.apache.spark.SparkException: Job aborted.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:496)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:261)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)
	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1215)
	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1420)
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:182)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:250)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:634)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:250)
	... 41 more

2024-08-03 11:14:36,264 - root - INFO - i am in the main method..
2024-08-03 11:14:36,264 - root - INFO - calling spark object
2024-08-03 11:14:36,264 - Create_spark - INFO - get_spark_object method started
2024-08-03 11:14:36,264 - Create_spark - INFO - master is local
2024-08-03 11:14:43,542 - Create_spark - INFO - Spark object created.....
2024-08-03 11:14:43,542 - root - INFO - Validating spark object..........
2024-08-03 11:14:43,542 - Validate - WARNING - started the get_current_date method...
2024-08-03 11:14:48,618 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 3))]
2024-08-03 11:14:48,618 - Validate - WARNING - Validation done , go frwd...
2024-08-03 11:14:48,619 - root - INFO - reading file which is of type parquet
2024-08-03 11:14:49,594 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-03 11:14:51,954 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-03 11:14:51,957 - root - INFO - reading file which is of type csv
2024-08-03 11:14:58,390 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-03 11:14:59,602 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-03 11:14:59,606 - root - INFO - Implementing data Processing now....
2024-08-03 11:14:59,606 - Data_processing - WARNING - Cleaning data method started
2024-08-03 11:15:06,800 - Data_processing - WARNING - Cleaning data method completed
2024-08-03 11:15:16,830 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o226.save.
: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "s3"
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
	at org.apache.spark.sql.execution.datasources.DataSource.planForWritingFileFormat(DataSource.scala:461)
	at org.apache.spark.sql.execution.datasources.DataSource.planForWriting(DataSource.scala:556)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)

2024-08-03 11:20:08,852 - root - INFO - i am in the main method..
2024-08-03 11:20:08,853 - root - INFO - calling spark object
2024-08-03 11:20:08,853 - Create_spark - INFO - get_spark_object method started
2024-08-03 11:20:08,853 - Create_spark - INFO - master is local
2024-08-03 11:20:16,221 - Create_spark - INFO - Spark object created.....
2024-08-03 11:20:16,222 - root - INFO - Validating spark object..........
2024-08-03 11:20:16,222 - Validate - WARNING - started the get_current_date method...
2024-08-03 11:20:20,634 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 3))]
2024-08-03 11:20:20,634 - Validate - WARNING - Validation done , go frwd...
2024-08-03 11:20:20,634 - root - INFO - reading file which is of type parquet
2024-08-03 11:20:21,585 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-03 11:20:23,402 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-03 11:20:23,403 - root - INFO - reading file which is of type csv
2024-08-03 11:20:28,497 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-03 11:20:29,609 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-03 11:20:29,614 - root - INFO - Implementing data Processing now....
2024-08-03 11:20:29,614 - Data_processing - WARNING - Cleaning data method started
2024-08-03 11:20:36,555 - Data_processing - WARNING - Cleaning data method completed
2024-08-03 11:20:49,386 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o226.save.
: org.apache.spark.SparkException: Job aborted.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:496)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:261)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)
	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1215)
	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1420)
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:182)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:250)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:634)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:250)
	... 41 more

2024-08-03 11:45:10,365 - root - INFO - i am in the main method..
2024-08-03 11:45:10,365 - root - INFO - calling spark object
2024-08-03 11:45:10,365 - Create_spark - INFO - get_spark_object method started
2024-08-03 11:45:10,365 - Create_spark - INFO - master is local
2024-08-03 11:45:18,169 - Create_spark - INFO - Spark object created.....
2024-08-03 11:45:18,169 - root - INFO - Validating spark object..........
2024-08-03 11:45:18,169 - Validate - WARNING - started the get_current_date method...
2024-08-03 11:45:22,552 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 3))]
2024-08-03 11:45:22,552 - Validate - WARNING - Validation done , go frwd...
2024-08-03 11:45:22,553 - root - INFO - reading file which is of type parquet
2024-08-03 11:45:23,778 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-03 11:45:27,190 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-03 11:45:27,191 - root - INFO - reading file which is of type csv
2024-08-03 11:45:33,534 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-03 11:45:34,926 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-03 11:45:34,930 - root - INFO - Implementing data Processing now....
2024-08-03 11:45:34,931 - Data_processing - WARNING - Cleaning data method started
2024-08-03 11:45:42,030 - Data_processing - WARNING - Cleaning data method completed
2024-08-03 11:45:53,085 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o238.save.
: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "s3"
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
	at org.apache.spark.sql.execution.datasources.DataSource.planForWritingFileFormat(DataSource.scala:461)
	at org.apache.spark.sql.execution.datasources.DataSource.planForWriting(DataSource.scala:556)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)

2024-08-03 11:49:17,435 - root - INFO - i am in the main method..
2024-08-03 11:49:17,436 - root - INFO - calling spark object
2024-08-03 11:49:17,436 - Create_spark - INFO - get_spark_object method started
2024-08-03 11:49:17,436 - Create_spark - INFO - master is local
2024-08-03 11:49:25,046 - Create_spark - INFO - Spark object created.....
2024-08-03 11:49:25,046 - root - INFO - Validating spark object..........
2024-08-03 11:49:25,047 - Validate - WARNING - started the get_current_date method...
2024-08-03 11:49:29,660 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 3))]
2024-08-03 11:49:29,660 - Validate - WARNING - Validation done , go frwd...
2024-08-03 11:49:29,661 - root - INFO - reading file which is of type parquet
2024-08-03 11:49:30,545 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-03 11:49:32,710 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-03 11:49:32,712 - root - INFO - reading file which is of type csv
2024-08-03 11:49:37,948 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-03 11:49:39,153 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-03 11:49:39,156 - root - INFO - Implementing data Processing now....
2024-08-03 11:49:39,156 - Data_processing - WARNING - Cleaning data method started
2024-08-03 11:49:46,265 - Data_processing - WARNING - Cleaning data method completed
2024-08-03 11:49:56,442 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o238.save.
: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2667)
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
	at org.apache.spark.sql.execution.datasources.DataSource.planForWritingFileFormat(DataSource.scala:461)
	at org.apache.spark.sql.execution.datasources.DataSource.planForWriting(DataSource.scala:556)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2571)
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2665)
	... 24 more

2024-08-06 11:35:47,804 - root - INFO - i am in the main method..
2024-08-06 11:35:47,809 - root - INFO - calling spark object
2024-08-06 11:35:47,810 - Create_spark - INFO - get_spark_object method started
2024-08-06 11:35:47,811 - Create_spark - INFO - master is local
2024-08-06 11:36:01,603 - Create_spark - INFO - Spark object created.....
2024-08-06 11:36:01,603 - root - INFO - Validating spark object..........
2024-08-06 11:36:01,603 - Validate - WARNING - started the get_current_date method...
2024-08-06 11:36:09,090 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 6))]
2024-08-06 11:36:09,090 - Validate - WARNING - Validation done , go frwd...
2024-08-06 11:36:09,091 - root - INFO - reading file which is of type parquet
2024-08-06 11:36:11,289 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-06 11:36:14,300 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-06 11:36:14,301 - root - INFO - reading file which is of type csv
2024-08-06 11:36:23,832 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-06 11:36:25,660 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-06 11:36:25,689 - root - INFO - Implementing data Processing now....
2024-08-06 11:36:25,689 - Data_processing - WARNING - Cleaning data method started
2024-08-06 11:36:38,910 - Data_processing - WARNING - Cleaning data method completed
2024-08-06 11:37:08,605 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o238.save.
: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "s3"
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
	at org.apache.spark.sql.execution.datasources.DataSource.planForWritingFileFormat(DataSource.scala:461)
	at org.apache.spark.sql.execution.datasources.DataSource.planForWriting(DataSource.scala:556)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)

2024-08-06 11:57:03,266 - root - INFO - i am in the main method..
2024-08-06 11:57:03,270 - root - INFO - calling spark object
2024-08-06 11:57:03,271 - Create_spark - INFO - get_spark_object method started
2024-08-06 11:57:03,271 - Create_spark - INFO - master is local
2024-08-06 11:57:17,288 - Create_spark - INFO - Spark object created.....
2024-08-06 11:57:17,289 - root - INFO - Validating spark object..........
2024-08-06 11:57:17,289 - Validate - WARNING - started the get_current_date method...
2024-08-06 11:57:24,092 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 6))]
2024-08-06 11:57:24,092 - Validate - WARNING - Validation done , go frwd...
2024-08-06 11:57:24,093 - root - INFO - reading file which is of type parquet
2024-08-06 11:57:25,622 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-06 11:57:28,860 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-06 11:57:28,862 - root - INFO - reading file which is of type csv
2024-08-06 11:57:37,662 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-06 11:57:40,315 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-06 11:57:40,320 - root - INFO - Implementing data Processing now....
2024-08-06 11:57:40,321 - Data_processing - WARNING - Cleaning data method started
2024-08-06 11:57:54,770 - Data_processing - WARNING - Cleaning data method completed
2024-08-06 11:58:12,832 - Persist - WARNING - Saving the cities data to hive database...
2024-08-06 11:58:12,861 - Persist - ERROR - An error has been occurred while saving dataframe to hive table city..
2024-08-06 11:58:12,872 - root - ERROR - An error occurred when calling main() please check the trace=== 
missing 'EXISTS' at 'exist'(line 1, pos 23)

== SQL ==
Create database if not exist cities
-----------------------^^^

2024-08-06 11:58:35,524 - root - INFO - i am in the main method..
2024-08-06 11:58:35,524 - root - INFO - calling spark object
2024-08-06 11:58:35,524 - Create_spark - INFO - get_spark_object method started
2024-08-06 11:58:35,524 - Create_spark - INFO - master is local
2024-08-06 11:58:47,238 - Create_spark - INFO - Spark object created.....
2024-08-06 11:58:47,238 - root - INFO - Validating spark object..........
2024-08-06 11:58:47,238 - Validate - WARNING - started the get_current_date method...
2024-08-06 11:58:53,262 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 6))]
2024-08-06 11:58:53,262 - Validate - WARNING - Validation done , go frwd...
2024-08-06 11:58:53,263 - root - INFO - reading file which is of type parquet
2024-08-06 11:58:54,523 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-06 11:58:58,132 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-06 11:58:58,132 - root - INFO - reading file which is of type csv
2024-08-06 11:59:07,451 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-06 11:59:09,587 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-06 11:59:09,592 - root - INFO - Implementing data Processing now....
2024-08-06 11:59:09,593 - Data_processing - WARNING - Cleaning data method started
2024-08-06 11:59:21,180 - Data_processing - WARNING - Cleaning data method completed
2024-08-06 11:59:39,798 - Persist - WARNING - Saving the cities data to hive database...
2024-08-06 11:59:52,973 - Persist - ERROR - An error has been occurred while saving dataframe to hive table city..
2024-08-06 11:59:52,975 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o25.sql.
: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)
	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1215)
	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1420)
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)
	at org.apache.hadoop.fs.FileSystem.listStatusBatch(FileSystem.java:1960)
	at org.apache.hadoop.fs.FileSystem$DirListingIterator.<init>(FileSystem.java:2216)
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatusIterator(ChecksumFileSystem.java:768)
	at org.apache.hadoop.fs.shell.PathData.getDirectoryContentsIterator(PathData.java:289)
	at org.apache.hadoop.fs.shell.Command.recursePath(Command.java:441)
	at org.apache.hadoop.fs.shell.Command.processPathInternal(Command.java:369)
	at org.apache.hadoop.fs.shell.Command.processPaths(Command.java:331)
	at org.apache.hadoop.fs.shell.Command.processPathArgument(Command.java:304)
	at org.apache.hadoop.fs.shell.Command.processArgument(Command.java:286)
	at org.apache.hadoop.fs.shell.Command.processArguments(Command.java:270)
	at org.apache.hadoop.fs.shell.FsCommand.processRawArguments(FsCommand.java:121)
	at org.apache.hadoop.fs.shell.Command.run(Command.java:177)
	at org.apache.hadoop.fs.FsShell.run(FsShell.java:327)
	at org.apache.hadoop.hive.io.HdfsUtils.run(HdfsUtils.java:201)
	at org.apache.hadoop.hive.io.HdfsUtils.setFullFileStatus(HdfsUtils.java:124)
	at org.apache.hadoop.hive.io.HdfsUtils.setFullFileStatus(HdfsUtils.java:94)
	at org.apache.hadoop.hive.io.HdfsUtils.setFullFileStatus(HdfsUtils.java:77)
	at org.apache.hadoop.hive.common.FileUtils.mkdir(FileUtils.java:544)
	at org.apache.hadoop.hive.metastore.Warehouse.mkdirs(Warehouse.java:194)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_database_core(HiveMetaStore.java:880)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_database(HiveMetaStore.java:939)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:148)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
	at com.sun.proxy.$Proxy24.create_database(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createDatabase(HiveMetaStoreClient.java:725)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:173)
	at com.sun.proxy.$Proxy25.createDatabase(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.createDatabase(Hive.java:434)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$createDatabase$1(HiveClientImpl.scala:347)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:305)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:236)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:235)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:285)
	at org.apache.spark.sql.hive.client.HiveClientImpl.createDatabase(HiveClientImpl.scala:345)
	at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$createDatabase$1(HiveExternalCatalog.scala:193)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:102)
	at org.apache.spark.sql.hive.HiveExternalCatalog.createDatabase(HiveExternalCatalog.scala:193)
	at org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.createDatabase(ExternalCatalogWithListener.scala:47)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createDatabase(SessionCatalog.scala:251)
	at org.apache.spark.sql.execution.command.CreateDatabaseCommand.run(ddl.scala:83)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:219)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:618)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)

2024-08-06 12:17:38,382 - root - INFO - i am in the main method..
2024-08-06 12:17:38,384 - root - INFO - calling spark object
2024-08-06 12:17:38,384 - Create_spark - INFO - get_spark_object method started
2024-08-06 12:17:38,384 - Create_spark - INFO - master is local
2024-08-06 12:17:50,834 - Create_spark - INFO - Spark object created.....
2024-08-06 12:17:50,834 - root - INFO - Validating spark object..........
2024-08-06 12:17:50,835 - Validate - WARNING - started the get_current_date method...
2024-08-06 12:17:58,636 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 6))]
2024-08-06 12:17:58,636 - Validate - WARNING - Validation done , go frwd...
2024-08-06 12:17:58,636 - root - INFO - reading file which is of type parquet
2024-08-06 12:18:00,331 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-06 12:18:03,368 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-06 12:18:03,370 - root - INFO - reading file which is of type csv
2024-08-06 12:18:14,339 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-06 12:18:16,509 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-06 12:18:16,514 - root - INFO - Implementing data Processing now....
2024-08-06 12:18:16,514 - Data_processing - WARNING - Cleaning data method started
2024-08-06 12:18:28,380 - Data_processing - WARNING - Cleaning data method completed
2024-08-06 12:18:45,341 - Persist - WARNING - Saving the cities data to hive database...
2024-08-06 12:19:08,562 - Persist - ERROR - An error has been occurred while saving dataframe to hive table city..
2024-08-06 12:19:08,584 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o231.saveAsTable.
: org.apache.spark.SparkException: Job aborted.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:496)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:261)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)
	at org.apache.spark.sql.execution.datasources.DataSource.writeAndRead(DataSource.scala:537)
	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.saveDataIntoTable(createDataSourceTables.scala:228)
	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:182)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)
	at org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:689)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:667)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:565)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)
	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1215)
	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1420)
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)
	at org.apache.parquet.hadoop.ParquetOutputCommitter.commitJob(ParquetOutputCommitter.java:48)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:182)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:250)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:634)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:250)
	... 44 more

2024-08-06 12:23:37,060 - root - INFO - i am in the main method..
2024-08-06 12:23:37,060 - root - INFO - calling spark object
2024-08-06 12:23:37,061 - Create_spark - INFO - get_spark_object method started
2024-08-06 12:23:37,061 - Create_spark - INFO - master is local
2024-08-06 12:23:48,854 - Create_spark - INFO - Spark object created.....
2024-08-06 12:23:48,854 - root - INFO - Validating spark object..........
2024-08-06 12:23:48,854 - Validate - WARNING - started the get_current_date method...
2024-08-06 12:23:55,509 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 6))]
2024-08-06 12:23:55,510 - Validate - WARNING - Validation done , go frwd...
2024-08-06 12:23:55,510 - root - INFO - reading file which is of type parquet
2024-08-06 12:24:02,615 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-06 12:24:16,055 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-06 12:24:16,056 - root - INFO - reading file which is of type csv
2024-08-06 12:24:32,330 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-06 12:24:34,524 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-06 12:24:34,524 - root - INFO - Implementing data Processing now....
2024-08-06 12:24:34,524 - Data_processing - WARNING - Cleaning data method started
2024-08-06 12:24:50,854 - Data_processing - WARNING - Cleaning data method completed
2024-08-06 12:25:18,834 - Persist - WARNING - Saving the cities data to hive database...
2024-08-06 12:25:35,854 - Persist - ERROR - An error has been occurred while saving dataframe to hive table city..
2024-08-06 12:25:35,858 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o231.saveAsTable.
: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)
	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1215)
	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1420)
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.validateTableLocation(SessionCatalog.scala:386)
	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:175)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)
	at org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:689)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:667)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:565)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)

2024-08-06 12:49:20,549 - root - INFO - i am in the main method..
2024-08-06 12:49:20,549 - root - INFO - calling spark object
2024-08-06 12:49:20,549 - Create_spark - INFO - get_spark_object method started
2024-08-06 12:49:20,549 - Create_spark - INFO - master is local
2024-08-06 12:49:40,027 - Create_spark - INFO - Spark object created.....
2024-08-06 12:49:40,027 - root - INFO - Validating spark object..........
2024-08-06 12:49:40,028 - Validate - WARNING - started the get_current_date method...
2024-08-06 12:49:48,954 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 6))]
2024-08-06 12:49:48,955 - Validate - WARNING - Validation done , go frwd...
2024-08-06 12:49:48,955 - root - INFO - reading file which is of type parquet
2024-08-06 12:49:51,138 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-06 12:49:55,268 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-06 12:49:55,269 - root - INFO - reading file which is of type csv
2024-08-06 12:50:09,281 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-06 12:50:12,457 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-06 12:50:12,468 - root - INFO - Implementing data Processing now....
2024-08-06 12:50:12,468 - Data_processing - WARNING - Cleaning data method started
2024-08-06 12:50:29,682 - Data_processing - WARNING - Cleaning data method completed
2024-08-06 12:50:53,552 - Persist - WARNING - Saving the cities data to hive database...
2024-08-06 12:51:02,377 - Persist - ERROR - An error has been occurred while saving dataframe to hive table city..
2024-08-06 12:51:02,379 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o231.saveAsTable.
: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)
	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1215)
	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1420)
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.validateTableLocation(SessionCatalog.scala:386)
	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:175)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)
	at org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:689)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:667)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:565)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)

2024-08-06 12:52:00,234 - root - INFO - i am in the main method..
2024-08-06 12:52:00,359 - root - INFO - calling spark object
2024-08-06 12:52:00,359 - Create_spark - INFO - get_spark_object method started
2024-08-06 12:52:00,359 - Create_spark - INFO - master is local
2024-08-06 12:52:22,094 - Create_spark - INFO - Spark object created.....
2024-08-06 12:52:22,097 - root - INFO - Validating spark object..........
2024-08-06 12:52:22,097 - Validate - WARNING - started the get_current_date method...
2024-08-06 12:52:30,830 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 6))]
2024-08-06 12:52:30,831 - Validate - WARNING - Validation done , go frwd...
2024-08-06 12:52:30,833 - root - INFO - reading file which is of type parquet
2024-08-06 12:52:34,492 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-06 12:52:39,257 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-06 12:52:39,258 - root - INFO - reading file which is of type csv
2024-08-06 12:52:51,296 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-06 12:52:54,553 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-06 12:52:54,559 - root - INFO - Implementing data Processing now....
2024-08-06 12:52:54,560 - Data_processing - WARNING - Cleaning data method started
2024-08-06 12:53:11,058 - Data_processing - WARNING - Cleaning data method completed
2024-08-06 12:53:37,559 - Persist - WARNING - Saving the cities data to hive database...
2024-08-06 12:53:46,413 - Persist - ERROR - An error has been occurred while saving dataframe to hive table city..
2024-08-06 12:53:46,414 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o231.saveAsTable.
: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)
	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1215)
	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1420)
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.validateTableLocation(SessionCatalog.scala:386)
	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:175)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)
	at org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:689)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:667)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:565)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)

2024-08-06 12:53:52,746 - root - INFO - i am in the main method..
2024-08-06 12:53:52,747 - root - INFO - calling spark object
2024-08-06 12:53:52,747 - Create_spark - INFO - get_spark_object method started
2024-08-06 12:53:52,747 - Create_spark - INFO - master is local
2024-08-06 12:54:13,000 - Create_spark - INFO - Spark object created.....
2024-08-06 12:54:13,000 - root - INFO - Validating spark object..........
2024-08-06 12:54:13,001 - Validate - WARNING - started the get_current_date method...
2024-08-06 12:54:30,049 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 6))]
2024-08-06 12:54:30,050 - Validate - WARNING - Validation done , go frwd...
2024-08-06 12:54:30,050 - root - INFO - reading file which is of type parquet
2024-08-06 12:54:33,656 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-06 12:54:37,708 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-06 12:54:37,709 - root - INFO - reading file which is of type csv
2024-08-06 12:54:59,561 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-06 12:55:04,247 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-06 12:55:04,260 - root - INFO - Implementing data Processing now....
2024-08-06 12:55:04,260 - Data_processing - WARNING - Cleaning data method started
2024-08-06 12:55:30,207 - Data_processing - WARNING - Cleaning data method completed
2024-08-06 12:56:29,959 - Persist - WARNING - Saving the cities data to hive database...
2024-08-06 12:56:39,342 - Persist - ERROR - An error has been occurred while saving dataframe to hive table city..
2024-08-06 12:56:39,346 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o231.saveAsTable.
: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)
	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1215)
	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1420)
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.validateTableLocation(SessionCatalog.scala:386)
	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:175)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)
	at org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:689)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:667)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:565)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)

2024-08-06 12:56:45,514 - root - INFO - i am in the main method..
2024-08-06 12:56:45,514 - root - INFO - calling spark object
2024-08-06 12:56:45,515 - Create_spark - INFO - get_spark_object method started
2024-08-06 12:56:45,515 - Create_spark - INFO - master is local
2024-08-06 12:57:08,479 - Create_spark - INFO - Spark object created.....
2024-08-06 12:57:08,481 - root - INFO - Validating spark object..........
2024-08-06 12:57:08,481 - Validate - WARNING - started the get_current_date method...
2024-08-06 12:57:37,005 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 6))]
2024-08-06 12:57:37,006 - Validate - WARNING - Validation done , go frwd...
2024-08-06 12:57:37,008 - root - INFO - reading file which is of type parquet
2024-08-06 12:57:40,036 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-06 12:57:45,514 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-06 12:57:45,516 - root - INFO - reading file which is of type csv
2024-08-06 12:58:02,707 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-06 12:58:06,793 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-06 12:58:06,799 - root - INFO - Implementing data Processing now....
2024-08-06 12:58:06,800 - Data_processing - WARNING - Cleaning data method started
2024-08-06 12:58:27,417 - Data_processing - WARNING - Cleaning data method completed
2024-08-06 12:59:09,315 - Extraction - WARNING - Writing files to the output directory....
2024-08-06 12:59:09,783 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o230.save.
: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "s3"
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
	at org.apache.spark.sql.execution.datasources.DataSource.planForWritingFileFormat(DataSource.scala:461)
	at org.apache.spark.sql.execution.datasources.DataSource.planForWriting(DataSource.scala:556)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)

2024-08-06 12:59:14,609 - root - INFO - i am in the main method..
2024-08-06 12:59:14,609 - root - INFO - calling spark object
2024-08-06 12:59:14,609 - Create_spark - INFO - get_spark_object method started
2024-08-06 12:59:14,609 - Create_spark - INFO - master is local
2024-08-06 12:59:31,247 - Create_spark - INFO - Spark object created.....
2024-08-06 12:59:31,247 - root - INFO - Validating spark object..........
2024-08-06 12:59:31,247 - Validate - WARNING - started the get_current_date method...
2024-08-06 12:59:48,681 - root - INFO - Exception while sending command.
Traceback (most recent call last):
  File "C:\Python38\lib\socket.py", line 669, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Admin\PycharmProjects\youtubeproject\.venv\lib\site-packages\py4j\clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Admin\PycharmProjects\youtubeproject\.venv\lib\site-packages\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "C:\Users\Admin\PycharmProjects\youtubeproject\.venv\lib\site-packages\py4j\clientserver.py", line 506, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending
2024-08-06 12:59:51,199 - root - ERROR - Exception while sending command.
Traceback (most recent call last):
  File "C:\Python38\lib\socket.py", line 669, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Admin\PycharmProjects\youtubeproject\.venv\lib\site-packages\py4j\clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Admin\PycharmProjects\youtubeproject\.venv\lib\site-packages\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "C:\Users\Admin\PycharmProjects\youtubeproject\.venv\lib\site-packages\py4j\clientserver.py", line 506, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Admin\PycharmProjects\youtubeproject\.venv\lib\site-packages\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "C:\Python38\lib\socket.py", line 669, in readinto
    return self._sock.recv_into(b)
  File "C:\Users\Admin\Downloads\spark-3.2.3-bin-hadoop3.2\spark-3.2.3-bin-hadoop3.2\python\pyspark\context.py", line 292, in signal_handler
    self.cancelAllJobs()
  File "C:\Users\Admin\Downloads\spark-3.2.3-bin-hadoop3.2\spark-3.2.3-bin-hadoop3.2\python\pyspark\context.py", line 1195, in cancelAllJobs
    self._jsc.sc().cancelAllJobs()
  File "C:\Users\Admin\PycharmProjects\youtubeproject\.venv\lib\site-packages\py4j\java_gateway.py", line 1320, in __call__
    answer = self.gateway_client.send_command(command)
  File "C:\Users\Admin\PycharmProjects\youtubeproject\.venv\lib\site-packages\py4j\java_gateway.py", line 1053, in send_command
    response = self.send_command(command, binary=binary)
  File "C:\Users\Admin\PycharmProjects\youtubeproject\.venv\lib\site-packages\py4j\java_gateway.py", line 1036, in send_command
    connection = self._get_connection()
  File "C:\Users\Admin\PycharmProjects\youtubeproject\.venv\lib\site-packages\py4j\clientserver.py", line 284, in _get_connection
    connection = self._create_new_connection()
  File "C:\Users\Admin\PycharmProjects\youtubeproject\.venv\lib\site-packages\py4j\clientserver.py", line 291, in _create_new_connection
    connection.connect_to_java_server()
  File "C:\Users\Admin\PycharmProjects\youtubeproject\.venv\lib\site-packages\py4j\clientserver.py", line 438, in connect_to_java_server
    self.socket.connect((self.java_address, self.java_port))
ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Admin\PycharmProjects\youtubeproject\.venv\lib\site-packages\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "C:\Users\Admin\PycharmProjects\youtubeproject\.venv\lib\site-packages\py4j\clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2024-08-06 12:59:51,209 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o25.sql
2024-08-06 12:59:52,745 - root - INFO - i am in the main method..
2024-08-06 12:59:52,745 - root - INFO - calling spark object
2024-08-06 12:59:52,746 - Create_spark - INFO - get_spark_object method started
2024-08-06 12:59:52,746 - Create_spark - INFO - master is local
2024-08-06 13:00:11,125 - Create_spark - INFO - Spark object created.....
2024-08-06 13:00:11,126 - root - INFO - Validating spark object..........
2024-08-06 13:00:11,126 - Validate - WARNING - started the get_current_date method...
2024-08-06 13:00:22,753 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 6))]
2024-08-06 13:00:22,753 - Validate - WARNING - Validation done , go frwd...
2024-08-06 13:00:22,754 - root - INFO - reading file which is of type parquet
2024-08-06 13:00:26,207 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-06 13:00:31,632 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-06 13:00:31,633 - root - INFO - reading file which is of type csv
2024-08-06 13:00:49,353 - root - ERROR - Exception while sending command.
Traceback (most recent call last):
  File "C:\Users\Admin\PycharmProjects\youtubeproject\.venv\lib\site-packages\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
RuntimeError: reentrant call inside <_io.BufferedReader name=628>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Admin\PycharmProjects\youtubeproject\.venv\lib\site-packages\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "C:\Users\Admin\PycharmProjects\youtubeproject\.venv\lib\site-packages\py4j\clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2024-08-06 13:00:49,360 - root - ERROR - Exception while sending command.
Traceback (most recent call last):
  File "C:\Users\Admin\PycharmProjects\youtubeproject\.venv\lib\site-packages\py4j\clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "C:\Python38\lib\socket.py", line 669, in readinto
    return self._sock.recv_into(b)
  File "C:\Users\Admin\Downloads\spark-3.2.3-bin-hadoop3.2\spark-3.2.3-bin-hadoop3.2\python\pyspark\context.py", line 292, in signal_handler
    self.cancelAllJobs()
  File "C:\Users\Admin\Downloads\spark-3.2.3-bin-hadoop3.2\spark-3.2.3-bin-hadoop3.2\python\pyspark\context.py", line 1195, in cancelAllJobs
    self._jsc.sc().cancelAllJobs()
  File "C:\Users\Admin\PycharmProjects\youtubeproject\.venv\lib\site-packages\py4j\java_gateway.py", line 1321, in __call__
    return_value = get_return_value(
  File "C:\Users\Admin\Downloads\spark-3.2.3-bin-hadoop3.2\spark-3.2.3-bin-hadoop3.2\python\pyspark\sql\utils.py", line 111, in deco
    return f(*a, **kw)
  File "C:\Users\Admin\PycharmProjects\youtubeproject\.venv\lib\site-packages\py4j\protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o15.sc

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Admin\PycharmProjects\youtubeproject\.venv\lib\site-packages\py4j\java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "C:\Users\Admin\PycharmProjects\youtubeproject\.venv\lib\site-packages\py4j\clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2024-08-06 13:00:49,372 - Ingest - ERROR - An error occured while loading files: An error occurred while calling o45.load
2024-08-06 13:00:49,372 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o45.load
2024-08-06 13:00:53,369 - root - INFO - i am in the main method..
2024-08-06 13:00:53,369 - root - INFO - calling spark object
2024-08-06 13:00:53,370 - Create_spark - INFO - get_spark_object method started
2024-08-06 13:00:53,370 - Create_spark - INFO - master is local
2024-08-06 13:01:08,018 - Create_spark - INFO - Spark object created.....
2024-08-06 13:01:08,019 - root - INFO - Validating spark object..........
2024-08-06 13:01:08,019 - Validate - WARNING - started the get_current_date method...
2024-08-06 13:01:17,326 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 6))]
2024-08-06 13:01:17,326 - Validate - WARNING - Validation done , go frwd...
2024-08-06 13:01:17,327 - root - INFO - reading file which is of type parquet
2024-08-06 13:01:19,255 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-06 13:01:23,242 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-06 13:01:23,242 - root - INFO - reading file which is of type csv
2024-08-06 13:01:33,984 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-06 13:01:36,398 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-06 13:01:36,403 - root - INFO - Implementing data Processing now....
2024-08-06 13:01:36,404 - Data_processing - WARNING - Cleaning data method started
2024-08-06 13:01:52,398 - Data_processing - WARNING - Cleaning data method completed
2024-08-06 13:02:17,379 - Extraction - WARNING - Writing files to the output directory....
2024-08-06 13:02:17,439 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o230.save.
: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme "s3"
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)
	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)
	at org.apache.spark.sql.execution.datasources.DataSource.planForWritingFileFormat(DataSource.scala:461)
	at org.apache.spark.sql.execution.datasources.DataSource.planForWriting(DataSource.scala:556)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)

2024-08-06 13:04:55,250 - root - INFO - i am in the main method..
2024-08-06 13:04:55,250 - root - INFO - calling spark object
2024-08-06 13:04:55,250 - Create_spark - INFO - get_spark_object method started
2024-08-06 13:04:55,250 - Create_spark - INFO - master is local
2024-08-06 13:05:14,379 - Create_spark - INFO - Spark object created.....
2024-08-06 13:05:14,380 - root - INFO - Validating spark object..........
2024-08-06 13:05:14,380 - Validate - WARNING - started the get_current_date method...
2024-08-06 13:05:21,874 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 6))]
2024-08-06 13:05:21,875 - Validate - WARNING - Validation done , go frwd...
2024-08-06 13:05:21,875 - root - INFO - reading file which is of type parquet
2024-08-06 13:05:23,631 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-06 13:05:27,358 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-06 13:05:27,359 - root - INFO - reading file which is of type csv
2024-08-06 13:05:37,874 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-06 13:05:40,519 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-06 13:05:40,524 - root - INFO - Implementing data Processing now....
2024-08-06 13:05:40,525 - Data_processing - WARNING - Cleaning data method started
2024-08-06 13:06:01,713 - Data_processing - WARNING - Cleaning data method completed
2024-08-06 13:06:23,265 - Extraction - WARNING - Writing files to the output directory....
2024-08-06 13:06:27,237 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o230.save.
: org.apache.spark.SparkException: Job aborted.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:496)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:261)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)
	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1215)
	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1420)
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:182)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:250)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:634)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:250)
	... 41 more

2024-08-06 13:08:54,995 - root - INFO - i am in the main method..
2024-08-06 13:08:54,996 - root - INFO - calling spark object
2024-08-06 13:08:54,997 - Create_spark - INFO - get_spark_object method started
2024-08-06 13:08:54,997 - Create_spark - INFO - master is local
2024-08-06 13:09:21,971 - Create_spark - INFO - Spark object created.....
2024-08-06 13:09:21,972 - root - INFO - Validating spark object..........
2024-08-06 13:09:21,972 - Validate - WARNING - started the get_current_date method...
2024-08-06 13:09:40,419 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 6))]
2024-08-06 13:09:40,419 - Validate - WARNING - Validation done , go frwd...
2024-08-06 13:09:40,420 - root - INFO - reading file which is of type parquet
2024-08-06 13:09:42,257 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-06 13:09:46,757 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-06 13:09:46,758 - root - INFO - reading file which is of type csv
2024-08-06 13:09:59,241 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-06 13:10:02,326 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-06 13:10:02,333 - root - INFO - Implementing data Processing now....
2024-08-06 13:10:02,333 - Data_processing - WARNING - Cleaning data method started
2024-08-06 13:10:19,678 - Data_processing - WARNING - Cleaning data method completed
2024-08-06 13:10:42,989 - Extraction - WARNING - Writing files to the output directory....
2024-08-06 13:10:46,935 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o230.save.
: org.apache.spark.SparkException: Job aborted.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:496)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:261)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)
	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1215)
	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1420)
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:182)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:250)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:634)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:250)
	... 41 more

2024-08-06 13:13:07,100 - root - INFO - i am in the main method..
2024-08-06 13:13:07,103 - root - INFO - calling spark object
2024-08-06 13:13:07,103 - Create_spark - INFO - get_spark_object method started
2024-08-06 13:13:07,103 - Create_spark - INFO - master is local
2024-08-06 13:13:24,185 - Create_spark - INFO - Spark object created.....
2024-08-06 13:13:24,187 - root - INFO - Validating spark object..........
2024-08-06 13:13:24,187 - Validate - WARNING - started the get_current_date method...
2024-08-06 13:13:32,356 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 6))]
2024-08-06 13:13:32,356 - Validate - WARNING - Validation done , go frwd...
2024-08-06 13:13:32,358 - root - INFO - reading file which is of type parquet
2024-08-06 13:13:34,054 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-06 13:13:37,849 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-06 13:13:37,850 - root - INFO - reading file which is of type csv
2024-08-06 13:13:51,446 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-06 13:13:55,793 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-06 13:13:55,798 - root - INFO - Implementing data Processing now....
2024-08-06 13:13:55,799 - Data_processing - WARNING - Cleaning data method started
2024-08-06 13:14:11,113 - Data_processing - WARNING - Cleaning data method completed
2024-08-06 13:14:34,879 - Extraction - WARNING - Writing files to the output directory....
2024-08-06 13:14:35,426 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o230.save.
: java.lang.RuntimeException: java.io.FileNotFoundException: Hadoop bin directory does not exist: C:\hadoop\hadoop-3.2.0\bin\bin -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:736)
	at org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:271)
	at org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:287)
	at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)
	at org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:178)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:192)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.io.FileNotFoundException: Hadoop bin directory does not exist: C:\hadoop\hadoop-3.2.0\bin\bin -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.getQualifiedBinInner(Shell.java:608)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1886)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1846)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1819)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)
	at org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)
	at org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)
	at org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)
	at org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)
	at org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)
	at org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)
	at org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)
	at org.apache.spark.util.Utils$.createTempDir(Utils.scala:335)
	at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:344)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:898)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1043)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1052)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

2024-08-06 13:17:49,743 - root - INFO - i am in the main method..
2024-08-06 13:17:49,744 - root - INFO - calling spark object
2024-08-06 13:17:49,744 - Create_spark - INFO - get_spark_object method started
2024-08-06 13:17:49,745 - Create_spark - INFO - master is local
2024-08-06 13:18:10,436 - Create_spark - INFO - Spark object created.....
2024-08-06 13:18:10,438 - root - INFO - Validating spark object..........
2024-08-06 13:18:10,438 - Validate - WARNING - started the get_current_date method...
2024-08-06 13:18:18,497 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 6))]
2024-08-06 13:18:18,497 - Validate - WARNING - Validation done , go frwd...
2024-08-06 13:18:18,499 - root - INFO - reading file which is of type parquet
2024-08-06 13:18:20,100 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-06 13:18:23,623 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-06 13:18:23,625 - root - INFO - reading file which is of type csv
2024-08-06 13:18:35,822 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-06 13:18:38,387 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-06 13:18:38,392 - root - INFO - Implementing data Processing now....
2024-08-06 13:18:38,393 - Data_processing - WARNING - Cleaning data method started
2024-08-06 13:19:04,124 - Data_processing - WARNING - Cleaning data method completed
2024-08-06 13:19:36,046 - Extraction - WARNING - Writing files to the output directory....
2024-08-06 13:19:36,527 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o230.save.
: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: Hadoop home directory C:\hadoop\hadoop-3.2.0;PYTHONUNBUFFERED=1 does not exist -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:736)
	at org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:271)
	at org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:287)
	at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)
	at org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:178)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:192)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.io.FileNotFoundException: java.io.FileNotFoundException: Hadoop home directory C:\hadoop\hadoop-3.2.0;PYTHONUNBUFFERED=1 does not exist -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1886)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1846)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1819)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)
	at org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)
	at org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)
	at org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)
	at org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)
	at org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)
	at org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)
	at org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)
	at org.apache.spark.util.Utils$.createTempDir(Utils.scala:335)
	at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:344)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:898)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1043)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1052)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.io.FileNotFoundException: Hadoop home directory C:\hadoop\hadoop-3.2.0;PYTHONUNBUFFERED=1 does not exist
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:491)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 22 more

2024-08-06 13:21:04,780 - root - INFO - i am in the main method..
2024-08-06 13:21:04,781 - root - INFO - calling spark object
2024-08-06 13:21:04,781 - Create_spark - INFO - get_spark_object method started
2024-08-06 13:21:04,781 - Create_spark - INFO - master is local
2024-08-06 13:21:24,994 - Create_spark - INFO - Spark object created.....
2024-08-06 13:21:24,995 - root - INFO - Validating spark object..........
2024-08-06 13:21:24,995 - Validate - WARNING - started the get_current_date method...
2024-08-06 13:21:35,348 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 6))]
2024-08-06 13:21:35,348 - Validate - WARNING - Validation done , go frwd...
2024-08-06 13:21:35,349 - root - INFO - reading file which is of type parquet
2024-08-06 13:21:37,405 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-06 13:21:41,111 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-06 13:21:41,112 - root - INFO - reading file which is of type csv
2024-08-06 13:21:57,313 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-06 13:22:00,231 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-06 13:22:00,251 - root - INFO - Implementing data Processing now....
2024-08-06 13:22:00,252 - Data_processing - WARNING - Cleaning data method started
2024-08-06 13:22:24,222 - Data_processing - WARNING - Cleaning data method completed
2024-08-06 13:23:02,077 - Extraction - WARNING - Writing files to the output directory....
2024-08-06 13:23:09,619 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o230.save.
: org.apache.spark.SparkException: Job aborted.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:496)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:261)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)
	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1215)
	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1420)
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:182)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:250)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:634)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:250)
	... 41 more

2024-08-06 13:24:40,366 - root - INFO - i am in the main method..
2024-08-06 13:24:40,366 - root - INFO - calling spark object
2024-08-06 13:24:40,366 - Create_spark - INFO - get_spark_object method started
2024-08-06 13:24:40,366 - Create_spark - INFO - master is local
2024-08-06 13:24:55,826 - Create_spark - INFO - Spark object created.....
2024-08-06 13:24:55,828 - root - INFO - Validating spark object..........
2024-08-06 13:24:55,828 - Validate - WARNING - started the get_current_date method...
2024-08-06 13:25:06,241 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 6))]
2024-08-06 13:25:06,242 - Validate - WARNING - Validation done , go frwd...
2024-08-06 13:25:06,242 - root - INFO - reading file which is of type parquet
2024-08-06 13:25:07,944 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-06 13:25:11,301 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-06 13:25:11,302 - root - INFO - reading file which is of type csv
2024-08-06 13:25:29,647 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-06 13:25:33,947 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-06 13:25:33,954 - root - INFO - Implementing data Processing now....
2024-08-06 13:25:33,955 - Data_processing - WARNING - Cleaning data method started
2024-08-06 13:26:00,164 - Data_processing - WARNING - Cleaning data method completed
2024-08-06 13:26:22,275 - Extraction - WARNING - Writing files to the output directory....
2024-08-06 13:26:22,680 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o230.save.
: java.lang.RuntimeException: java.io.FileNotFoundException: Could not locate Hadoop executable: C:\Users\Admin\Downloads\spark-3.2.3-bin-hadoop3.2\spark-3.2.3-bin-hadoop3.2\bin\winutils.exe -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:736)
	at org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:271)
	at org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:287)
	at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)
	at org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:178)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:192)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.io.FileNotFoundException: Could not locate Hadoop executable: C:\Users\Admin\Downloads\spark-3.2.3-bin-hadoop3.2\spark-3.2.3-bin-hadoop3.2\bin\winutils.exe -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.getQualifiedBinInner(Shell.java:619)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1886)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1846)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1819)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)
	at org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)
	at org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)
	at org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)
	at org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)
	at org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)
	at org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)
	at org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)
	at org.apache.spark.util.Utils$.createTempDir(Utils.scala:335)
	at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:344)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:898)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1043)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1052)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

2024-08-06 13:26:39,653 - root - INFO - i am in the main method..
2024-08-06 13:26:39,653 - root - INFO - calling spark object
2024-08-06 13:26:39,653 - Create_spark - INFO - get_spark_object method started
2024-08-06 13:26:39,654 - Create_spark - INFO - master is local
2024-08-06 13:26:52,662 - Create_spark - INFO - Spark object created.....
2024-08-06 13:26:52,662 - root - INFO - Validating spark object..........
2024-08-06 13:26:52,662 - Validate - WARNING - started the get_current_date method...
2024-08-06 13:27:06,824 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 6))]
2024-08-06 13:27:06,824 - Validate - WARNING - Validation done , go frwd...
2024-08-06 13:27:06,825 - root - INFO - reading file which is of type parquet
2024-08-06 13:27:10,074 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-06 13:27:17,060 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-06 13:27:17,061 - root - INFO - reading file which is of type csv
2024-08-06 13:27:38,617 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-06 13:27:42,460 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-06 13:27:42,466 - root - INFO - Implementing data Processing now....
2024-08-06 13:27:42,466 - Data_processing - WARNING - Cleaning data method started
2024-08-06 13:27:57,611 - Data_processing - WARNING - Cleaning data method completed
2024-08-06 13:28:18,755 - Extraction - WARNING - Writing files to the output directory....
2024-08-06 13:28:21,927 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o230.save.
: org.apache.spark.SparkException: Job aborted.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:496)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:261)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)
	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1215)
	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1420)
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:182)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:250)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:634)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:250)
	... 41 more

2024-08-06 13:43:05,541 - root - INFO - i am in the main method..
2024-08-06 13:43:05,764 - root - INFO - calling spark object
2024-08-06 13:43:05,764 - Create_spark - INFO - get_spark_object method started
2024-08-06 13:43:05,765 - Create_spark - INFO - master is local
2024-08-06 13:43:22,470 - Create_spark - INFO - Spark object created.....
2024-08-06 13:43:22,471 - root - INFO - Validating spark object..........
2024-08-06 13:43:22,471 - Validate - WARNING - started the get_current_date method...
2024-08-06 13:43:30,313 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 6))]
2024-08-06 13:43:30,313 - Validate - WARNING - Validation done , go frwd...
2024-08-06 13:43:30,314 - root - INFO - reading file which is of type parquet
2024-08-06 13:43:32,452 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-06 13:43:36,588 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-06 13:43:36,589 - root - INFO - reading file which is of type csv
2024-08-06 13:43:48,420 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-06 13:43:52,306 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-06 13:43:52,313 - root - INFO - Implementing data Processing now....
2024-08-06 13:43:52,314 - Data_processing - WARNING - Cleaning data method started
2024-08-06 13:44:08,714 - Data_processing - WARNING - Cleaning data method completed
2024-08-06 13:44:32,209 - Extraction - WARNING - Writing files to the output directory....
2024-08-06 13:44:35,989 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o230.save.
: org.apache.spark.SparkException: Job aborted.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:496)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:261)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)
	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1215)
	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1420)
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:182)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:250)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:634)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:250)
	... 41 more

2024-08-06 14:01:30,823 - root - INFO - i am in the main method..
2024-08-06 14:01:30,824 - root - INFO - calling spark object
2024-08-06 14:01:30,824 - Create_spark - INFO - get_spark_object method started
2024-08-06 14:01:30,824 - Create_spark - INFO - master is local
2024-08-06 14:01:49,453 - Create_spark - INFO - Spark object created.....
2024-08-06 14:01:49,454 - root - INFO - Validating spark object..........
2024-08-06 14:01:49,454 - Validate - WARNING - started the get_current_date method...
2024-08-06 14:02:12,348 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 6))]
2024-08-06 14:02:12,348 - Validate - WARNING - Validation done , go frwd...
2024-08-06 14:02:12,350 - root - INFO - reading file which is of type parquet
2024-08-06 14:02:14,357 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-06 14:02:18,421 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-06 14:02:18,421 - root - INFO - reading file which is of type csv
2024-08-06 14:02:30,808 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-06 14:02:36,335 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-06 14:02:36,341 - root - INFO - Implementing data Processing now....
2024-08-06 14:02:36,342 - Data_processing - WARNING - Cleaning data method started
2024-08-06 14:02:53,413 - Data_processing - WARNING - Cleaning data method completed
2024-08-06 14:03:17,865 - Extraction - WARNING - Writing files to the output directory....
2024-08-06 14:03:18,275 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o230.save.
: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: Hadoop home directory C:\hadoop;PYTHONUNBUFFERED=1 does not exist -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:736)
	at org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:271)
	at org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:287)
	at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)
	at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)
	at org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:178)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:192)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.io.FileNotFoundException: java.io.FileNotFoundException: Hadoop home directory C:\hadoop;PYTHONUNBUFFERED=1 does not exist -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1886)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1846)
	at org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1819)
	at org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)
	at org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)
	at org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)
	at org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)
	at org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)
	at org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)
	at org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)
	at org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)
	at org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)
	at org.apache.spark.util.Utils$.createTempDir(Utils.scala:335)
	at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:344)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:898)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1043)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1052)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.io.FileNotFoundException: Hadoop home directory C:\hadoop;PYTHONUNBUFFERED=1 does not exist
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:491)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 22 more

2024-08-06 14:04:39,378 - root - INFO - i am in the main method..
2024-08-06 14:04:39,379 - root - INFO - calling spark object
2024-08-06 14:04:39,380 - Create_spark - INFO - get_spark_object method started
2024-08-06 14:04:39,380 - Create_spark - INFO - master is local
2024-08-06 14:04:51,651 - Create_spark - INFO - Spark object created.....
2024-08-06 14:04:51,651 - root - INFO - Validating spark object..........
2024-08-06 14:04:51,651 - Validate - WARNING - started the get_current_date method...
2024-08-06 14:04:58,421 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 6))]
2024-08-06 14:04:58,421 - Validate - WARNING - Validation done , go frwd...
2024-08-06 14:04:58,421 - root - INFO - reading file which is of type parquet
2024-08-06 14:04:59,985 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-06 14:05:03,111 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-06 14:05:03,112 - root - INFO - reading file which is of type csv
2024-08-06 14:05:17,553 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-06 14:05:24,549 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-06 14:05:24,556 - root - INFO - Implementing data Processing now....
2024-08-06 14:05:24,557 - Data_processing - WARNING - Cleaning data method started
2024-08-06 14:05:40,069 - Data_processing - WARNING - Cleaning data method completed
2024-08-06 14:06:06,248 - Extraction - WARNING - Writing files to the output directory....
2024-08-06 14:06:11,018 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o230.save.
: org.apache.spark.SparkException: Job aborted.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:496)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:261)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)
	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1215)
	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1420)
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:182)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:250)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:634)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:250)
	... 41 more

2024-08-06 14:09:36,621 - root - INFO - i am in the main method..
2024-08-06 14:09:36,621 - root - INFO - calling spark object
2024-08-06 14:09:36,621 - Create_spark - INFO - get_spark_object method started
2024-08-06 14:09:36,636 - Create_spark - INFO - master is local
2024-08-06 14:09:55,763 - Create_spark - INFO - Spark object created.....
2024-08-06 14:09:55,763 - root - INFO - Validating spark object..........
2024-08-06 14:09:55,764 - Validate - WARNING - started the get_current_date method...
2024-08-06 14:10:07,107 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 6))]
2024-08-06 14:10:07,107 - Validate - WARNING - Validation done , go frwd...
2024-08-06 14:10:07,108 - root - INFO - reading file which is of type parquet
2024-08-06 14:10:09,005 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-06 14:10:15,551 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-06 14:10:15,552 - root - INFO - reading file which is of type csv
2024-08-06 14:10:33,749 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-06 14:10:38,969 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-06 14:10:38,977 - root - INFO - Implementing data Processing now....
2024-08-06 14:10:38,977 - Data_processing - WARNING - Cleaning data method started
2024-08-06 14:11:03,116 - Data_processing - WARNING - Cleaning data method completed
2024-08-06 14:11:31,411 - Extraction - WARNING - Writing files to the output directory....
2024-08-06 14:11:36,121 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o230.save.
: org.apache.spark.SparkException: Job aborted.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:496)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:261)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)
	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1215)
	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1420)
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:182)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:250)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:634)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:250)
	... 41 more

2024-08-06 15:01:31,939 - root - INFO - i am in the main method..
2024-08-06 15:01:31,941 - root - INFO - calling spark object
2024-08-06 15:01:31,941 - Create_spark - INFO - get_spark_object method started
2024-08-06 15:01:31,941 - Create_spark - INFO - master is local
2024-08-06 15:01:46,272 - Create_spark - INFO - Spark object created.....
2024-08-06 15:01:46,272 - root - INFO - Validating spark object..........
2024-08-06 15:01:46,273 - Validate - WARNING - started the get_current_date method...
2024-08-06 15:01:53,301 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 6))]
2024-08-06 15:01:53,301 - Validate - WARNING - Validation done , go frwd...
2024-08-06 15:01:53,302 - root - INFO - reading file which is of type parquet
2024-08-06 15:01:58,005 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-06 15:02:02,461 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-06 15:02:02,463 - root - INFO - reading file which is of type csv
2024-08-06 15:02:18,403 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-06 15:02:21,292 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-06 15:02:21,292 - root - INFO - Implementing data Processing now....
2024-08-06 15:02:21,292 - Data_processing - WARNING - Cleaning data method started
2024-08-06 15:02:36,786 - Data_processing - WARNING - Cleaning data method completed
2024-08-06 15:03:03,485 - Persist - WARNING - Saving the cities data to hive database...
2024-08-06 15:03:10,635 - Persist - ERROR - An error has been occurred while saving dataframe to hive table city..
2024-08-06 15:03:10,635 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o231.saveAsTable.
: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)
	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1215)
	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1420)
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.validateTableLocation(SessionCatalog.scala:386)
	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:175)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)
	at org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:689)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:667)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:565)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)

2024-08-06 15:06:15,138 - root - INFO - i am in the main method..
2024-08-06 15:06:15,139 - root - INFO - calling spark object
2024-08-06 15:06:15,139 - Create_spark - INFO - get_spark_object method started
2024-08-06 15:06:15,140 - Create_spark - INFO - master is local
2024-08-06 15:06:28,362 - Create_spark - INFO - Spark object created.....
2024-08-06 15:06:28,362 - root - INFO - Validating spark object..........
2024-08-06 15:06:28,363 - Validate - WARNING - started the get_current_date method...
2024-08-06 15:06:40,052 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 6))]
2024-08-06 15:06:40,053 - Validate - WARNING - Validation done , go frwd...
2024-08-06 15:06:40,053 - root - INFO - reading file which is of type parquet
2024-08-06 15:06:41,691 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-06 15:06:46,659 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-06 15:06:46,659 - root - INFO - reading file which is of type csv
2024-08-06 15:06:56,142 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-06 15:06:58,232 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-06 15:06:58,238 - root - INFO - Implementing data Processing now....
2024-08-06 15:06:58,238 - Data_processing - WARNING - Cleaning data method started
2024-08-06 15:07:29,199 - Data_processing - WARNING - Cleaning data method completed
2024-08-06 15:07:48,627 - Persist - WARNING - Saving the cities data to hive database...
2024-08-06 15:07:55,569 - Persist - ERROR - An error has been occurred while saving dataframe to hive table city..
2024-08-06 15:07:55,571 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o231.saveAsTable.
: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)
	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1215)
	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1420)
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.validateTableLocation(SessionCatalog.scala:386)
	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:175)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)
	at org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:689)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:667)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:565)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)

2024-08-06 15:08:58,268 - root - INFO - i am in the main method..
2024-08-06 15:08:58,268 - root - INFO - calling spark object
2024-08-06 15:08:58,268 - Create_spark - INFO - get_spark_object method started
2024-08-06 15:08:58,268 - Create_spark - INFO - master is local
2024-08-06 15:09:09,814 - Create_spark - INFO - Spark object created.....
2024-08-06 15:09:09,814 - root - INFO - Validating spark object..........
2024-08-06 15:09:09,814 - Validate - WARNING - started the get_current_date method...
2024-08-06 15:09:16,166 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 6))]
2024-08-06 15:09:16,167 - Validate - WARNING - Validation done , go frwd...
2024-08-06 15:09:16,167 - root - INFO - reading file which is of type parquet
2024-08-06 15:09:17,522 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-06 15:09:20,798 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-06 15:09:20,799 - root - INFO - reading file which is of type csv
2024-08-06 15:09:32,664 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-06 15:09:35,803 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-06 15:09:35,810 - root - INFO - Implementing data Processing now....
2024-08-06 15:09:35,810 - Data_processing - WARNING - Cleaning data method started
2024-08-06 15:10:33,865 - Data_processing - WARNING - Cleaning data method completed
2024-08-06 15:10:53,139 - Extraction - WARNING - Writing files to the output directory....
2024-08-06 15:10:56,005 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o230.save.
: org.apache.spark.SparkException: Job aborted.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:496)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:261)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)
	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1215)
	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1420)
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:182)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:250)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:634)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:250)
	... 41 more

2024-08-06 15:11:24,466 - root - INFO - i am in the main method..
2024-08-06 15:11:24,467 - root - INFO - calling spark object
2024-08-06 15:11:24,467 - Create_spark - INFO - get_spark_object method started
2024-08-06 15:11:24,467 - Create_spark - INFO - master is local
2024-08-06 15:11:43,062 - Create_spark - INFO - Spark object created.....
2024-08-06 15:11:43,062 - root - INFO - Validating spark object..........
2024-08-06 15:11:43,062 - Validate - WARNING - started the get_current_date method...
2024-08-06 15:11:56,728 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 6))]
2024-08-06 15:11:56,728 - Validate - WARNING - Validation done , go frwd...
2024-08-06 15:11:56,729 - root - INFO - reading file which is of type parquet
2024-08-06 15:11:59,112 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-06 15:12:03,682 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-06 15:12:03,683 - root - INFO - reading file which is of type csv
2024-08-06 15:12:13,880 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-06 15:12:16,677 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-06 15:12:16,686 - root - INFO - Implementing data Processing now....
2024-08-06 15:12:16,686 - Data_processing - WARNING - Cleaning data method started
2024-08-06 15:12:27,881 - Data_processing - WARNING - Cleaning data method completed
2024-08-06 15:12:46,367 - Extraction - WARNING - Writing files to the output directory....
2024-08-06 15:12:48,972 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o230.save.
: org.apache.spark.SparkException: Job aborted.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:496)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:261)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)
	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1215)
	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1420)
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:182)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:250)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:634)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:250)
	... 41 more

2024-08-06 15:19:52,545 - root - INFO - i am in the main method..
2024-08-06 15:19:52,545 - root - INFO - calling spark object
2024-08-06 15:19:52,553 - Create_spark - INFO - get_spark_object method started
2024-08-06 15:19:52,553 - Create_spark - INFO - master is local
2024-08-06 15:20:12,902 - Create_spark - INFO - Spark object created.....
2024-08-06 15:20:12,903 - root - INFO - Validating spark object..........
2024-08-06 15:20:12,903 - Validate - WARNING - started the get_current_date method...
2024-08-06 15:20:28,524 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 6))]
2024-08-06 15:20:28,524 - Validate - WARNING - Validation done , go frwd...
2024-08-06 15:20:28,527 - root - INFO - reading file which is of type parquet
2024-08-06 15:20:31,313 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-06 15:20:34,989 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-06 15:20:34,995 - root - INFO - reading file which is of type csv
2024-08-06 15:20:46,200 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-06 15:20:48,473 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-06 15:20:48,473 - root - INFO - Implementing data Processing now....
2024-08-06 15:20:48,489 - Data_processing - WARNING - Cleaning data method started
2024-08-06 15:21:02,348 - Data_processing - WARNING - Cleaning data method completed
2024-08-06 15:21:27,059 - Extraction - WARNING - Writing files to the output directory....
2024-08-06 15:21:35,371 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o230.save.
: org.apache.spark.SparkException: Job aborted.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:496)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:261)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)
	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1215)
	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1420)
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:182)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:250)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:634)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:250)
	... 41 more

2024-08-06 15:28:51,001 - root - INFO - i am in the main method..
2024-08-06 15:28:51,001 - root - INFO - calling spark object
2024-08-06 15:28:51,001 - Create_spark - INFO - get_spark_object method started
2024-08-06 15:28:51,001 - Create_spark - INFO - master is local
2024-08-06 15:29:04,384 - Create_spark - INFO - Spark object created.....
2024-08-06 15:29:04,385 - root - INFO - Validating spark object..........
2024-08-06 15:29:04,385 - Validate - WARNING - started the get_current_date method...
2024-08-06 15:29:10,502 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 6))]
2024-08-06 15:29:10,502 - Validate - WARNING - Validation done , go frwd...
2024-08-06 15:29:10,503 - root - INFO - reading file which is of type parquet
2024-08-06 15:29:12,061 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-06 15:29:15,308 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-06 15:29:15,308 - root - INFO - reading file which is of type csv
2024-08-06 15:29:24,538 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-06 15:29:27,403 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-06 15:29:27,410 - root - INFO - Implementing data Processing now....
2024-08-06 15:29:27,411 - Data_processing - WARNING - Cleaning data method started
2024-08-06 15:29:40,392 - Data_processing - WARNING - Cleaning data method completed
2024-08-06 15:29:57,745 - Extraction - WARNING - Writing files to the output directory....
2024-08-06 15:30:00,493 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o230.save.
: org.apache.spark.SparkException: Job aborted.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:496)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:261)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)
	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1215)
	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1420)
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:182)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:250)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:634)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:250)
	... 41 more

2024-08-06 15:31:02,274 - root - INFO - i am in the main method..
2024-08-06 15:31:02,274 - root - INFO - calling spark object
2024-08-06 15:31:02,274 - Create_spark - INFO - get_spark_object method started
2024-08-06 15:31:02,275 - Create_spark - INFO - master is local
2024-08-06 15:31:14,962 - Create_spark - INFO - Spark object created.....
2024-08-06 15:31:14,962 - root - INFO - Validating spark object..........
2024-08-06 15:31:14,964 - Validate - WARNING - started the get_current_date method...
2024-08-06 15:31:21,606 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 6))]
2024-08-06 15:31:21,606 - Validate - WARNING - Validation done , go frwd...
2024-08-06 15:31:21,607 - root - INFO - reading file which is of type parquet
2024-08-06 15:31:23,058 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-06 15:31:26,384 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-06 15:31:26,385 - root - INFO - reading file which is of type csv
2024-08-06 15:31:35,212 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-06 15:31:37,407 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-06 15:31:37,410 - root - INFO - Implementing data Processing now....
2024-08-06 15:31:37,411 - Data_processing - WARNING - Cleaning data method started
2024-08-06 15:31:48,584 - Data_processing - WARNING - Cleaning data method completed
2024-08-06 15:32:06,427 - Extraction - WARNING - Writing files to the output directory....
2024-08-06 15:32:09,048 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o230.save.
: org.apache.spark.SparkException: Job aborted.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:496)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:261)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)
	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1215)
	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1420)
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:182)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:250)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:634)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:250)
	... 41 more

2024-08-06 17:10:01,722 - root - INFO - i am in the main method..
2024-08-06 17:10:01,723 - root - INFO - calling spark object
2024-08-06 17:10:01,723 - Create_spark - INFO - get_spark_object method started
2024-08-06 17:10:01,723 - Create_spark - INFO - master is local
2024-08-06 17:10:17,608 - Create_spark - INFO - Spark object created.....
2024-08-06 17:10:17,608 - root - INFO - Validating spark object..........
2024-08-06 17:10:17,608 - Validate - WARNING - started the get_current_date method...
2024-08-06 17:10:24,708 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 6))]
2024-08-06 17:10:24,708 - Validate - WARNING - Validation done , go frwd...
2024-08-06 17:10:24,708 - root - INFO - reading file which is of type parquet
2024-08-06 17:10:26,192 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-06 17:10:29,882 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-06 17:10:29,883 - root - INFO - reading file which is of type csv
2024-08-06 17:10:40,365 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-06 17:10:43,188 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-06 17:10:43,196 - root - INFO - Implementing data Processing now....
2024-08-06 17:10:43,197 - Data_processing - WARNING - Cleaning data method started
2024-08-06 17:10:55,325 - Data_processing - WARNING - Cleaning data method completed
2024-08-06 17:11:16,272 - Extraction - WARNING - Writing files to the output directory....
2024-08-06 17:11:22,994 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o230.save.
: org.apache.spark.SparkException: Job aborted.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:496)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:261)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)
	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1215)
	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1420)
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:182)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:250)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:634)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:250)
	... 41 more

2024-08-06 17:13:05,710 - root - INFO - i am in the main method..
2024-08-06 17:13:05,711 - root - INFO - calling spark object
2024-08-06 17:13:05,711 - Create_spark - INFO - get_spark_object method started
2024-08-06 17:13:05,711 - Create_spark - INFO - master is local
2024-08-06 17:13:17,328 - Create_spark - INFO - Spark object created.....
2024-08-06 17:13:17,329 - root - INFO - Validating spark object..........
2024-08-06 17:13:17,329 - Validate - WARNING - started the get_current_date method...
2024-08-06 17:13:23,739 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 6))]
2024-08-06 17:13:23,741 - Validate - WARNING - Validation done , go frwd...
2024-08-06 17:13:23,748 - root - INFO - reading file which is of type parquet
2024-08-06 17:13:25,029 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-06 17:13:28,099 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-06 17:13:28,099 - root - INFO - reading file which is of type csv
2024-08-06 17:13:37,748 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-06 17:13:40,301 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-06 17:13:40,306 - root - INFO - Implementing data Processing now....
2024-08-06 17:13:40,306 - Data_processing - WARNING - Cleaning data method started
2024-08-06 17:13:52,908 - Data_processing - WARNING - Cleaning data method completed
2024-08-06 17:14:10,587 - Extraction - WARNING - Writing files to the output directory....
2024-08-06 17:14:13,452 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o230.save.
: org.apache.spark.SparkException: Job aborted.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:496)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:261)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)
	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1215)
	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1420)
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:182)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:250)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:634)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:250)
	... 41 more

2024-08-06 17:17:52,813 - root - INFO - i am in the main method..
2024-08-06 17:17:52,814 - root - INFO - calling spark object
2024-08-06 17:17:52,814 - Create_spark - INFO - get_spark_object method started
2024-08-06 17:17:52,814 - Create_spark - INFO - master is local
2024-08-06 17:18:04,927 - Create_spark - INFO - Spark object created.....
2024-08-06 17:18:04,927 - root - INFO - Validating spark object..........
2024-08-06 17:18:04,927 - Validate - WARNING - started the get_current_date method...
2024-08-06 17:18:11,636 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 6))]
2024-08-06 17:18:11,636 - Validate - WARNING - Validation done , go frwd...
2024-08-06 17:18:11,637 - root - INFO - reading file which is of type parquet
2024-08-06 17:18:13,117 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-06 17:18:15,885 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-06 17:18:15,885 - root - INFO - reading file which is of type csv
2024-08-06 17:18:26,793 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-06 17:18:28,855 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-06 17:18:28,860 - root - INFO - Implementing data Processing now....
2024-08-06 17:18:28,860 - Data_processing - WARNING - Cleaning data method started
2024-08-06 17:18:40,696 - Data_processing - WARNING - Cleaning data method completed
2024-08-06 17:18:58,516 - Extraction - WARNING - Writing files to the output directory....
2024-08-06 17:19:01,558 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o230.save.
: org.apache.spark.SparkException: Job aborted.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:496)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:261)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)
	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1215)
	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1420)
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:182)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:250)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:634)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:250)
	... 41 more

2024-08-06 17:28:51,742 - root - INFO - i am in the main method..
2024-08-06 17:28:51,743 - root - INFO - calling spark object
2024-08-06 17:28:51,743 - Create_spark - INFO - get_spark_object method started
2024-08-06 17:28:51,743 - Create_spark - INFO - master is local
2024-08-06 17:29:05,323 - Create_spark - INFO - Spark object created.....
2024-08-06 17:29:05,323 - root - INFO - Validating spark object..........
2024-08-06 17:29:05,323 - Validate - WARNING - started the get_current_date method...
2024-08-06 17:29:12,242 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 6))]
2024-08-06 17:29:12,243 - Validate - WARNING - Validation done , go frwd...
2024-08-06 17:29:12,243 - root - INFO - reading file which is of type parquet
2024-08-06 17:29:13,560 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-06 17:29:16,268 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-06 17:29:16,269 - root - INFO - reading file which is of type csv
2024-08-06 17:29:25,449 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-06 17:29:27,454 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-06 17:29:27,458 - root - INFO - Implementing data Processing now....
2024-08-06 17:29:27,458 - Data_processing - WARNING - Cleaning data method started
2024-08-06 17:29:38,756 - Data_processing - WARNING - Cleaning data method completed
2024-08-06 17:29:57,589 - Persist - WARNING - Saving the cities data to hive database...
2024-08-06 17:30:04,859 - Persist - ERROR - An error has been occurred while saving dataframe to hive table city..
2024-08-06 17:30:04,861 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o231.saveAsTable.
: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)
	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1215)
	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1420)
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.validateTableLocation(SessionCatalog.scala:386)
	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:175)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)
	at org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:689)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:667)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:565)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)

2024-08-06 17:42:06,485 - root - INFO - i am in the main method..
2024-08-06 17:42:06,486 - root - INFO - calling spark object
2024-08-06 17:42:06,486 - Create_spark - INFO - get_spark_object method started
2024-08-06 17:42:06,486 - Create_spark - INFO - master is local
2024-08-06 17:42:24,412 - Create_spark - INFO - Spark object created.....
2024-08-06 17:42:24,415 - root - INFO - Validating spark object..........
2024-08-06 17:42:24,416 - Validate - WARNING - started the get_current_date method...
2024-08-06 17:42:31,073 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 6))]
2024-08-06 17:42:31,073 - Validate - WARNING - Validation done , go frwd...
2024-08-06 17:42:31,074 - root - INFO - reading file which is of type parquet
2024-08-06 17:42:32,703 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-06 17:42:36,282 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-06 17:42:36,285 - root - INFO - reading file which is of type csv
2024-08-06 17:42:45,627 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-06 17:42:48,075 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-06 17:42:48,081 - root - INFO - Implementing data Processing now....
2024-08-06 17:42:48,082 - Data_processing - WARNING - Cleaning data method started
2024-08-06 17:43:00,211 - Data_processing - WARNING - Cleaning data method completed
2024-08-06 17:43:19,402 - Persist - WARNING - Saving the cities data to hive database...
2024-08-06 17:43:37,618 - Persist - WARNING - Successfully saved dataframe to hive table city..
2024-08-06 17:43:37,618 - Persist - WARNING - Saving the cities data to hive database...
2024-08-06 17:43:37,645 - Persist - ERROR - An error has been occurred while saving dataframe to hive table city..
2024-08-06 17:43:37,655 - root - ERROR - An error occurred when calling main() please check the trace=== 
missing 'EXISTS' at 'exist'(line 1, pos 23)

== SQL ==
Create database if not exist prescribers
-----------------------^^^

2024-08-06 17:45:48,236 - root - INFO - i am in the main method..
2024-08-06 17:45:48,649 - root - INFO - calling spark object
2024-08-06 17:45:48,649 - Create_spark - INFO - get_spark_object method started
2024-08-06 17:45:48,649 - Create_spark - INFO - master is local
2024-08-06 17:46:06,426 - Create_spark - INFO - Spark object created.....
2024-08-06 17:46:06,426 - root - INFO - Validating spark object..........
2024-08-06 17:46:06,426 - Validate - WARNING - started the get_current_date method...
2024-08-06 17:46:14,490 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 6))]
2024-08-06 17:46:14,490 - Validate - WARNING - Validation done , go frwd...
2024-08-06 17:46:14,491 - root - INFO - reading file which is of type parquet
2024-08-06 17:46:15,939 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-06 17:46:19,712 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-06 17:46:19,714 - root - INFO - reading file which is of type csv
2024-08-06 17:46:28,805 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-06 17:46:30,903 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-06 17:46:30,909 - root - INFO - Implementing data Processing now....
2024-08-06 17:46:30,910 - Data_processing - WARNING - Cleaning data method started
2024-08-06 17:46:50,193 - Data_processing - WARNING - Cleaning data method completed
2024-08-06 17:47:10,962 - Extraction - WARNING - Writing files to the output directory....
2024-08-06 17:47:13,115 - Extraction - WARNING - Successfully written files to the output directory..
2024-08-06 17:47:13,117 - Extraction - WARNING - Writing files to the output directory....
2024-08-06 17:47:30,691 - Extraction - WARNING - Successfully written files to the output directory..
2024-08-06 17:47:30,692 - Persist - WARNING - Saving the cities data to hive database...
2024-08-06 17:48:10,654 - Persist - WARNING - Successfully saved dataframe to hive table city..
2024-08-06 17:48:10,664 - Persist - WARNING - Saving the cities data to hive database...
2024-08-06 17:48:10,711 - Persist - ERROR - An error has been occurred while saving dataframe to hive table city..
2024-08-06 17:48:10,724 - root - ERROR - An error occurred when calling main() please check the trace=== 
missing 'EXISTS' at 'exist'(line 1, pos 23)

== SQL ==
Create database if not exist prescribers
-----------------------^^^

2024-08-06 17:49:03,025 - root - INFO - i am in the main method..
2024-08-06 17:49:03,026 - root - INFO - calling spark object
2024-08-06 17:49:03,026 - Create_spark - INFO - get_spark_object method started
2024-08-06 17:49:03,026 - Create_spark - INFO - master is local
2024-08-06 17:49:57,399 - Create_spark - INFO - Spark object created.....
2024-08-06 17:49:57,399 - root - INFO - Validating spark object..........
2024-08-06 17:49:57,399 - Validate - WARNING - started the get_current_date method...
2024-08-06 17:50:11,734 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 6))]
2024-08-06 17:50:11,736 - Validate - WARNING - Validation done , go frwd...
2024-08-06 17:50:11,739 - root - INFO - reading file which is of type parquet
2024-08-06 17:50:17,916 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-06 17:50:21,421 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-06 17:50:21,422 - root - INFO - reading file which is of type csv
2024-08-06 17:50:52,387 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-06 17:51:05,347 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-06 17:51:05,347 - root - INFO - Implementing data Processing now....
2024-08-06 17:51:05,347 - Data_processing - WARNING - Cleaning data method started
2024-08-06 17:51:59,173 - Data_processing - WARNING - Cleaning data method completed
2024-08-06 17:52:20,138 - root - INFO - Saving data to local destination...
2024-08-06 17:52:20,141 - Extraction - WARNING - Writing files to the output directory....
2024-08-06 17:52:22,398 - Extraction - WARNING - Successfully written files to the output directory..
2024-08-06 17:52:22,398 - Extraction - WARNING - Writing files to the output directory....
2024-08-06 17:52:50,604 - Extraction - WARNING - Successfully written files to the output directory..
2024-08-06 17:52:50,605 - Persist - WARNING - Saving the cities data to hive database...
2024-08-06 17:53:03,299 - Persist - WARNING - Successfully saved dataframe to hive table city..
2024-08-06 17:53:03,300 - Persist - WARNING - Saving the cities data to hive database...
2024-08-06 17:55:24,957 - Persist - WARNING - Successfully saved dataframe to hive table city..
2024-08-06 17:55:24,965 - root - INFO - Application done
2024-08-06 18:03:26,024 - root - INFO - i am in the main method..
2024-08-06 18:03:26,025 - root - INFO - calling spark object
2024-08-06 18:03:26,025 - Create_spark - INFO - get_spark_object method started
2024-08-06 18:03:26,025 - Create_spark - INFO - master is local
2024-08-06 18:03:46,694 - Create_spark - INFO - Spark object created.....
2024-08-06 18:03:46,694 - root - INFO - Validating spark object..........
2024-08-06 18:03:46,694 - Validate - WARNING - started the get_current_date method...
2024-08-06 18:03:54,569 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 6))]
2024-08-06 18:03:54,569 - Validate - WARNING - Validation done , go frwd...
2024-08-06 18:03:54,570 - root - INFO - reading file which is of type parquet
2024-08-06 18:03:56,119 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-06 18:03:59,465 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-06 18:03:59,466 - root - INFO - reading file which is of type csv
2024-08-06 18:04:10,065 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-06 18:04:12,253 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-06 18:04:12,257 - root - INFO - Implementing data Processing now....
2024-08-06 18:04:12,257 - Data_processing - WARNING - Cleaning data method started
2024-08-06 18:04:24,331 - Data_processing - WARNING - Cleaning data method completed
2024-08-06 18:04:42,860 - root - INFO - Saving data to local destination...
2024-08-06 18:04:42,860 - Extraction - WARNING - Writing files to the output directory....
2024-08-06 18:04:44,759 - Extraction - WARNING - Successfully written files to the output directory..
2024-08-06 18:04:44,759 - Extraction - WARNING - Writing files to the output directory....
2024-08-06 18:05:02,017 - Extraction - WARNING - Successfully written files to the output directory..
2024-08-06 18:05:02,017 - root - INFO - Implementing hive function now....
2024-08-06 18:05:02,018 - Persist - WARNING - Saving the cities data to hive database...
2024-08-06 18:05:25,657 - Persist - WARNING - Successfully saved dataframe to hive table city..
2024-08-06 18:05:25,658 - Persist - WARNING - Saving the cities data to hive database...
2024-08-06 18:05:46,208 - Persist - WARNING - Successfully saved dataframe to hive table city..
2024-08-06 18:05:46,209 - root - INFO - Application done
2024-08-07 16:05:12,323 - root - INFO - i am in the main method..
2024-08-07 16:05:12,325 - root - INFO - calling spark object
2024-08-07 16:05:12,325 - Create_spark - INFO - get_spark_object method started
2024-08-07 16:05:12,326 - Create_spark - INFO - master is local
2024-08-07 16:05:22,028 - Create_spark - INFO - Spark object created.....
2024-08-07 16:05:22,028 - root - INFO - Validating spark object..........
2024-08-07 16:05:22,028 - Validate - WARNING - started the get_current_date method...
2024-08-07 16:05:28,506 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 7))]
2024-08-07 16:05:28,506 - Validate - WARNING - Validation done , go frwd...
2024-08-07 16:05:28,507 - root - INFO - reading file which is of type parquet
2024-08-07 16:05:30,100 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-07 16:05:32,706 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-07 16:05:32,708 - root - INFO - reading file which is of type csv
2024-08-07 16:05:41,641 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-07 16:05:44,140 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-07 16:05:44,142 - root - INFO - Implementing data Processing now....
2024-08-07 16:05:44,143 - Data_processing - WARNING - Cleaning data method started
2024-08-07 16:05:54,555 - Data_processing - WARNING - Cleaning data method completed
2024-08-07 16:06:10,132 - root - INFO - Saving data to local destination...
2024-08-07 16:06:10,132 - Extraction - WARNING - Writing files to the output directory....
2024-08-07 16:06:11,852 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o230.save.
: org.apache.spark.SparkException: Job aborted.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:496)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:261)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)
	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1215)
	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1420)
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:182)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:250)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:634)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:250)
	... 41 more

2024-08-07 16:07:51,533 - root - INFO - i am in the main method..
2024-08-07 16:07:51,534 - root - INFO - calling spark object
2024-08-07 16:07:51,534 - Create_spark - INFO - get_spark_object method started
2024-08-07 16:07:51,534 - Create_spark - INFO - master is local
2024-08-07 16:08:04,419 - Create_spark - INFO - Spark object created.....
2024-08-07 16:08:04,419 - root - INFO - Validating spark object..........
2024-08-07 16:08:04,420 - Validate - WARNING - started the get_current_date method...
2024-08-07 16:08:17,816 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 7))]
2024-08-07 16:08:17,817 - Validate - WARNING - Validation done , go frwd...
2024-08-07 16:08:17,817 - root - INFO - reading file which is of type parquet
2024-08-07 16:08:20,566 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-07 16:08:24,832 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-07 16:08:24,833 - root - INFO - reading file which is of type csv
2024-08-07 16:08:30,687 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-07 16:08:32,406 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-07 16:08:32,409 - root - INFO - Implementing data Processing now....
2024-08-07 16:08:32,410 - Data_processing - WARNING - Cleaning data method started
2024-08-07 16:08:44,846 - Data_processing - WARNING - Cleaning data method completed
2024-08-07 16:09:02,145 - root - INFO - Saving data to local destination...
2024-08-07 16:09:02,147 - Extraction - WARNING - Writing files to the output directory....
2024-08-07 16:09:04,797 - root - ERROR - An error occurred when calling main() please check the trace=== An error occurred while calling o230.save.
: org.apache.spark.SparkException: Job aborted.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:496)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:261)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:115)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)
	at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)
	at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1215)
	at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1420)
	at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)
	at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)
	at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:182)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$21(FileFormatWriter.scala:250)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:634)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:250)
	... 41 more

2024-08-07 16:11:33,188 - root - INFO - i am in the main method..
2024-08-07 16:11:33,188 - root - INFO - calling spark object
2024-08-07 16:11:33,189 - Create_spark - INFO - get_spark_object method started
2024-08-07 16:11:33,189 - Create_spark - INFO - master is local
2024-08-07 16:11:42,557 - Create_spark - INFO - Spark object created.....
2024-08-07 16:11:42,558 - root - INFO - Validating spark object..........
2024-08-07 16:11:42,558 - Validate - WARNING - started the get_current_date method...
2024-08-07 16:11:47,314 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 7))]
2024-08-07 16:11:47,314 - Validate - WARNING - Validation done , go frwd...
2024-08-07 16:11:47,315 - root - INFO - reading file which is of type parquet
2024-08-07 16:11:48,480 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-07 16:11:50,931 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-07 16:11:50,932 - root - INFO - reading file which is of type csv
2024-08-07 16:11:58,171 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-07 16:11:59,811 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-07 16:11:59,815 - root - INFO - Implementing data Processing now....
2024-08-07 16:11:59,815 - Data_processing - WARNING - Cleaning data method started
2024-08-07 16:12:09,097 - Data_processing - WARNING - Cleaning data method completed
2024-08-07 16:12:24,835 - root - INFO - Saving data to local destination...
2024-08-07 16:12:24,836 - Extraction - WARNING - Writing files to the output directory....
2024-08-07 16:12:26,390 - Extraction - WARNING - Successfully written files to the output directory..
2024-08-07 16:12:26,391 - Extraction - WARNING - Writing files to the output directory....
2024-08-07 16:12:43,698 - Extraction - WARNING - Successfully written files to the output directory..
2024-08-07 16:12:43,699 - root - INFO - Implementing hive function now....
2024-08-07 16:12:43,699 - Persist - WARNING - Saving the cities data to hive database...
2024-08-07 16:12:52,672 - Persist - WARNING - Successfully saved dataframe to hive table city..
2024-08-07 16:12:52,672 - Persist - WARNING - Saving the cities data to hive database...
2024-08-07 16:13:08,265 - Persist - WARNING - Successfully saved dataframe to hive table city..
2024-08-07 16:14:11,576 - root - INFO - i am in the main method..
2024-08-07 16:14:11,576 - root - INFO - calling spark object
2024-08-07 16:14:11,576 - Create_spark - INFO - get_spark_object method started
2024-08-07 16:14:11,576 - Create_spark - INFO - master is local
2024-08-07 16:14:20,792 - Create_spark - INFO - Spark object created.....
2024-08-07 16:14:20,792 - root - INFO - Validating spark object..........
2024-08-07 16:14:20,792 - Validate - WARNING - started the get_current_date method...
2024-08-07 16:14:26,205 - Validate - WARNING - validating spark object with current date-[Row(current_date()=datetime.date(2024, 8, 7))]
2024-08-07 16:14:26,206 - Validate - WARNING - Validation done , go frwd...
2024-08-07 16:14:26,206 - root - INFO - reading file which is of type parquet
2024-08-07 16:14:27,150 - Ingest - WARNING - dataframe is created successfully which is of parquet
2024-08-07 16:14:30,184 - Ingest - WARNING - Number of records present in the DataFrame[city: string, city_ascii: string, state_id: string, state_name: string, county_fips: int, county_name: string, lat: double, lng: double, population: int, density: int, timezone: string, zips: string] are :: 28338
2024-08-07 16:14:30,184 - root - INFO - reading file which is of type csv
2024-08-07 16:14:36,638 - Ingest - WARNING - dataframe is created successfully which is of csv
2024-08-07 16:14:38,423 - Ingest - WARNING - Number of records present in the DataFrame[npi: int, nppes_provider_last_org_name: string, nppes_provider_first_name: string, nppes_provider_city: string, nppes_provider_state: string, specialty_description: string, description_flag: string, drug_name: string, generic_name: string, bene_count: int, total_claim_count: int, total_30_day_fill_count: double, total_day_supply: int, total_drug_cost: double, bene_count_ge65: int, bene_count_ge65_suppress_flag: string, total_claim_count_ge65: int, ge65_suppress_flag: string, total_30_day_fill_count_ge65: double, total_day_supply_ge65: int, total_drug_cost_ge65: double, years_of_exp: string] are :: 1329329
2024-08-07 16:14:38,427 - root - INFO - Implementing data Processing now....
2024-08-07 16:14:38,427 - Data_processing - WARNING - Cleaning data method started
2024-08-07 16:14:47,832 - Data_processing - WARNING - Cleaning data method completed
2024-08-07 16:15:01,239 - root - INFO - Saving data to local destination...
2024-08-07 16:15:01,239 - Extraction - WARNING - Writing files to the output directory....
2024-08-07 16:15:02,963 - Extraction - WARNING - Successfully written files to the output directory..
2024-08-07 16:15:02,963 - Extraction - WARNING - Writing files to the output directory....
2024-08-07 16:15:16,009 - Extraction - WARNING - Successfully written files to the output directory..
2024-08-07 16:15:16,010 - root - INFO - Implementing hive function now....
2024-08-07 16:15:16,010 - Persist - WARNING - Saving the cities data to hive database...
2024-08-07 16:15:24,915 - Persist - WARNING - Successfully saved dataframe to hive table city..
2024-08-07 16:15:24,915 - Persist - WARNING - Saving the cities data to hive database...
2024-08-07 16:15:40,846 - Persist - WARNING - Successfully saved dataframe to hive table city..
2024-08-07 16:15:40,848 - root - INFO - Application done
